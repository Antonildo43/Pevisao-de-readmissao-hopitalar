{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previsão de readmissão hospitalar utilizando modelo de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### por Antonildo Santos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "O objetivo desta análise é criar um modelo de Rede Neural Profunda (Deep Learning) capaz prever, com o mais alto grau de precisão possível, os atendimentos propícios a ocorrência de Readmissão Hospitalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apresentação dos dados\n",
    "\n",
    "Para realizar este trabalho utilizei um conjunto de dados disponível publicamente no repositório da UCI [Link]( https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008# ). Este conjunto de dados representa 10 anos (1999-2008) de atendimento clínico em 130 hospitais dos EUA, contendo 101.766 observações em 10 anos. Inclui mais de 50 atributos, que representam características do paciente, diagnósticos, exames, etc. As informações foram extraídas do banco de dados contendo registros que satisfizeram os seguintes critérios. \n",
    " \n",
    "* 1) Cada atendimento representa uma internação hospitalar. \n",
    "* 2) Contempla apenas atendimentos de pacientes diabéticos, ou seja, aquele em que qualquer tipo de diabetes foi introduzido no sistema como um diagnóstico. \n",
    "* 3) O tempo de permanência foi de no mínimo 1 dia e no máximo 14 dias. \n",
    "* 4) Testes laboratoriais foram realizados durante o atendimento. \n",
    "* 5) Medicamentos foram administrados durante o atendimento.\n",
    " \n",
    "Os dados contêm atributos como número do paciente, raça, gênero, idade, tipo de internação, tempo no hospital, número de exames laboratoriais realizados, resultado do exame de HbA1c, diagnósticos, número de medicamentos utilizados, se usa medicamentos para diabético e quais, número de pacientes ambulatoriais , internação e visitas de emergência no ano anterior à hospitalização, etc. Alguns desses atributos serão desconsiderados nesta análise pois não terão relevância para o objetivo do trabalho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "import matplotlib as mpl\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import seaborn as sns   \n",
    "import warnings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando a base de dados\n",
    "df_uci_diabetic = pd.read_csv('diabetic_data.csv', decimal=b',')\n",
    "\n",
    "# Criando um novo dataframe a partir do df_uci_diabetic\n",
    "df = df_uci_diabetic.copy (deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Dataframe diabetic_data possui 101766 linhas e 50 colunas\n"
     ]
    }
   ],
   "source": [
    "print('O Dataframe diabetic_data possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Data columns (total 50 columns):\n",
      "encounter_id                101766 non-null int64\n",
      "patient_nbr                 101766 non-null int64\n",
      "race                        101766 non-null object\n",
      "gender                      101766 non-null object\n",
      "age                         101766 non-null object\n",
      "weight                      101766 non-null object\n",
      "admission_type_id           101766 non-null int64\n",
      "discharge_disposition_id    101766 non-null int64\n",
      "admission_source_id         101766 non-null int64\n",
      "time_in_hospital            101766 non-null int64\n",
      "payer_code                  101766 non-null object\n",
      "medical_specialty           101766 non-null object\n",
      "num_lab_procedures          101766 non-null int64\n",
      "num_procedures              101766 non-null int64\n",
      "num_medications             101766 non-null int64\n",
      "number_outpatient           101766 non-null int64\n",
      "number_emergency            101766 non-null int64\n",
      "number_inpatient            101766 non-null int64\n",
      "diag_1                      101766 non-null object\n",
      "diag_2                      101766 non-null object\n",
      "diag_3                      101766 non-null object\n",
      "number_diagnoses            101766 non-null int64\n",
      "max_glu_serum               101766 non-null object\n",
      "A1Cresult                   101766 non-null object\n",
      "metformin                   101766 non-null object\n",
      "repaglinide                 101766 non-null object\n",
      "nateglinide                 101766 non-null object\n",
      "chlorpropamide              101766 non-null object\n",
      "glimepiride                 101766 non-null object\n",
      "acetohexamide               101766 non-null object\n",
      "glipizide                   101766 non-null object\n",
      "glyburide                   101766 non-null object\n",
      "tolbutamide                 101766 non-null object\n",
      "pioglitazone                101766 non-null object\n",
      "rosiglitazone               101766 non-null object\n",
      "acarbose                    101766 non-null object\n",
      "miglitol                    101766 non-null object\n",
      "troglitazone                101766 non-null object\n",
      "tolazamide                  101766 non-null object\n",
      "examide                     101766 non-null object\n",
      "citoglipton                 101766 non-null object\n",
      "insulin                     101766 non-null object\n",
      "glyburide-metformin         101766 non-null object\n",
      "glipizide-metformin         101766 non-null object\n",
      "glimepiride-pioglitazone    101766 non-null object\n",
      "metformin-rosiglitazone     101766 non-null object\n",
      "metformin-pioglitazone      101766 non-null object\n",
      "change                      101766 non-null object\n",
      "diabetesMed                 101766 non-null object\n",
      "readmitted                  101766 non-null object\n",
      "dtypes: int64(13), object(37)\n",
      "memory usage: 38.8+ MB\n",
      "None\n",
      "       encounter_id   patient_nbr  admission_type_id  \\\n",
      "count  1.017660e+05  1.017660e+05      101766.000000   \n",
      "mean   1.652016e+08  5.433040e+07           2.024006   \n",
      "std    1.026403e+08  3.869636e+07           1.445403   \n",
      "min    1.252200e+04  1.350000e+02           1.000000   \n",
      "25%    8.496119e+07  2.341322e+07           1.000000   \n",
      "50%    1.523890e+08  4.550514e+07           1.000000   \n",
      "75%    2.302709e+08  8.754595e+07           3.000000   \n",
      "max    4.438672e+08  1.895026e+08           8.000000   \n",
      "\n",
      "       discharge_disposition_id  admission_source_id  time_in_hospital  \\\n",
      "count             101766.000000        101766.000000     101766.000000   \n",
      "mean                   3.715642             5.754437          4.395987   \n",
      "std                    5.280166             4.064081          2.985108   \n",
      "min                    1.000000             1.000000          1.000000   \n",
      "25%                    1.000000             1.000000          2.000000   \n",
      "50%                    1.000000             7.000000          4.000000   \n",
      "75%                    4.000000             7.000000          6.000000   \n",
      "max                   28.000000            25.000000         14.000000   \n",
      "\n",
      "       num_lab_procedures  num_procedures  num_medications  number_outpatient  \\\n",
      "count       101766.000000   101766.000000    101766.000000      101766.000000   \n",
      "mean            43.095641        1.339730        16.021844           0.369357   \n",
      "std             19.674362        1.705807         8.127566           1.267265   \n",
      "min              1.000000        0.000000         1.000000           0.000000   \n",
      "25%             31.000000        0.000000        10.000000           0.000000   \n",
      "50%             44.000000        1.000000        15.000000           0.000000   \n",
      "75%             57.000000        2.000000        20.000000           0.000000   \n",
      "max            132.000000        6.000000        81.000000          42.000000   \n",
      "\n",
      "       number_emergency  number_inpatient  number_diagnoses  \n",
      "count     101766.000000     101766.000000     101766.000000  \n",
      "mean           0.197836          0.635566          7.422607  \n",
      "std            0.930472          1.262863          1.933600  \n",
      "min            0.000000          0.000000          1.000000  \n",
      "25%            0.000000          0.000000          6.000000  \n",
      "50%            0.000000          0.000000          8.000000  \n",
      "75%            0.000000          1.000000          9.000000  \n",
      "max           76.000000         21.000000         16.000000  \n"
     ]
    }
   ],
   "source": [
    "# examinar os tipos de dados e estatísticas descritivas \n",
    "print (df.info ()) \n",
    "print (df.describe ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35754</td>\n",
       "      <td>82637451</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[50-60)</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55842</td>\n",
       "      <td>84259809</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[60-70)</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63768</td>\n",
       "      <td>114882984</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12522</td>\n",
       "      <td>48330783</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15738</td>\n",
       "      <td>63555939</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[90-100)</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  patient_nbr             race  gender       age weight  \\\n",
       "0       2278392      8222157        Caucasian  Female    [0-10)      ?   \n",
       "1        149190     55629189        Caucasian  Female   [10-20)      ?   \n",
       "2         64410     86047875  AfricanAmerican  Female   [20-30)      ?   \n",
       "3        500364     82442376        Caucasian    Male   [30-40)      ?   \n",
       "4         16680     42519267        Caucasian    Male   [40-50)      ?   \n",
       "5         35754     82637451        Caucasian    Male   [50-60)      ?   \n",
       "6         55842     84259809        Caucasian    Male   [60-70)      ?   \n",
       "7         63768    114882984        Caucasian    Male   [70-80)      ?   \n",
       "8         12522     48330783        Caucasian  Female   [80-90)      ?   \n",
       "9         15738     63555939        Caucasian  Female  [90-100)      ?   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "5                  2                         1                    2   \n",
       "6                  3                         1                    2   \n",
       "7                  1                         1                    7   \n",
       "8                  2                         1                    4   \n",
       "9                  3                         3                    4   \n",
       "\n",
       "   time_in_hospital    ...     citoglipton insulin  glyburide-metformin  \\\n",
       "0                 1    ...              No      No                   No   \n",
       "1                 3    ...              No      Up                   No   \n",
       "2                 2    ...              No      No                   No   \n",
       "3                 2    ...              No      Up                   No   \n",
       "4                 1    ...              No  Steady                   No   \n",
       "5                 3    ...              No  Steady                   No   \n",
       "6                 4    ...              No  Steady                   No   \n",
       "7                 5    ...              No      No                   No   \n",
       "8                13    ...              No  Steady                   No   \n",
       "9                12    ...              No  Steady                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "5                   No                        No                       No   \n",
       "6                   No                        No                       No   \n",
       "7                   No                        No                       No   \n",
       "8                   No                        No                       No   \n",
       "9                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmitted  \n",
       "0                      No      No          No         NO  \n",
       "1                      No      Ch         Yes        >30  \n",
       "2                      No      No         Yes         NO  \n",
       "3                      No      Ch         Yes         NO  \n",
       "4                      No      Ch         Yes         NO  \n",
       "5                      No      No         Yes        >30  \n",
       "6                      No      Ch         Yes         NO  \n",
       "7                      No      No         Yes        >30  \n",
       "8                      No      Ch         Yes         NO  \n",
       "9                      No      Ch         Yes         NO  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando as primeiras 10 linhas do dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race 2273  Correspondendo a  2.23 % das observações\n",
      "weight 98569  Correspondendo a  96.86 % das observações\n",
      "payer_code 40256  Correspondendo a  39.56 % das observações\n",
      "medical_specialty 49949  Correspondendo a  49.08 % das observações\n",
      "diag_1 21  Correspondendo a  0.02 % das observações\n",
      "diag_2 358  Correspondendo a  0.35 % das observações\n",
      "diag_3 1423  Correspondendo a  1.4 % das observações\n"
     ]
    }
   ],
   "source": [
    "# Verificando a existencia de dados missing (dados faltantes)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        if df[col][df[col] == '?'].count() > 0:\n",
    "            print(col,df[col][df[col] == '?'].count(),' Correspondendo a ',np.around((df[col][df[col] == '?'].count()/df[col].count())*100,2), '% das observações')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "?                   2273\n",
      "AfricanAmerican    19210\n",
      "Asian                641\n",
      "Caucasian          76099\n",
      "Hispanic            2037\n",
      "Other               1506\n",
      "Name: race, dtype: int64\n",
      "\n",
      "weight\n",
      ">200             3\n",
      "?            98569\n",
      "[0-25)          48\n",
      "[100-125)      625\n",
      "[125-150)      145\n",
      "[150-175)       35\n",
      "[175-200)       11\n",
      "[25-50)         97\n",
      "[50-75)        897\n",
      "[75-100)      1336\n",
      "Name: weight, dtype: int64\n",
      "\n",
      "payer_code\n",
      "?     40256\n",
      "BC     4655\n",
      "CH      146\n",
      "CM     1937\n",
      "CP     2533\n",
      "DM      549\n",
      "FR        1\n",
      "HM     6274\n",
      "MC    32439\n",
      "MD     3532\n",
      "MP       79\n",
      "OG     1033\n",
      "OT       95\n",
      "PO      592\n",
      "SI       55\n",
      "SP     5007\n",
      "UN     2448\n",
      "WC      135\n",
      "Name: payer_code, dtype: int64\n",
      "\n",
      "medical_specialty\n",
      "?                                       49949\n",
      "AllergyandImmunology                        7\n",
      "Anesthesiology                             12\n",
      "Anesthesiology-Pediatric                   19\n",
      "Cardiology                               5352\n",
      "Cardiology-Pediatric                        7\n",
      "DCPTEAM                                     6\n",
      "Dentistry                                   4\n",
      "Dermatology                                 1\n",
      "Emergency/Trauma                         7565\n",
      "Endocrinology                             120\n",
      "Endocrinology-Metabolism                    8\n",
      "Family/GeneralPractice                   7440\n",
      "Gastroenterology                          564\n",
      "Gynecology                                 58\n",
      "Hematology                                 82\n",
      "Hematology/Oncology                       207\n",
      "Hospitalist                                57\n",
      "InfectiousDiseases                         37\n",
      "InternalMedicine                        14635\n",
      "Nephrology                               1613\n",
      "Neurology                                 203\n",
      "Neurophysiology                             1\n",
      "Obsterics&Gynecology-GynecologicOnco       25\n",
      "Obstetrics                                 19\n",
      "ObstetricsandGynecology                   671\n",
      "Oncology                                  348\n",
      "Ophthalmology                              38\n",
      "Orthopedics                              1400\n",
      "Orthopedics-Reconstructive               1233\n",
      "                                        ...  \n",
      "Perinatology                                1\n",
      "PhysicalMedicineandRehabilitation         391\n",
      "PhysicianNotFound                          11\n",
      "Podiatry                                  100\n",
      "Proctology                                  1\n",
      "Psychiatry                                854\n",
      "Psychiatry-Addictive                        1\n",
      "Psychiatry-Child/Adolescent                 7\n",
      "Psychology                                101\n",
      "Pulmonology                               871\n",
      "Radiologist                              1140\n",
      "Radiology                                  53\n",
      "Resident                                    2\n",
      "Rheumatology                               17\n",
      "Speech                                      1\n",
      "SportsMedicine                              1\n",
      "Surgeon                                    45\n",
      "Surgery-Cardiovascular                     98\n",
      "Surgery-Cardiovascular/Thoracic           652\n",
      "Surgery-Colon&Rectal                       11\n",
      "Surgery-General                          3099\n",
      "Surgery-Maxillofacial                      11\n",
      "Surgery-Neuro                             468\n",
      "Surgery-Pediatric                           8\n",
      "Surgery-Plastic                            41\n",
      "Surgery-PlasticwithinHeadandNeck            1\n",
      "Surgery-Thoracic                          109\n",
      "Surgery-Vascular                          533\n",
      "SurgicalSpecialty                          33\n",
      "Urology                                   685\n",
      "Name: medical_specialty, Length: 73, dtype: int64\n",
      "\n",
      "diag_1\n",
      "10         1\n",
      "11        10\n",
      "110        2\n",
      "112       73\n",
      "114        1\n",
      "115        2\n",
      "117        9\n",
      "131        2\n",
      "133        1\n",
      "135       27\n",
      "136        5\n",
      "141        6\n",
      "142        4\n",
      "143        1\n",
      "145        1\n",
      "146        4\n",
      "147        2\n",
      "148        1\n",
      "149        2\n",
      "150       27\n",
      "151       62\n",
      "152       15\n",
      "153      311\n",
      "154       94\n",
      "155       54\n",
      "156       18\n",
      "157      114\n",
      "158       12\n",
      "160        1\n",
      "161       12\n",
      "        ... \n",
      "989       11\n",
      "990        2\n",
      "991        7\n",
      "992        9\n",
      "994        1\n",
      "995       78\n",
      "996     1967\n",
      "997      424\n",
      "998      784\n",
      "999       29\n",
      "?         21\n",
      "E909       1\n",
      "V07        1\n",
      "V25        1\n",
      "V26        2\n",
      "V43        1\n",
      "V45        5\n",
      "V51        1\n",
      "V53       44\n",
      "V54       45\n",
      "V55       71\n",
      "V56       16\n",
      "V57     1207\n",
      "V58      228\n",
      "V60        1\n",
      "V63        8\n",
      "V66        2\n",
      "V67        1\n",
      "V70        1\n",
      "V71        9\n",
      "Name: diag_1, Length: 717, dtype: int64\n",
      "\n",
      "diag_2\n",
      "11       3\n",
      "110      8\n",
      "111      1\n",
      "112    201\n",
      "114      1\n",
      "115      1\n",
      "117     10\n",
      "123      1\n",
      "130      1\n",
      "131      3\n",
      "135    115\n",
      "136      6\n",
      "137      1\n",
      "138     22\n",
      "140      1\n",
      "141      2\n",
      "145      1\n",
      "150     35\n",
      "151     24\n",
      "152     13\n",
      "153    107\n",
      "154     45\n",
      "155     33\n",
      "156     16\n",
      "157     85\n",
      "162    401\n",
      "163      1\n",
      "164      1\n",
      "171      2\n",
      "172      8\n",
      "      ... \n",
      "V15    128\n",
      "V16      2\n",
      "V17      9\n",
      "V18      8\n",
      "V23      3\n",
      "V25      1\n",
      "V42    264\n",
      "V43    130\n",
      "V44     11\n",
      "V45    408\n",
      "V46     11\n",
      "V49     39\n",
      "V50      1\n",
      "V53      2\n",
      "V54     78\n",
      "V55      2\n",
      "V57      9\n",
      "V58    157\n",
      "V60      1\n",
      "V61      2\n",
      "V62     66\n",
      "V63     30\n",
      "V64     27\n",
      "V65     23\n",
      "V66      5\n",
      "V69      1\n",
      "V70      7\n",
      "V72     13\n",
      "V85    169\n",
      "V86      2\n",
      "Name: diag_2, Length: 749, dtype: int64\n",
      "\n",
      "diag_3\n",
      "11        2\n",
      "110      20\n",
      "111       1\n",
      "112     206\n",
      "115       1\n",
      "117       7\n",
      "122       1\n",
      "123       1\n",
      "131       6\n",
      "132       3\n",
      "135      64\n",
      "136       2\n",
      "138      21\n",
      "139       2\n",
      "14        1\n",
      "141       4\n",
      "146       1\n",
      "148       1\n",
      "150      13\n",
      "151      10\n",
      "152       3\n",
      "153      55\n",
      "154      13\n",
      "155      12\n",
      "156       4\n",
      "157      38\n",
      "158       1\n",
      "161       3\n",
      "162     181\n",
      "163       5\n",
      "       ... \n",
      "V15     334\n",
      "V16       7\n",
      "V17      22\n",
      "V18      15\n",
      "V22       1\n",
      "V23       4\n",
      "V25       2\n",
      "V27      37\n",
      "V42     243\n",
      "V43     211\n",
      "V44      27\n",
      "V45    1389\n",
      "V46      60\n",
      "V49      54\n",
      "V53       7\n",
      "V54      58\n",
      "V55       8\n",
      "V57       7\n",
      "V58     501\n",
      "V60       4\n",
      "V61       5\n",
      "V62      29\n",
      "V63      13\n",
      "V64      45\n",
      "V65      24\n",
      "V66      18\n",
      "V70       2\n",
      "V72       8\n",
      "V85      96\n",
      "V86       3\n",
      "Name: diag_3, Length: 790, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avaliando a distribuição dos dados em cada atributo\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        if df[col][df[col] == '?'].count() != 0:       \n",
    "            print(df.groupby([col])[col].count())\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "Female             54708\n",
      "Male               47055\n",
      "Unknown/Invalid        3\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "age\n",
      "[0-10)        161\n",
      "[10-20)       691\n",
      "[20-30)      1657\n",
      "[30-40)      3775\n",
      "[40-50)      9685\n",
      "[50-60)     17256\n",
      "[60-70)     22483\n",
      "[70-80)     26068\n",
      "[80-90)     17197\n",
      "[90-100)     2793\n",
      "Name: age, dtype: int64\n",
      "\n",
      "max_glu_serum\n",
      ">200     1485\n",
      ">300     1264\n",
      "None    96420\n",
      "Norm     2597\n",
      "Name: max_glu_serum, dtype: int64\n",
      "\n",
      "A1Cresult\n",
      ">7       3812\n",
      ">8       8216\n",
      "None    84748\n",
      "Norm     4990\n",
      "Name: A1Cresult, dtype: int64\n",
      "\n",
      "metformin\n",
      "Down        575\n",
      "No        81778\n",
      "Steady    18346\n",
      "Up         1067\n",
      "Name: metformin, dtype: int64\n",
      "\n",
      "repaglinide\n",
      "Down          45\n",
      "No        100227\n",
      "Steady      1384\n",
      "Up           110\n",
      "Name: repaglinide, dtype: int64\n",
      "\n",
      "nateglinide\n",
      "Down          11\n",
      "No        101063\n",
      "Steady       668\n",
      "Up            24\n",
      "Name: nateglinide, dtype: int64\n",
      "\n",
      "chlorpropamide\n",
      "Down           1\n",
      "No        101680\n",
      "Steady        79\n",
      "Up             6\n",
      "Name: chlorpropamide, dtype: int64\n",
      "\n",
      "glimepiride\n",
      "Down        194\n",
      "No        96575\n",
      "Steady     4670\n",
      "Up          327\n",
      "Name: glimepiride, dtype: int64\n",
      "\n",
      "acetohexamide\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: acetohexamide, dtype: int64\n",
      "\n",
      "glipizide\n",
      "Down        560\n",
      "No        89080\n",
      "Steady    11356\n",
      "Up          770\n",
      "Name: glipizide, dtype: int64\n",
      "\n",
      "glyburide\n",
      "Down        564\n",
      "No        91116\n",
      "Steady     9274\n",
      "Up          812\n",
      "Name: glyburide, dtype: int64\n",
      "\n",
      "tolbutamide\n",
      "No        101743\n",
      "Steady        23\n",
      "Name: tolbutamide, dtype: int64\n",
      "\n",
      "pioglitazone\n",
      "Down        118\n",
      "No        94438\n",
      "Steady     6976\n",
      "Up          234\n",
      "Name: pioglitazone, dtype: int64\n",
      "\n",
      "rosiglitazone\n",
      "Down         87\n",
      "No        95401\n",
      "Steady     6100\n",
      "Up          178\n",
      "Name: rosiglitazone, dtype: int64\n",
      "\n",
      "acarbose\n",
      "Down           3\n",
      "No        101458\n",
      "Steady       295\n",
      "Up            10\n",
      "Name: acarbose, dtype: int64\n",
      "\n",
      "miglitol\n",
      "Down           5\n",
      "No        101728\n",
      "Steady        31\n",
      "Up             2\n",
      "Name: miglitol, dtype: int64\n",
      "\n",
      "troglitazone\n",
      "No        101763\n",
      "Steady         3\n",
      "Name: troglitazone, dtype: int64\n",
      "\n",
      "tolazamide\n",
      "No        101727\n",
      "Steady        38\n",
      "Up             1\n",
      "Name: tolazamide, dtype: int64\n",
      "\n",
      "examide\n",
      "No    101766\n",
      "Name: examide, dtype: int64\n",
      "\n",
      "citoglipton\n",
      "No    101766\n",
      "Name: citoglipton, dtype: int64\n",
      "\n",
      "insulin\n",
      "Down      12218\n",
      "No        47383\n",
      "Steady    30849\n",
      "Up        11316\n",
      "Name: insulin, dtype: int64\n",
      "\n",
      "glyburide-metformin\n",
      "Down           6\n",
      "No        101060\n",
      "Steady       692\n",
      "Up             8\n",
      "Name: glyburide-metformin, dtype: int64\n",
      "\n",
      "glipizide-metformin\n",
      "No        101753\n",
      "Steady        13\n",
      "Name: glipizide-metformin, dtype: int64\n",
      "\n",
      "glimepiride-pioglitazone\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: glimepiride-pioglitazone, dtype: int64\n",
      "\n",
      "metformin-rosiglitazone\n",
      "No        101764\n",
      "Steady         2\n",
      "Name: metformin-rosiglitazone, dtype: int64\n",
      "\n",
      "metformin-pioglitazone\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: metformin-pioglitazone, dtype: int64\n",
      "\n",
      "change\n",
      "Ch    47011\n",
      "No    54755\n",
      "Name: change, dtype: int64\n",
      "\n",
      "diabetesMed\n",
      "No     23403\n",
      "Yes    78363\n",
      "Name: diabetesMed, dtype: int64\n",
      "\n",
      "readmitted\n",
      "<30    11357\n",
      ">30    35545\n",
      "NO     54864\n",
      "Name: readmitted, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avaliando a distribuição dos dados em cada atributo\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        if df[col][df[col] == '?'].count() == 0:       \n",
    "            print(df.groupby([col])[col].count())\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encounter_id 152388987.0\n",
      "\n",
      "patient_nbr 45505143.0\n",
      "\n",
      "admission_type_id 1.0\n",
      "\n",
      "discharge_disposition_id 1.0\n",
      "\n",
      "admission_source_id 7.0\n",
      "\n",
      "time_in_hospital 4.0\n",
      "\n",
      "num_lab_procedures 44.0\n",
      "\n",
      "num_procedures 1.0\n",
      "\n",
      "num_medications 15.0\n",
      "\n",
      "number_outpatient 0.0\n",
      "\n",
      "number_emergency 0.0\n",
      "\n",
      "number_inpatient 0.0\n",
      "\n",
      "number_diagnoses 8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando a mediana\n",
    "for col in df.columns:\n",
    "    if df[col].dtype != object:\n",
    "        print(col, df[col].median())\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza e Transformação dos dados\n",
    "\n",
    "Os critérios de eliminação de atributos e observações depende muito da interpretação dos dados feita pelo Cientista de Dados na fase de exploração. Estes critérios passam pela avaliação de cada atributo do conjunto de dados, verificação da distribuição de frequência, analise de correlações entre variáveis, modelo preditivo que será aplicado, além de um certo conhecimento do negócio em estudo, para então decidir quais atributos e/ou observações devem ser descartados. Considerando o conjunto de dados em questão, decidi eliminar alguns atributos nos quais avalio que não impactará no resultado das análises preditivas. Descartarei os atributos \"encounter_id\", \"patient_nbr\", \"weight\", \"payer_code\", \"examide\", \"citoglipton\" e \"medical_specialty\". Por exemplo o atributo \"weight\", que corresponde ao peso do paciente, seria um atributo muito importante a ser considerado na análise, porém em 97% das observação este atributo está sem valor, tornando-se um dado insuficientemente consistente para o modelo aplicar algum tipo de generalização.\n",
    "\n",
    "Com base na consulta a documentação disponibilizada pelo repositório dos dados e entendimento de cada atributo do conjunto de dados, decidi também eliminar algumas observações que acredito não impactar no objetivo proposta neste trabalho. Precisarei também transformar alguns dados, com o objetivo de prepará-los para serem entregue ao modelo preditivo proposto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando colunas que não serão utilizadas\n",
    "df.drop(['encounter_id', 'patient_nbr', 'weight', 'payer_code', 'examide', 'citoglipton', 'medical_specialty'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorização de diagnósticos\n",
    "No conjunto de dados existem três diagnosticos, um principal e dois secundários, contendo em média 752 codigos distintos em cada um, por isso resolvi realizar um reagrupamento com base numa análise realizada por Strack et al. em 2014, sobre o mesmo tema e utilizando o mesmo conjunto de dados, publicado em ( https://www.hindawi.com/journals/bmri/2014/781670/abs/ ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('Agrupamento_CID_9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando novas colunas para atribuir os valores transformados\n",
    "df['d1'] = df['diag_1']\n",
    "df['d2'] = df['diag_2']\n",
    "df['d3'] = df['diag_3']\n",
    "df['classe'] = -1\n",
    "df['change_t'] = -1\n",
    "df['gender_t'] = -1\n",
    "df['diabetesMed_t'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reagrupamento do diagnóstico principal\n",
    "df['d1'] = df.apply(lambda row: 1 if (row['diag_1'][0:3].zfill(3) >= '390') and (row['diag_1'][0:3].zfill(3) <= '459' ) or  (row['diag_1'][0:3].zfill(3) == '785' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 2 if (row['diag_1'][0:3].zfill(3) >= '460') and (row['diag_1'][0:3].zfill(3) <= '519' ) or  (row['diag_1'][0:3].zfill(3) == '786' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 3 if (row['diag_1'][0:3].zfill(3) >= '520') and (row['diag_1'][0:3].zfill(3) <= '579' ) or  (row['diag_1'][0:3].zfill(3) == '787' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 4 if (row['diag_1'][0:3].zfill(3) == '250') else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 5 if (row['diag_1'][0:3].zfill(3) >= '800') and (row['diag_1'][0:3].zfill(3) <= '999' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 6 if (row['diag_1'][0:3].zfill(3) >= '710') and (row['diag_1'][0:3].zfill(3) <= '739' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 7 if (row['diag_1'][0:3].zfill(3) >= '580') and (row['diag_1'][0:3].zfill(3) <= '629' ) or  (row['diag_1'][0:3].zfill(3) == '788' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 8 if (row['diag_1'][0:3].zfill(3) >= '140') and (row['diag_1'][0:3].zfill(3) <= '239' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 9 if (row['diag_1'][0:3].zfill(3) >= '790') and (row['diag_1'][0:3].zfill(3) <= '799' ) or  (row['diag_1'][0:3].zfill(3) == '780' ) or  (row['diag_1'][0:3].zfill(3) == '781' ) or  (row['diag_1'][0:3].zfill(3) == '784' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '240') and (row['diag_1'][0:3].zfill(3) <= '249' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '251') and (row['diag_1'][0:3].zfill(3) <= '279' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 11 if (row['diag_1'][0:3].zfill(3) >= '680') and (row['diag_1'][0:3].zfill(3) <= '709' ) or  (row['diag_1'][0:3].zfill(3) == '782' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 12 if (row['diag_1'][0:3].zfill(3) >= '001') and (row['diag_1'][0:3].zfill(3) <= '139' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '290') and (row['diag_1'][0:3].zfill(3) <= '319' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:1] >= 'E') and (row['diag_1'][0:1] <= 'V' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '280') and (row['diag_1'][0:3].zfill(3) <= '289' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '320') and (row['diag_1'][0:3].zfill(3) <= '359' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '630') and (row['diag_1'][0:3].zfill(3) <= '679' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '360') and (row['diag_1'][0:3].zfill(3) <= '389' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '740') and (row['diag_1'][0:3].zfill(3) <= '759' ) else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: 0 if (row['diag_1'][0:3].zfill(3)  == '783' or row['diag_1'][0:3].zfill(3)  == '789') else row['d1'], axis=1)\n",
    "df['d1'] = df.apply(lambda row: -1 if (row['diag_1'][0:1] == '?') else row['d1'], axis=1)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reagrupamento do primeiro diagnóstico secundário\n",
    "df['d2'] = df.apply(lambda row: 1 if (row['diag_2'][0:3].zfill(3) >= '390') and (row['diag_2'][0:3].zfill(3) <= '459' ) or  (row['diag_2'][0:3].zfill(3) == '785' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 2 if (row['diag_2'][0:3].zfill(3) >= '460') and (row['diag_2'][0:3].zfill(3) <= '519' ) or  (row['diag_2'][0:3].zfill(3) == '786' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 3 if (row['diag_2'][0:3].zfill(3) >= '520') and (row['diag_2'][0:3].zfill(3) <= '579' ) or  (row['diag_2'][0:3].zfill(3) == '787' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 4 if (row['diag_2'][0:3].zfill(3) == '250') else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 5 if (row['diag_2'][0:3].zfill(3) >= '800') and (row['diag_2'][0:3].zfill(3) <= '999' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 6 if (row['diag_2'][0:3].zfill(3) >= '710') and (row['diag_2'][0:3].zfill(3) <= '739' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 7 if (row['diag_2'][0:3].zfill(3) >= '580') and (row['diag_2'][0:3].zfill(3) <= '629' ) or  (row['diag_2'][0:3].zfill(3) == '788' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 8 if (row['diag_2'][0:3].zfill(3) >= '140') and (row['diag_2'][0:3].zfill(3) <= '239' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 9 if (row['diag_2'][0:3].zfill(3) >= '790') and (row['diag_2'][0:3].zfill(3) <= '799' ) or  (row['diag_2'][0:3].zfill(3) == '780' ) or  (row['diag_2'][0:3].zfill(3) == '781' ) or  (row['diag_2'][0:3].zfill(3) == '784' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '240') and (row['diag_2'][0:3].zfill(3) <= '249' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '251') and (row['diag_2'][0:3].zfill(3) <= '279' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 11 if (row['diag_2'][0:3].zfill(3) >= '680') and (row['diag_2'][0:3].zfill(3) <= '709' ) or  (row['diag_2'][0:3].zfill(3) == '782' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 12 if (row['diag_2'][0:3].zfill(3) >= '001') and (row['diag_2'][0:3].zfill(3) <= '139' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '290') and (row['diag_2'][0:3].zfill(3) <= '319' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:1] >= 'E') and (row['diag_2'][0:1] <= 'V' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '280') and (row['diag_2'][0:3].zfill(3) <= '289' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '320') and (row['diag_2'][0:3].zfill(3) <= '359' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '630') and (row['diag_2'][0:3].zfill(3) <= '679' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '360') and (row['diag_2'][0:3].zfill(3) <= '389' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '740') and (row['diag_2'][0:3].zfill(3) <= '759' ) else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: 0 if (row['diag_2'][0:3].zfill(3)  == '783' or row['diag_2'][0:3].zfill(3)  == '789') else row['d2'], axis=1)\n",
    "df['d2'] = df.apply(lambda row: -1 if (row['diag_2'][0:1] == '?') else row['d2'], axis=1)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reagrupamento do segundo diagnóstico secundário\n",
    "df['d3'] = df.apply(lambda row: 1 if (row['diag_3'][0:3].zfill(3) >= '390') and (row['diag_3'][0:3].zfill(3) <= '459' ) or  (row['diag_3'][0:3].zfill(3) == '785' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 2 if (row['diag_3'][0:3].zfill(3) >= '460') and (row['diag_3'][0:3].zfill(3) <= '519' ) or  (row['diag_3'][0:3].zfill(3) == '786' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 3 if (row['diag_3'][0:3].zfill(3) >= '520') and (row['diag_3'][0:3].zfill(3) <= '579' ) or  (row['diag_3'][0:3].zfill(3) == '787' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 4 if (row['diag_3'][0:3].zfill(3) == '250') else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 5 if (row['diag_3'][0:3].zfill(3) >= '800') and (row['diag_3'][0:3].zfill(3) <= '999' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 6 if (row['diag_3'][0:3].zfill(3) >= '710') and (row['diag_3'][0:3].zfill(3) <= '739' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 7 if (row['diag_3'][0:3].zfill(3) .zfill(3)>= '580') and (row['diag_3'][0:3].zfill(3) <= '629' ) or  (row['diag_3'][0:3].zfill(3) == '788' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 8 if (row['diag_3'][0:3].zfill(3) >= '140') and (row['diag_3'][0:3].zfill(3) <= '239' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 9 if (row['diag_3'][0:3].zfill(3) >= '790') and (row['diag_3'][0:3].zfill(3) <= '799' ) or  (row['diag_3'][0:3].zfill(3) == '780' ) or  (row['diag_3'][0:3].zfill(3) == '781' ) or  (row['diag_3'][0:3].zfill(3) == '784' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '240') and (row['diag_3'][0:3].zfill(3) <= '249' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '251') and (row['diag_3'][0:3].zfill(3) <= '279' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 11 if (row['diag_3'][0:3].zfill(3) >= '680') and (row['diag_3'][0:3].zfill(3) <= '709' ) or  (row['diag_3'][0:3].zfill(3) == '782' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 12 if (row['diag_3'][0:3].zfill(3) >= '001') and (row['diag_3'][0:3].zfill(3) <= '139' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '290') and (row['diag_3'][0:3].zfill(3) <= '319' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:1] >= 'E') and (row['diag_3'][0:1] <= 'V' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '280') and (row['diag_3'][0:3].zfill(3) <= '289' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '320') and (row['diag_3'][0:3].zfill(3) <= '359' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '630') and (row['diag_3'][0:3].zfill(3) <= '679' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '360') and (row['diag_3'][0:3].zfill(3) <= '389' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '740') and (row['diag_3'][0:3].zfill(3) <= '759' ) else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: 0 if (row['diag_3'][0:3].zfill(3)  == '783' or row['diag_3'][0:3].zfill(3)  == '789') else row['d3'], axis=1)\n",
    "df['d3'] = df.apply(lambda row: -1 if (row['diag_3'][0:1] == '?') else row['d3'], axis=1)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1   diag_1\n",
      "-1   ?           21\n",
      " 0   783         29\n",
      "     789        561\n",
      " 1   391          1\n",
      "     394         11\n",
      "     395          5\n",
      "     396         30\n",
      "     397          5\n",
      "     398        128\n",
      "     401        346\n",
      "     402        449\n",
      "     403        513\n",
      "     404        262\n",
      "     405          3\n",
      "     410       3614\n",
      "     411        254\n",
      "     412          1\n",
      "     413        117\n",
      "     414       6581\n",
      "     415        449\n",
      "     416         68\n",
      "     417          2\n",
      "     420         64\n",
      "     421         41\n",
      "     422          2\n",
      "     423         72\n",
      "     424        183\n",
      "     425         86\n",
      "     426        288\n",
      "     427       2766\n",
      "               ... \n",
      " 13  665          3\n",
      "     669          3\n",
      "     671          1\n",
      "     674          4\n",
      "     745          7\n",
      "     746          7\n",
      "     747          7\n",
      "     751          5\n",
      "     753          5\n",
      "     756         15\n",
      "     759          5\n",
      "     E909         1\n",
      "     V07          1\n",
      "     V25          1\n",
      "     V26          2\n",
      "     V43          1\n",
      "     V45          5\n",
      "     V51          1\n",
      "     V53         44\n",
      "     V54         45\n",
      "     V55         71\n",
      "     V56         16\n",
      "     V57       1207\n",
      "     V58        228\n",
      "     V60          1\n",
      "     V63          8\n",
      "     V66          2\n",
      "     V67          1\n",
      "     V70          1\n",
      "     V71          9\n",
      "Name: d2, Length: 717, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['d1', 'diag_1']).d2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2   diag_2\n",
      "-1   ?          358\n",
      " 0   783         24\n",
      "     789        343\n",
      " 1   394         21\n",
      "     395          3\n",
      "     396        179\n",
      "     397         68\n",
      "     398         39\n",
      "     401       3736\n",
      "     402        335\n",
      "     403       2823\n",
      "     404        203\n",
      "     405          3\n",
      "     410        549\n",
      "     411       2566\n",
      "     412        102\n",
      "     413       1042\n",
      "     414       2650\n",
      "     415        107\n",
      "     416         96\n",
      "     420         25\n",
      "     421         21\n",
      "     422          4\n",
      "     423         16\n",
      "     424       1071\n",
      "     425       1434\n",
      "     426        301\n",
      "     427       5036\n",
      "     428       6662\n",
      "     429         32\n",
      "               ... \n",
      " 13  V15        128\n",
      "     V16          2\n",
      "     V17          9\n",
      "     V18          8\n",
      "     V23          3\n",
      "     V25          1\n",
      "     V42        264\n",
      "     V43        130\n",
      "     V44         11\n",
      "     V45        408\n",
      "     V46         11\n",
      "     V49         39\n",
      "     V50          1\n",
      "     V53          2\n",
      "     V54         78\n",
      "     V55          2\n",
      "     V57          9\n",
      "     V58        157\n",
      "     V60          1\n",
      "     V61          2\n",
      "     V62         66\n",
      "     V63         30\n",
      "     V64         27\n",
      "     V65         23\n",
      "     V66          5\n",
      "     V69          1\n",
      "     V70          7\n",
      "     V72         13\n",
      "     V85        169\n",
      "     V86          2\n",
      "Name: d2, Length: 749, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['d2', 'diag_2']).d2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d3   diag_3\n",
      "-1   ?         1423\n",
      " 0   783         57\n",
      "     789        290\n",
      " 1   391          1\n",
      "     394         17\n",
      "     395          1\n",
      "     396        229\n",
      "     397        150\n",
      "     398         28\n",
      "     401       8289\n",
      "     402        375\n",
      "     403       2357\n",
      "     404        124\n",
      "     405          9\n",
      "     410        209\n",
      "     411        399\n",
      "     412        346\n",
      "     413        329\n",
      "     414       3664\n",
      "     415         33\n",
      "     416        170\n",
      "     417          3\n",
      "     420         10\n",
      "     421          6\n",
      "     423         25\n",
      "     424       1063\n",
      "     425       1136\n",
      "     426        272\n",
      "     427       3955\n",
      "     428       4577\n",
      "               ... \n",
      " 13  V15        334\n",
      "     V16          7\n",
      "     V17         22\n",
      "     V18         15\n",
      "     V22          1\n",
      "     V23          4\n",
      "     V25          2\n",
      "     V27         37\n",
      "     V42        243\n",
      "     V43        211\n",
      "     V44         27\n",
      "     V45       1389\n",
      "     V46         60\n",
      "     V49         54\n",
      "     V53          7\n",
      "     V54         58\n",
      "     V55          8\n",
      "     V57          7\n",
      "     V58        501\n",
      "     V60          4\n",
      "     V61          5\n",
      "     V62         29\n",
      "     V63         13\n",
      "     V64         45\n",
      "     V65         24\n",
      "     V66         18\n",
      "     V70          2\n",
      "     V72          8\n",
      "     V85         96\n",
      "     V86          3\n",
      "Name: d3, Length: 790, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['d3', 'diag_3']).d3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme consta na documentação, trata-se de um do conjunto de dados contendo atendimentos onde qualquer tipo de diabetes foi introduzido no sistema como um diagnóstico, então eliminarei as observações onde não existe nenhum diagnóstico registrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.d1 > -1) | (df.d2 > -1) | (df.d3 > -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando as colunas originais do diagnósticos\n",
    "df.drop(['diag_1'], axis = 1, inplace = True)\n",
    "df.drop(['diag_2'], axis = 1, inplace = True)\n",
    "df.drop(['diag_3'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando a transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribui a classe os valores 1 ou 0, correspondendo a ocorrencias de readmissão em menos de 30 dias ou não \n",
    "df['classe'] = df.apply(lambda row: 0 if (row['readmitted'][0:3] == '>30' or row['readmitted'][0:2] == 'NO') else row['classe'], axis=1) \n",
    "df['classe'] = df.apply(lambda row: 1 if (row['readmitted'][0:3] == '<30') else row['classe'], axis=1)\n",
    "df.drop(['readmitted'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['change_t'] = df.apply(lambda row: 1 if (row['change'] == 'Ch') else -1, axis=1)\n",
    "df['change_t'] = df.apply(lambda row: 0 if (row['change'] == 'No') else row['change_t'], axis=1)\n",
    "df.drop(['change'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender_t'] = df.apply(lambda row: 1 if (row['gender'] == 'Male') else -1, axis=1)\n",
    "df['gender_t'] = df.apply(lambda row: 0 if (row['gender'] == 'Female') else row['gender_t'], axis=1)\n",
    "df.drop(['gender'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diabetesMed_t'] = df.apply(lambda row: 1 if (row['diabetesMed'] == 'Yes') else -1, axis=1)\n",
    "df['diabetesMed_t'] = df.apply(lambda row: 0 if (row['diabetesMed'] == 'No') else row['diabetesMed_t'], axis=1)\n",
    "df.drop(['diabetesMed'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "medicacoes = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', \n",
    "              'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', \n",
    "              'metformin-pioglitazone','metformin-rosiglitazone', 'glimepiride-pioglitazone', \n",
    "              'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\n",
    "for col in df.columns:\n",
    "    if col in medicacoes:       \n",
    "        colname = 'Med' + str(m) + '_t'\n",
    "        df[colname] = df.apply(lambda row: 0 if (row[col] == 'No') else 1, axis=1)\n",
    "        df.drop([col], axis = 1, inplace = True)\n",
    "        m = m + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A1Cresult_t'] = df.apply(lambda row: 0 if (row['A1Cresult'][0:4] == 'Norm') else -1, axis=1) \n",
    "df['A1Cresult_t'] = df.apply(lambda row: 1 if (row['A1Cresult'][0:2] == '>7' or row['A1Cresult'][0:2] == '>8') else row['A1Cresult_t'], axis=1) \n",
    "df.drop(['A1Cresult'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_glu_serum_t'] = df.apply(lambda row: 0 if (row['max_glu_serum'][0:4] == 'Norm') else -1, axis=1) \n",
    "df['max_glu_serum_t'] = df.apply(lambda row: 1 if (row['max_glu_serum'][0:2] == '>7' or row['max_glu_serum'][0:2] == '>8') else row['max_glu_serum_t'], axis=1) \n",
    "df.drop(['max_glu_serum'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_faixa'] = df.apply(lambda row: 0 if (row['age'] == '[0-10)') else -1, axis=1) \n",
    "df['age_faixa'] = df.apply(lambda row: 1 if (row['age'] == '[10-20)') else row['age_faixa'], axis=1)\n",
    "df['age_faixa'] = df.apply(lambda row: 2 if (row['age'] == '[20-30)') else row['age_faixa'], axis=1) \n",
    "df['age_faixa'] = df.apply(lambda row: 3 if (row['age'] == '[30-40)') else row['age_faixa'], axis=1)\n",
    "df['age_faixa'] = df.apply(lambda row: 4 if (row['age'] == '[40-50)') else row['age_faixa'], axis=1) \n",
    "df['age_faixa'] = df.apply(lambda row: 5 if (row['age'] == '[50-60)') else row['age_faixa'], axis=1)\n",
    "df['age_faixa'] = df.apply(lambda row: 6 if (row['age'] == '[70-80)') else row['age_faixa'], axis=1) \n",
    "df['age_faixa'] = df.apply(lambda row: 7 if (row['age'] == '[80-90)') else row['age_faixa'], axis=1)\n",
    "df['age_faixa'] = df.apply(lambda row: 8 if (row['age'] == '[90-100)') else row['age_faixa'], axis=1)\n",
    "df.drop(['age'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race_t'] = df.apply(lambda row: 0 if (row['race'] == '?') else -1, axis=1) \n",
    "df['race_t'] = df.apply(lambda row: 1 if (row['race'] == 'AfricanAmerican') else row['race_t'], axis=1)\n",
    "df['race_t'] = df.apply(lambda row: 2 if (row['race'] == 'Asian') else row['race_t'], axis=1) \n",
    "df['race_t'] = df.apply(lambda row: 3 if (row['race'] == 'Caucasian') else row['race_t'], axis=1)\n",
    "df['race_t'] = df.apply(lambda row: 4 if (row['race'] == 'Hispanic') else row['race_t'], axis=1) \n",
    "df['race_t'] = df.apply(lambda row: 5 if (row['race'] == 'Other') else row['race_t'], axis=1)\n",
    "df.drop(['race'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o dataset com as tranformações realizadas\n",
    "df.to_csv('./diabetes_data_modificado.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando a base de dados\n",
    "df = pd.read_csv('diabetes_data_modificado.csv', decimal=b',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>...</th>\n",
       "      <th>Med15_t</th>\n",
       "      <th>Med16_t</th>\n",
       "      <th>Med17_t</th>\n",
       "      <th>Med18_t</th>\n",
       "      <th>Med19_t</th>\n",
       "      <th>Med20_t</th>\n",
       "      <th>A1Cresult_t</th>\n",
       "      <th>max_glu_serum_t</th>\n",
       "      <th>age_faixa</th>\n",
       "      <th>race_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "5                  2                         1                    2   \n",
       "6                  3                         1                    2   \n",
       "7                  1                         1                    7   \n",
       "8                  2                         1                    4   \n",
       "9                  3                         3                    4   \n",
       "\n",
       "   time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
       "0                 1                  41               0                1   \n",
       "1                 3                  59               0               18   \n",
       "2                 2                  11               5               13   \n",
       "3                 2                  44               1               16   \n",
       "4                 1                  51               0                8   \n",
       "5                 3                  31               6               16   \n",
       "6                 4                  70               1               21   \n",
       "7                 5                  73               0               12   \n",
       "8                13                  68               2               28   \n",
       "9                12                  33               3               18   \n",
       "\n",
       "   number_outpatient  number_emergency  number_inpatient   ...    Med15_t  \\\n",
       "0                  0                 0                 0   ...          0   \n",
       "1                  0                 0                 0   ...          1   \n",
       "2                  2                 0                 1   ...          0   \n",
       "3                  0                 0                 0   ...          1   \n",
       "4                  0                 0                 0   ...          1   \n",
       "5                  0                 0                 0   ...          1   \n",
       "6                  0                 0                 0   ...          1   \n",
       "7                  0                 0                 0   ...          0   \n",
       "8                  0                 0                 0   ...          1   \n",
       "9                  0                 0                 0   ...          1   \n",
       "\n",
       "   Med16_t  Med17_t  Med18_t  Med19_t  Med20_t  A1Cresult_t  max_glu_serum_t  \\\n",
       "0        0        0        0        0        0           -1               -1   \n",
       "1        0        0        0        0        0           -1               -1   \n",
       "2        0        0        0        0        0           -1               -1   \n",
       "3        0        0        0        0        0           -1               -1   \n",
       "4        0        0        0        0        0           -1               -1   \n",
       "5        0        0        0        0        0           -1               -1   \n",
       "6        0        0        0        0        0           -1               -1   \n",
       "7        0        0        0        0        0           -1               -1   \n",
       "8        0        0        0        0        0           -1               -1   \n",
       "9        0        0        0        0        0           -1               -1   \n",
       "\n",
       "   age_faixa  race_t  \n",
       "0          0       3  \n",
       "1          1       3  \n",
       "2          2       1  \n",
       "3          3       3  \n",
       "4          4       3  \n",
       "5          5       3  \n",
       "6         -1       3  \n",
       "7          6       3  \n",
       "8          7       3  \n",
       "9          8       3  \n",
       "\n",
       "[10 rows x 43 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.info ()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe\n",
      "0    90408\n",
      "1    11357\n",
      "Name: classe, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['classe']).classe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção do Modelo Preditivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O indicador de Readmissão Hospitalar  mede a taxa de pacientes que retornaram ao hospital em até 30 dias desde a última alta hospitalar correspondente a primeira admissão. Este indicador avalia a capacidade progressiva do prestador de serviço em ajudar na recuperação do paciente. Nos Estados unidos, várias iniciativas já foram tomadas para garantir o sucesso da recuperação da saúde de seus pacientes, usando técnicas de trabalho em equipe a tecnologia para diminuir a readmissão hospitalar.\n",
    "\n",
    "A taxa de readmissão hospitalar é frequentemente usada como uma medida da qualidade assistencial de um hospital, segundo determina a ANS, o indicador de Readmissão Hospitalar é um dos critérios para o estabelecimento alcançar o reajuste de 100% do IPCA, consequentemente uma alta taxa de readmissão pode afetar o índice de reajuste dos contratos firmados entre operadoras de planos de saúde e prestadores de serviço. Segundo a diretora-adjunta de Desenvolvimento Setorial da ANS, Michelle Mello “Esse é um indicador internacional clássico para avaliação da qualidade de atendimento e cuidado prestados ao paciente nos hospitais. Quanto menor for a reincidência de internação, ou seja, quanto menor for a readmissão potencialmente evitável, melhor é considerado o atendimento prestado pela unidade hospitalar”.\n",
    "\n",
    "Um dos grandes desafios dos hospitais é identificar as readmissões que poderiam ser evitadas. Ser capaz de prever quais pacientes serão readmitidos pode ajudar os hospitais e operadoras de plano de saúde a economizar milhões de reais e melhorar a qualidade dos cuidados e recuperação dos pacientes\n",
    "\n",
    "O objetivo deste trabalhe é implementar um modelo de Deep Learning, capaz de classificar os pacientes que serão readmitidos, com o mais alto grau de precisão possível. Um dos desafios ao analisar este conjunto de dados é o enorme desequilíbrio da variável target: as readmissões com menos de 30 dias correspondem apenas 11,16% dos atendimentos. Nesse caso, é muito pior ter falsos negativos do que falsos positivos em nossas previsões, pois falsos negativos significam que algum paciente foi readmitido, porém o modelo não foi capaz de prever, isso poderá comprometer os idicadores de qualidade da instituição. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\Anaconda3\\envs\\theano\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Pacotes de Manipulação de Dados\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, MultiLabelBinarizer, QuantileTransformer, Normalizer, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# Keras e TensorFlow\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pacotes para Confusion Matrix e Balanceamento de Classes\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import pandas_ml as pdml\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['classe'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Dataframe diabetic_data_modificado possui 101765 linhas e 43 colunas\n"
     ]
    }
   ],
   "source": [
    "print('O Dataframe diabetic_data_modificado possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temos 11357 pontos de dados como readmissões e 90408 pontos de dados considerados normais.\n"
     ]
    }
   ],
   "source": [
    "readmissoes = df.loc[df['classe'] == 1]\n",
    "nao_readmissoes = df.loc[df['classe'] == 0]\n",
    "print(\"Temos\", len(readmissoes), \"pontos de dados como readmissões e\", len(nao_readmissoes), \"pontos de dados considerados normais.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuido valores as variáveis X e y do dodelo\n",
    "X = df.iloc[:,:-1]\n",
    "y = df['classe']\n",
    "\n",
    "# Aplicando Scala e Redução de dimensionalidade com PCA\n",
    "X = scale(X)\n",
    "pca = PCA(n_components = 10, random_state=38)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "# Gerando dados de treino, teste e validação\n",
    "X1, X_valid, y1, y_valid = train_test_split(X, y, test_size = 0.10, random_state = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.26, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Dataset de Treino:  (67775, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do Dataset de Treino: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Dataset de Validaçao:  (10177, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do Dataset de Validaçao: \", X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Dataset de Test:  (23813, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do Dataset de Test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começando com uma Rede Neural Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = 10, activation = 'relu'))     \n",
    "model.add(Dense(1, activation = 'sigmoid'))                \n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67775 samples, validate on 10177 samples\n",
      "Epoch 1/1\n",
      "67775/67775 [==============================] - 10s 154us/step - loss: 0.2800 - acc: 0.8971 - val_loss: 0.2492 - val_acc: 0.9070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15803ce56d8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro/Acurácia:  [0.24923400468578902, 0.9070452982214798]\n"
     ]
    }
   ],
   "source": [
    "print(\"Erro/Acurácia: \", model.evaluate(X_valid, y_valid, verbose = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_valid).T[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted      0  1  __all__\n",
      "Actual                      \n",
      "0           9046  0     9046\n",
      "1           1131  0     1131\n",
      "__all__    10177  0    10177\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted      0  1  __all__\n",
      "Actual                      \n",
      "0           9046  0     9046\n",
      "1           1131  0     1131\n",
      "__all__    10177  0    10177\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.8888670531590842\n",
      "95% CI: (0.8826000161171167, 0.8949105853310739)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 1.0\n",
      "Kappa: 0.0\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                       0         1\n",
      "Population                                10177     10177\n",
      "P: Condition positive                      9046      1131\n",
      "N: Condition negative                      1131      9046\n",
      "Test outcome positive                     10177         0\n",
      "Test outcome negative                         0     10177\n",
      "TP: True Positive                          9046         0\n",
      "TN: True Negative                             0      9046\n",
      "FP: False Positive                         1131         0\n",
      "FN: False Negative                            0      1131\n",
      "TPR: (Sensitivity, hit rate, recall)          1         0\n",
      "TNR=SPC: (Specificity)                        0         1\n",
      "PPV: Pos Pred Value (Precision)        0.888867       NaN\n",
      "NPV: Neg Pred Value                         NaN  0.888867\n",
      "FPR: False-out                                1         0\n",
      "FDR: False Discovery Rate              0.111133       NaN\n",
      "FNR: Miss Rate                                0         1\n",
      "ACC: Accuracy                          0.888867  0.888867\n",
      "F1 score                               0.941164         0\n",
      "MCC: Matthews correlation coefficient       NaN       NaN\n",
      "Informedness                                  0         0\n",
      "Markedness                                  NaN       NaN\n",
      "Prevalence                             0.888867  0.111133\n",
      "LR+: Positive likelihood ratio                1       NaN\n",
      "LR-: Negative likelihood ratio              NaN         1\n",
      "DOR: Diagnostic odds ratio                  NaN       NaN\n",
      "FOR: False omission rate                    NaN  0.111133\n"
     ]
    }
   ],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "y_right = np.array(y_valid)\n",
    "confusion_matrix = ConfusionMatrix(y_right, y_predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "confusion_matrix.plot(normalized=True)\n",
    "plt.show()\n",
    "confusion_matrix.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que apesar do modelo está demonstrando uma acurácia de aproximadamente 88%, analisando a ConfusionMatrix percebemos que os resultados não foram satisfatório, consequencia do desbalanciamento dos dados. Então vamos aplicar um oversampling para corrigir um viés no conjunto de dados original, empregando Synthetic Minority Over-sampling Technique para balancear os dados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pdml.ModelFrame(X, target=y.ravel())\n",
    "sampler = df2.imbalance.over_sampling.SMOTE(random_state=42, ratio='minority')\n",
    "oversampled = df2.fit_sample(sampler)\n",
    "X2, y2 = oversampled.iloc[:,1:], oversampled.iloc[:,0]\n",
    "X2 = X2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oversampled.iloc[:,0].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando dados de Treino\n",
    "\n",
    "Os dados balanceados servirão para gerar apenas o conjunto de dados treino, para que os dados sintéticos gerados não vazem para os conjuntos de teste e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados de treino com base nos dados balanceados\n",
    "X2_train, X_test_, y2_train, y_test_ = train_test_split(X2, y2.as_matrix(), test_size = 0.33, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentando o Número de Camadas na Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import set_random_seed\n",
    "import keras as keras\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER = Adam(lr=0.01, beta_1=0.99, beta_2=0.999, amsgrad=True) # otimizador\n",
    "OPTIMIZER = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para calcular a metrica de acurácia com base no recall\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.round(model2.predict(X_val)).T[0]\n",
    "    \n",
    "        self._data.append({\n",
    "            'val_recall': recall_score(y_val, np.round(model2.predict(X_val)).T[0]),\n",
    "            'val_precision': precision_score(y_val, np.round(model2.predict(X_val)).T[0]),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8790\n",
    "seed = 100\n",
    "set_random_seed(seed)\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                16400     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 27,959\n",
      "Trainable params: 27,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(10, input_dim = 10,   kernel_initializer='ones', activation = 'tanh')) \n",
    "model2.add(Dense(1024, activation = 'tanh'))\n",
    "model2.add(Dropout(0.40))\n",
    "model2.add(Dense(16, activation = 'tanh'))\n",
    "model2.add(Dropout(0.40))\n",
    "model2.add(Dense(8,  activation = 'tanh'))\n",
    "model2.add(Dropout(0.40))\n",
    "model2.add(Dense(4,  activation = 'tanh'))\n",
    "model2.add(Dropout(0.40))\n",
    "model2.add(Dense(2,  activation = 'tanh'))\n",
    "model2.add(Dropout(0.40))\n",
    "model2.add(Dense(1,  activation = 'sigmoid'))\n",
    "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')   \n",
    "model2.compile(loss = 'binary_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 1s 9us/step - loss: 0.7098 - acc: 0.5345 - val_loss: 0.6671 - val_acc: 0.5815\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.6828 - acc: 0.5604 - val_loss: 0.6058 - val_acc: 0.6531\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.6590 - acc: 0.6130 - val_loss: 0.6506 - val_acc: 0.6239\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.6315 - acc: 0.6636 - val_loss: 0.5862 - val_acc: 0.6846\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.6100 - acc: 0.6901 - val_loss: 0.5835 - val_acc: 0.6990\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5937 - acc: 0.7070 - val_loss: 0.5415 - val_acc: 0.7365\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5743 - acc: 0.7245 - val_loss: 0.4363 - val_acc: 0.8335\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5745 - acc: 0.7244 - val_loss: 0.5288 - val_acc: 0.7454\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5556 - acc: 0.7402 - val_loss: 0.5007 - val_acc: 0.7696\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5568 - acc: 0.7387 - val_loss: 0.5064 - val_acc: 0.7809\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5410 - acc: 0.7517 - val_loss: 0.5151 - val_acc: 0.7740\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5381 - acc: 0.7552 - val_loss: 0.4907 - val_acc: 0.7932\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X2_train, y2_train, epochs = 100, batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5980\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5358 - acc: 0.7562 - val_loss: 0.4950 - val_acc: 0.7881\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5379 - acc: 0.7527 - val_loss: 0.4834 - val_acc: 0.7910\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5347 - acc: 0.7550 - val_loss: 0.4909 - val_acc: 0.7854\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5349 - acc: 0.7555 - val_loss: 0.4870 - val_acc: 0.7929\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5334 - acc: 0.7561 - val_loss: 0.4720 - val_acc: 0.8061\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5340 - acc: 0.7546 - val_loss: 0.4735 - val_acc: 0.8021\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5304 - acc: 0.7579 - val_loss: 0.4867 - val_acc: 0.7973\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5310 - acc: 0.7582 - val_loss: 0.4730 - val_acc: 0.8039\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5309 - acc: 0.7571 - val_loss: 0.4823 - val_acc: 0.8028\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5300 - acc: 0.7584 - val_loss: 0.4505 - val_acc: 0.8213\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5320 - acc: 0.7570 - val_loss: 0.4896 - val_acc: 0.7958\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5274 - acc: 0.7599 - val_loss: 0.4746 - val_acc: 0.8077\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5277 - acc: 0.7599 - val_loss: 0.4846 - val_acc: 0.7920\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5266 - acc: 0.7604 - val_loss: 0.4645 - val_acc: 0.8148\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5268 - acc: 0.7604 - val_loss: 0.4834 - val_acc: 0.7934\n",
      "Epoch 00015: early stopping\n",
      "0.8198298187199408 5980\n",
      "5990\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5254 - acc: 0.7599 - val_loss: 0.4335 - val_acc: 0.8292\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5275 - acc: 0.7591 - val_loss: 0.4821 - val_acc: 0.7963\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5231 - acc: 0.7629 - val_loss: 0.4758 - val_acc: 0.8003\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5249 - acc: 0.7611 - val_loss: 0.4799 - val_acc: 0.7936\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5227 - acc: 0.7616 - val_loss: 0.4507 - val_acc: 0.8232\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5240 - acc: 0.7618 - val_loss: 0.4868 - val_acc: 0.7976\n",
      "Epoch 00006: early stopping\n",
      "0.8331483536810951 5990\n",
      "6000\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5226 - acc: 0.7600 - val_loss: 0.4525 - val_acc: 0.8158\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5205 - acc: 0.7632 - val_loss: 0.4717 - val_acc: 0.8024\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5219 - acc: 0.7612 - val_loss: 0.4445 - val_acc: 0.8294\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5198 - acc: 0.7641 - val_loss: 0.4352 - val_acc: 0.8350\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5223 - acc: 0.7605 - val_loss: 0.4720 - val_acc: 0.7930\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5201 - acc: 0.7638 - val_loss: 0.4697 - val_acc: 0.8023\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5191 - acc: 0.7639 - val_loss: 0.4351 - val_acc: 0.8257\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5215 - acc: 0.7617 - val_loss: 0.4402 - val_acc: 0.8315\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5170 - acc: 0.7628 - val_loss: 0.4351 - val_acc: 0.8294\n",
      "Epoch 00009: early stopping\n",
      "6010\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5144 - acc: 0.7652 - val_loss: 0.4343 - val_acc: 0.8351\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5136 - acc: 0.7651 - val_loss: 0.4202 - val_acc: 0.8437\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5150 - acc: 0.7634 - val_loss: 0.4267 - val_acc: 0.8364\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5134 - acc: 0.7632 - val_loss: 0.4229 - val_acc: 0.8411\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5114 - acc: 0.7658 - val_loss: 0.4528 - val_acc: 0.8094\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5110 - acc: 0.7661 - val_loss: 0.4080 - val_acc: 0.8460\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5108 - acc: 0.7659 - val_loss: 0.4314 - val_acc: 0.8272\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5063 - acc: 0.7668 - val_loss: 0.4128 - val_acc: 0.8524\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5018 - acc: 0.7704 - val_loss: 0.3992 - val_acc: 0.8500\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.5003 - acc: 0.7696 - val_loss: 0.4103 - val_acc: 0.8379\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4980 - acc: 0.7690 - val_loss: 0.4073 - val_acc: 0.8431\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4918 - acc: 0.7729 - val_loss: 0.4012 - val_acc: 0.8398\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4903 - acc: 0.7736 - val_loss: 0.3638 - val_acc: 0.8698\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4891 - acc: 0.7754 - val_loss: 0.4247 - val_acc: 0.8297\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4841 - acc: 0.7787 - val_loss: 0.4123 - val_acc: 0.8317\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4830 - acc: 0.7796 - val_loss: 0.3829 - val_acc: 0.8562\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4788 - acc: 0.7804 - val_loss: 0.3344 - val_acc: 0.8867\n",
      "Epoch 18/100\n",
      "121146/121146 [==============================] - 1s 5us/step - loss: 0.4764 - acc: 0.7819 - val_loss: 0.3563 - val_acc: 0.8774\n",
      "Epoch 19/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4721 - acc: 0.7836 - val_loss: 0.3688 - val_acc: 0.8666\n",
      "Epoch 20/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.4730 - acc: 0.7838 - val_loss: 0.3742 - val_acc: 0.8599\n",
      "Epoch 21/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4660 - acc: 0.7866 - val_loss: 0.3387 - val_acc: 0.8813\n",
      "Epoch 22/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4657 - acc: 0.7874 - val_loss: 0.4044 - val_acc: 0.8351\n",
      "Epoch 00022: early stopping\n",
      "0.8642249352571217 6010\n",
      "6020\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4633 - acc: 0.7886 - val_loss: 0.3471 - val_acc: 0.8745\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4617 - acc: 0.7886 - val_loss: 0.3272 - val_acc: 0.8934\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4592 - acc: 0.7897 - val_loss: 0.3388 - val_acc: 0.8805\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4578 - acc: 0.7918 - val_loss: 0.3536 - val_acc: 0.8723\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4595 - acc: 0.7915 - val_loss: 0.3293 - val_acc: 0.8889\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4541 - acc: 0.7918 - val_loss: 0.3451 - val_acc: 0.8792\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4544 - acc: 0.7934 - val_loss: 0.3210 - val_acc: 0.8973\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4533 - acc: 0.7923 - val_loss: 0.3239 - val_acc: 0.8899\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4492 - acc: 0.7962 - val_loss: 0.3563 - val_acc: 0.8589\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4469 - acc: 0.7950 - val_loss: 0.2797 - val_acc: 0.9181\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4500 - acc: 0.7950 - val_loss: 0.2870 - val_acc: 0.9159\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4464 - acc: 0.7948 - val_loss: 0.3424 - val_acc: 0.8729\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4464 - acc: 0.7958 - val_loss: 0.3290 - val_acc: 0.8879\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4451 - acc: 0.7971 - val_loss: 0.2930 - val_acc: 0.9013\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4492 - acc: 0.7943 - val_loss: 0.3283 - val_acc: 0.8894\n",
      "Epoch 00015: early stopping\n",
      "6030\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4424 - acc: 0.7980 - val_loss: 0.2990 - val_acc: 0.9058\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4413 - acc: 0.7988 - val_loss: 0.3040 - val_acc: 0.9018\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4384 - acc: 0.7987 - val_loss: 0.2740 - val_acc: 0.9169\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4415 - acc: 0.7974 - val_loss: 0.2979 - val_acc: 0.9053\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4367 - acc: 0.7997 - val_loss: 0.2779 - val_acc: 0.9079\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4384 - acc: 0.7995 - val_loss: 0.2846 - val_acc: 0.9121\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4408 - acc: 0.7966 - val_loss: 0.3026 - val_acc: 0.8993\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4384 - acc: 0.7989 - val_loss: 0.3036 - val_acc: 0.8971\n",
      "Epoch 00008: early stopping\n",
      "6040\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4367 - acc: 0.7997 - val_loss: 0.2887 - val_acc: 0.9086\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4362 - acc: 0.7988 - val_loss: 0.2809 - val_acc: 0.9087\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4337 - acc: 0.8008 - val_loss: 0.3925 - val_acc: 0.8499\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4340 - acc: 0.8025 - val_loss: 0.2931 - val_acc: 0.9059\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4377 - acc: 0.7986 - val_loss: 0.3068 - val_acc: 0.9022\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4355 - acc: 0.8006 - val_loss: 0.2601 - val_acc: 0.9254\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4380 - acc: 0.7980 - val_loss: 0.3563 - val_acc: 0.8673\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4371 - acc: 0.8009 - val_loss: 0.3139 - val_acc: 0.8971\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4331 - acc: 0.8019 - val_loss: 0.2800 - val_acc: 0.9131\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4353 - acc: 0.8001 - val_loss: 0.3118 - val_acc: 0.8914\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4353 - acc: 0.8022 - val_loss: 0.2914 - val_acc: 0.9090\n",
      "Epoch 00011: early stopping\n",
      "6050\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4350 - acc: 0.8007 - val_loss: 0.3048 - val_acc: 0.8995\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4354 - acc: 0.8017 - val_loss: 0.3650 - val_acc: 0.8625\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4338 - acc: 0.8033 - val_loss: 0.3998 - val_acc: 0.8429\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4344 - acc: 0.8030 - val_loss: 0.3246 - val_acc: 0.8853\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4320 - acc: 0.8043 - val_loss: 0.3503 - val_acc: 0.8617\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4345 - acc: 0.8016 - val_loss: 0.3080 - val_acc: 0.9012\n",
      "Epoch 00006: early stopping\n",
      "6060\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4367 - acc: 0.7979 - val_loss: 0.2771 - val_acc: 0.9062\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4295 - acc: 0.8039 - val_loss: 0.3023 - val_acc: 0.8924\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4351 - acc: 0.8019 - val_loss: 0.3273 - val_acc: 0.8805\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4323 - acc: 0.8022 - val_loss: 0.3120 - val_acc: 0.8911\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4291 - acc: 0.8043 - val_loss: 0.2721 - val_acc: 0.9108\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4333 - acc: 0.8017 - val_loss: 0.2723 - val_acc: 0.9093\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4337 - acc: 0.8017 - val_loss: 0.3174 - val_acc: 0.8868\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4282 - acc: 0.8049 - val_loss: 0.3188 - val_acc: 0.8831\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4301 - acc: 0.8022 - val_loss: 0.3065 - val_acc: 0.8924\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4297 - acc: 0.8033 - val_loss: 0.2917 - val_acc: 0.8988\n",
      "Epoch 00010: early stopping\n",
      "6070\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4294 - acc: 0.8038 - val_loss: 0.2910 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4315 - acc: 0.8032 - val_loss: 0.2828 - val_acc: 0.9045\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4279 - acc: 0.8051 - val_loss: 0.3024 - val_acc: 0.8904\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4311 - acc: 0.8038 - val_loss: 0.2904 - val_acc: 0.9035\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4289 - acc: 0.8037 - val_loss: 0.3000 - val_acc: 0.8983\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4280 - acc: 0.8027 - val_loss: 0.2952 - val_acc: 0.9011\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4281 - acc: 0.8034 - val_loss: 0.3135 - val_acc: 0.8955\n",
      "Epoch 00007: early stopping\n",
      "6080\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4297 - acc: 0.8043 - val_loss: 0.2983 - val_acc: 0.8948\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4271 - acc: 0.8044 - val_loss: 0.3125 - val_acc: 0.8862\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4299 - acc: 0.8023 - val_loss: 0.2912 - val_acc: 0.9005\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4284 - acc: 0.8049 - val_loss: 0.2968 - val_acc: 0.8982\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4286 - acc: 0.8053 - val_loss: 0.2822 - val_acc: 0.9038\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4254 - acc: 0.8069 - val_loss: 0.3340 - val_acc: 0.8633\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4285 - acc: 0.8030 - val_loss: 0.2887 - val_acc: 0.9006\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4271 - acc: 0.8048 - val_loss: 0.3149 - val_acc: 0.8874\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.4286 - acc: 0.8056 - val_loss: 0.2975 - val_acc: 0.9010\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4281 - acc: 0.8038 - val_loss: 0.3068 - val_acc: 0.8930\n",
      "Epoch 00010: early stopping\n",
      "6090\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4276 - acc: 0.8063 - val_loss: 0.2943 - val_acc: 0.8990\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4275 - acc: 0.8051 - val_loss: 0.2931 - val_acc: 0.8954\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4253 - acc: 0.8074 - val_loss: 0.3064 - val_acc: 0.8875\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4281 - acc: 0.8046 - val_loss: 0.3254 - val_acc: 0.8782\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4249 - acc: 0.8058 - val_loss: 0.3182 - val_acc: 0.8827\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4263 - acc: 0.8057 - val_loss: 0.2928 - val_acc: 0.8975\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4247 - acc: 0.8061 - val_loss: 0.2724 - val_acc: 0.9072\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4242 - acc: 0.8060 - val_loss: 0.3350 - val_acc: 0.8672\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4260 - acc: 0.8047 - val_loss: 0.2979 - val_acc: 0.8892\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4283 - acc: 0.8043 - val_loss: 0.3189 - val_acc: 0.8788\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4252 - acc: 0.8065 - val_loss: 0.2835 - val_acc: 0.9023\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4220 - acc: 0.8086 - val_loss: 0.3693 - val_acc: 0.8487\n",
      "Epoch 00012: early stopping\n",
      "0.900850906400296 6090\n",
      "6100\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4259 - acc: 0.8044 - val_loss: 0.2866 - val_acc: 0.8998\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4249 - acc: 0.8061 - val_loss: 0.2628 - val_acc: 0.9111\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.4254 - acc: 0.8059 - val_loss: 0.2978 - val_acc: 0.8925\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4255 - acc: 0.8046 - val_loss: 0.2956 - val_acc: 0.8990\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4235 - acc: 0.8051 - val_loss: 0.2913 - val_acc: 0.8978\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4236 - acc: 0.8058 - val_loss: 0.2846 - val_acc: 0.8994\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4257 - acc: 0.8047 - val_loss: 0.2915 - val_acc: 0.8980\n",
      "Epoch 00007: early stopping\n",
      "6110\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4235 - acc: 0.8066 - val_loss: 0.2743 - val_acc: 0.9077\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4231 - acc: 0.8079 - val_loss: 0.2756 - val_acc: 0.9057\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4232 - acc: 0.8073 - val_loss: 0.2896 - val_acc: 0.8972\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4228 - acc: 0.8057 - val_loss: 0.2831 - val_acc: 0.9031\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4260 - acc: 0.8048 - val_loss: 0.3127 - val_acc: 0.8877\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4232 - acc: 0.8073 - val_loss: 0.2931 - val_acc: 0.8986\n",
      "Epoch 00006: early stopping\n",
      "6120\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4218 - acc: 0.8076 - val_loss: 0.3186 - val_acc: 0.8777\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4238 - acc: 0.8057 - val_loss: 0.3183 - val_acc: 0.8803\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4260 - acc: 0.8048 - val_loss: 0.3343 - val_acc: 0.8755\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4202 - acc: 0.8070 - val_loss: 0.2769 - val_acc: 0.9041\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4228 - acc: 0.8056 - val_loss: 0.3143 - val_acc: 0.8842\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4233 - acc: 0.8080 - val_loss: 0.3096 - val_acc: 0.8870\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4195 - acc: 0.8103 - val_loss: 0.2833 - val_acc: 0.9017\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4244 - acc: 0.8079 - val_loss: 0.2949 - val_acc: 0.9006\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4202 - acc: 0.8092 - val_loss: 0.3162 - val_acc: 0.8835\n",
      "Epoch 00009: early stopping\n",
      "6130\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4218 - acc: 0.8076 - val_loss: 0.3041 - val_acc: 0.8897\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4205 - acc: 0.8080 - val_loss: 0.3362 - val_acc: 0.8664\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4197 - acc: 0.8096 - val_loss: 0.3034 - val_acc: 0.8898\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4221 - acc: 0.8070 - val_loss: 0.3130 - val_acc: 0.8896\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4215 - acc: 0.8072 - val_loss: 0.2729 - val_acc: 0.9018\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4216 - acc: 0.8075 - val_loss: 0.2921 - val_acc: 0.8954\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4237 - acc: 0.8076 - val_loss: 0.2920 - val_acc: 0.9016\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4188 - acc: 0.8093 - val_loss: 0.2761 - val_acc: 0.9061\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4207 - acc: 0.8081 - val_loss: 0.3359 - val_acc: 0.8705\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4225 - acc: 0.8069 - val_loss: 0.3020 - val_acc: 0.8962\n",
      "Epoch 00010: early stopping\n",
      "6140\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4199 - acc: 0.8098 - val_loss: 0.3310 - val_acc: 0.8756\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4202 - acc: 0.8090 - val_loss: 0.2754 - val_acc: 0.9081\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4166 - acc: 0.8106 - val_loss: 0.2781 - val_acc: 0.9050\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4220 - acc: 0.8088 - val_loss: 0.3127 - val_acc: 0.8881\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4198 - acc: 0.8087 - val_loss: 0.2864 - val_acc: 0.8992\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4213 - acc: 0.8073 - val_loss: 0.3196 - val_acc: 0.8821\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4206 - acc: 0.8081 - val_loss: 0.3007 - val_acc: 0.8931\n",
      "Epoch 00007: early stopping\n",
      "6150\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4187 - acc: 0.8075 - val_loss: 0.2799 - val_acc: 0.9033\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4190 - acc: 0.8068 - val_loss: 0.2817 - val_acc: 0.9040\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4233 - acc: 0.8057 - val_loss: 0.2836 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4180 - acc: 0.8085 - val_loss: 0.2731 - val_acc: 0.9088\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4210 - acc: 0.8075 - val_loss: 0.2902 - val_acc: 0.8990\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4166 - acc: 0.8115 - val_loss: 0.2716 - val_acc: 0.9033\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4177 - acc: 0.8090 - val_loss: 0.2516 - val_acc: 0.9183\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4206 - acc: 0.8091 - val_loss: 0.3024 - val_acc: 0.8919\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4170 - acc: 0.8101 - val_loss: 0.2790 - val_acc: 0.9040\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4191 - acc: 0.8086 - val_loss: 0.2715 - val_acc: 0.9035\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4206 - acc: 0.8096 - val_loss: 0.2872 - val_acc: 0.8999\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4204 - acc: 0.8096 - val_loss: 0.3177 - val_acc: 0.8842\n",
      "Epoch 00012: early stopping\n",
      "6160\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4189 - acc: 0.8078 - val_loss: 0.2798 - val_acc: 0.9046\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4180 - acc: 0.8082 - val_loss: 0.2911 - val_acc: 0.8982\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4181 - acc: 0.8094 - val_loss: 0.2851 - val_acc: 0.8998\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4182 - acc: 0.8090 - val_loss: 0.2967 - val_acc: 0.8981\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4183 - acc: 0.8093 - val_loss: 0.3506 - val_acc: 0.8633\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4187 - acc: 0.8076 - val_loss: 0.2626 - val_acc: 0.9105\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4178 - acc: 0.8091 - val_loss: 0.2910 - val_acc: 0.8979\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4193 - acc: 0.8083 - val_loss: 0.2937 - val_acc: 0.8961\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4186 - acc: 0.8071 - val_loss: 0.3077 - val_acc: 0.8905\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4194 - acc: 0.8078 - val_loss: 0.2658 - val_acc: 0.9083\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4175 - acc: 0.8088 - val_loss: 0.2728 - val_acc: 0.9063\n",
      "Epoch 00011: early stopping\n",
      "6170\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4194 - acc: 0.8068 - val_loss: 0.2881 - val_acc: 0.9027\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4171 - acc: 0.8100 - val_loss: 0.2580 - val_acc: 0.9133\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4193 - acc: 0.8071 - val_loss: 0.2720 - val_acc: 0.9041\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4181 - acc: 0.8091 - val_loss: 0.2839 - val_acc: 0.8990\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4171 - acc: 0.8099 - val_loss: 0.2743 - val_acc: 0.9073\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4168 - acc: 0.8089 - val_loss: 0.2809 - val_acc: 0.9021\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4175 - acc: 0.8094 - val_loss: 0.2874 - val_acc: 0.8982\n",
      "Epoch 00007: early stopping\n",
      "6180\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4183 - acc: 0.8096 - val_loss: 0.2805 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4183 - acc: 0.8098 - val_loss: 0.2925 - val_acc: 0.8937\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4182 - acc: 0.8093 - val_loss: 0.3117 - val_acc: 0.8790\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4172 - acc: 0.8100 - val_loss: 0.2791 - val_acc: 0.9042\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4178 - acc: 0.8087 - val_loss: 0.2730 - val_acc: 0.9009\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4158 - acc: 0.8105 - val_loss: 0.2874 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4178 - acc: 0.8107 - val_loss: 0.3087 - val_acc: 0.8831\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4183 - acc: 0.8113 - val_loss: 0.2672 - val_acc: 0.9099\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4173 - acc: 0.8121 - val_loss: 0.2947 - val_acc: 0.8928\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4174 - acc: 0.8102 - val_loss: 0.2992 - val_acc: 0.8974\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4161 - acc: 0.8101 - val_loss: 0.3164 - val_acc: 0.8864\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4149 - acc: 0.8112 - val_loss: 0.2861 - val_acc: 0.8970\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4197 - acc: 0.8103 - val_loss: 0.3030 - val_acc: 0.8908\n",
      "Epoch 00013: early stopping\n",
      "6190\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4151 - acc: 0.8116 - val_loss: 0.2623 - val_acc: 0.9081\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4181 - acc: 0.8106 - val_loss: 0.2977 - val_acc: 0.8909\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4167 - acc: 0.8114 - val_loss: 0.2960 - val_acc: 0.8936\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4138 - acc: 0.8131 - val_loss: 0.3177 - val_acc: 0.8834\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4180 - acc: 0.8104 - val_loss: 0.3094 - val_acc: 0.8923\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4159 - acc: 0.8111 - val_loss: 0.2898 - val_acc: 0.8949\n",
      "Epoch 00006: early stopping\n",
      "6200\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4164 - acc: 0.8117 - val_loss: 0.2971 - val_acc: 0.8948\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4190 - acc: 0.8092 - val_loss: 0.2968 - val_acc: 0.8955\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4152 - acc: 0.8130 - val_loss: 0.2740 - val_acc: 0.9059\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4169 - acc: 0.8087 - val_loss: 0.3294 - val_acc: 0.8751\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4142 - acc: 0.8090 - val_loss: 0.2868 - val_acc: 0.8973\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4159 - acc: 0.8097 - val_loss: 0.2875 - val_acc: 0.9021\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4146 - acc: 0.8109 - val_loss: 0.2965 - val_acc: 0.8963\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4157 - acc: 0.8115 - val_loss: 0.2813 - val_acc: 0.9011\n",
      "Epoch 00008: early stopping\n",
      "6210\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4144 - acc: 0.8115 - val_loss: 0.2690 - val_acc: 0.9095\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4150 - acc: 0.8100 - val_loss: 0.2953 - val_acc: 0.8962\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4129 - acc: 0.8115 - val_loss: 0.2708 - val_acc: 0.9082\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4165 - acc: 0.8092 - val_loss: 0.2793 - val_acc: 0.8987\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4159 - acc: 0.8116 - val_loss: 0.3007 - val_acc: 0.8914\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4124 - acc: 0.8118 - val_loss: 0.2751 - val_acc: 0.9011\n",
      "Epoch 00006: early stopping\n",
      "6220\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4163 - acc: 0.8108 - val_loss: 0.2559 - val_acc: 0.9139\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4135 - acc: 0.8112 - val_loss: 0.2535 - val_acc: 0.9120\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4158 - acc: 0.8124 - val_loss: 0.2600 - val_acc: 0.9134\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4156 - acc: 0.8120 - val_loss: 0.2801 - val_acc: 0.9046\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4142 - acc: 0.8117 - val_loss: 0.2803 - val_acc: 0.9008\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4116 - acc: 0.8125 - val_loss: 0.2645 - val_acc: 0.9097\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4145 - acc: 0.8111 - val_loss: 0.2947 - val_acc: 0.8976\n",
      "Epoch 00007: early stopping\n",
      "6230\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8117 - val_loss: 0.2732 - val_acc: 0.9098\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4162 - acc: 0.8143 - val_loss: 0.3185 - val_acc: 0.8820\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4144 - acc: 0.8128 - val_loss: 0.2681 - val_acc: 0.9085\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4136 - acc: 0.8134 - val_loss: 0.3026 - val_acc: 0.8975\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4146 - acc: 0.8109 - val_loss: 0.2512 - val_acc: 0.9149\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4176 - acc: 0.8107 - val_loss: 0.3173 - val_acc: 0.8725\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4135 - acc: 0.8125 - val_loss: 0.2944 - val_acc: 0.8907\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4154 - acc: 0.8107 - val_loss: 0.2791 - val_acc: 0.9018\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4158 - acc: 0.8125 - val_loss: 0.2831 - val_acc: 0.8992\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4139 - acc: 0.8123 - val_loss: 0.3113 - val_acc: 0.8821\n",
      "Epoch 00010: early stopping\n",
      "6240\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4156 - acc: 0.8104 - val_loss: 0.2928 - val_acc: 0.8895\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4132 - acc: 0.8098 - val_loss: 0.2839 - val_acc: 0.8947\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4154 - acc: 0.8107 - val_loss: 0.2674 - val_acc: 0.9124\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4179 - acc: 0.8087 - val_loss: 0.3661 - val_acc: 0.8199\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4139 - acc: 0.8110 - val_loss: 0.3071 - val_acc: 0.8885\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4155 - acc: 0.8113 - val_loss: 0.2977 - val_acc: 0.8892\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4134 - acc: 0.8120 - val_loss: 0.2695 - val_acc: 0.9057\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4150 - acc: 0.8101 - val_loss: 0.2788 - val_acc: 0.9002\n",
      "Epoch 00008: early stopping\n",
      "6250\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4138 - acc: 0.8125 - val_loss: 0.2500 - val_acc: 0.9193\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4124 - acc: 0.8119 - val_loss: 0.3374 - val_acc: 0.8654\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4153 - acc: 0.8092 - val_loss: 0.2876 - val_acc: 0.8993\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4126 - acc: 0.8119 - val_loss: 0.2560 - val_acc: 0.9184\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4107 - acc: 0.8113 - val_loss: 0.3273 - val_acc: 0.8729\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4111 - acc: 0.8125 - val_loss: 0.3361 - val_acc: 0.8707\n",
      "Epoch 00006: early stopping\n",
      "6260\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4117 - acc: 0.8137 - val_loss: 0.2774 - val_acc: 0.9017\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4114 - acc: 0.8128 - val_loss: 0.3006 - val_acc: 0.8933\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4116 - acc: 0.8130 - val_loss: 0.3349 - val_acc: 0.8588\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4134 - acc: 0.8126 - val_loss: 0.3126 - val_acc: 0.8846\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4128 - acc: 0.8135 - val_loss: 0.2690 - val_acc: 0.9121\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4122 - acc: 0.8127 - val_loss: 0.2935 - val_acc: 0.8984\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4138 - acc: 0.8139 - val_loss: 0.2517 - val_acc: 0.9184\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4136 - acc: 0.8117 - val_loss: 0.2662 - val_acc: 0.9099\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4128 - acc: 0.8112 - val_loss: 0.2960 - val_acc: 0.8900\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8114 - val_loss: 0.2902 - val_acc: 0.8954\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4123 - acc: 0.8133 - val_loss: 0.3248 - val_acc: 0.8847\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4130 - acc: 0.8118 - val_loss: 0.2573 - val_acc: 0.9134\n",
      "Epoch 00012: early stopping\n",
      "6270\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4132 - acc: 0.8123 - val_loss: 0.2818 - val_acc: 0.9056\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4106 - acc: 0.8138 - val_loss: 0.3318 - val_acc: 0.8699\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4130 - acc: 0.8127 - val_loss: 0.3143 - val_acc: 0.8707\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4146 - acc: 0.8126 - val_loss: 0.2665 - val_acc: 0.9102\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4149 - acc: 0.8125 - val_loss: 0.2929 - val_acc: 0.8915\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4127 - acc: 0.8123 - val_loss: 0.2835 - val_acc: 0.9059\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4112 - acc: 0.8123 - val_loss: 0.2696 - val_acc: 0.9082\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4126 - acc: 0.8125 - val_loss: 0.2915 - val_acc: 0.8943\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4129 - acc: 0.8126 - val_loss: 0.2875 - val_acc: 0.8972\n",
      "Epoch 00009: early stopping\n",
      "6280\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8119 - val_loss: 0.2888 - val_acc: 0.8924\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4117 - acc: 0.8125 - val_loss: 0.2817 - val_acc: 0.8955\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4103 - acc: 0.8115 - val_loss: 0.2413 - val_acc: 0.9214\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4122 - acc: 0.8139 - val_loss: 0.2917 - val_acc: 0.8950\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4123 - acc: 0.8137 - val_loss: 0.3228 - val_acc: 0.8741\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4145 - acc: 0.8119 - val_loss: 0.2478 - val_acc: 0.9175\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4159 - acc: 0.8092 - val_loss: 0.3363 - val_acc: 0.8722\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4123 - acc: 0.8134 - val_loss: 0.3006 - val_acc: 0.8920\n",
      "Epoch 00008: early stopping\n",
      "6290\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4104 - acc: 0.8140 - val_loss: 0.3030 - val_acc: 0.8948\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4115 - acc: 0.8137 - val_loss: 0.4047 - val_acc: 0.8336\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4119 - acc: 0.8112 - val_loss: 0.2548 - val_acc: 0.9129\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8112 - val_loss: 0.2654 - val_acc: 0.9049\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4113 - acc: 0.8118 - val_loss: 0.2799 - val_acc: 0.9053\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4122 - acc: 0.8125 - val_loss: 0.2808 - val_acc: 0.9028\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4138 - acc: 0.8104 - val_loss: 0.2813 - val_acc: 0.9013\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4098 - acc: 0.8131 - val_loss: 0.2670 - val_acc: 0.9040\n",
      "Epoch 00008: early stopping\n",
      "6300\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4116 - acc: 0.8122 - val_loss: 0.3136 - val_acc: 0.8802\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4099 - acc: 0.8133 - val_loss: 0.3112 - val_acc: 0.8789\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4104 - acc: 0.8144 - val_loss: 0.2824 - val_acc: 0.9011\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4093 - acc: 0.8121 - val_loss: 0.3004 - val_acc: 0.8924\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4113 - acc: 0.8125 - val_loss: 0.2780 - val_acc: 0.9049\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4088 - acc: 0.8136 - val_loss: 0.2844 - val_acc: 0.8986\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4121 - acc: 0.8130 - val_loss: 0.2614 - val_acc: 0.9136\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4096 - acc: 0.8130 - val_loss: 0.2444 - val_acc: 0.9231\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4122 - acc: 0.8122 - val_loss: 0.3043 - val_acc: 0.8842\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4098 - acc: 0.8141 - val_loss: 0.2919 - val_acc: 0.8932\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4109 - acc: 0.8149 - val_loss: 0.2699 - val_acc: 0.9067\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4135 - acc: 0.8116 - val_loss: 0.2820 - val_acc: 0.8987\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8129 - val_loss: 0.3326 - val_acc: 0.8798\n",
      "Epoch 00013: early stopping\n",
      "6310\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4115 - acc: 0.8133 - val_loss: 0.2720 - val_acc: 0.9072\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4105 - acc: 0.8126 - val_loss: 0.2553 - val_acc: 0.9151\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4132 - acc: 0.8121 - val_loss: 0.2880 - val_acc: 0.8979\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4093 - acc: 0.8126 - val_loss: 0.2777 - val_acc: 0.9048\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4105 - acc: 0.8142 - val_loss: 0.2701 - val_acc: 0.9091\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4114 - acc: 0.8142 - val_loss: 0.2468 - val_acc: 0.9198\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4103 - acc: 0.8137 - val_loss: 0.2997 - val_acc: 0.8901\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4100 - acc: 0.8132 - val_loss: 0.2438 - val_acc: 0.9225\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4097 - acc: 0.8121 - val_loss: 0.2890 - val_acc: 0.8950\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4068 - acc: 0.8144 - val_loss: 0.2509 - val_acc: 0.9193\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4130 - acc: 0.8129 - val_loss: 0.2848 - val_acc: 0.8973\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4125 - acc: 0.8136 - val_loss: 0.2623 - val_acc: 0.9167\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4088 - acc: 0.8145 - val_loss: 0.2885 - val_acc: 0.8975\n",
      "Epoch 00013: early stopping\n",
      "6320\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4119 - acc: 0.8123 - val_loss: 0.2848 - val_acc: 0.9015\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4097 - acc: 0.8139 - val_loss: 0.2943 - val_acc: 0.8927\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4109 - acc: 0.8123 - val_loss: 0.2484 - val_acc: 0.9133\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4095 - acc: 0.8152 - val_loss: 0.2690 - val_acc: 0.9114\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4114 - acc: 0.8141 - val_loss: 0.2599 - val_acc: 0.9149\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4107 - acc: 0.8140 - val_loss: 0.3019 - val_acc: 0.8920\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4101 - acc: 0.8128 - val_loss: 0.3302 - val_acc: 0.8718\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4074 - acc: 0.8131 - val_loss: 0.2570 - val_acc: 0.9131\n",
      "Epoch 00008: early stopping\n",
      "6330\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4123 - acc: 0.8127 - val_loss: 0.2550 - val_acc: 0.9126\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4101 - acc: 0.8131 - val_loss: 0.2372 - val_acc: 0.9289\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4089 - acc: 0.8126 - val_loss: 0.2535 - val_acc: 0.9158\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4129 - acc: 0.8115 - val_loss: 0.2619 - val_acc: 0.9116\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4088 - acc: 0.8154 - val_loss: 0.2950 - val_acc: 0.8925\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4108 - acc: 0.8127 - val_loss: 0.3306 - val_acc: 0.8749\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4082 - acc: 0.8136 - val_loss: 0.2644 - val_acc: 0.9137\n",
      "Epoch 00007: early stopping\n",
      "6340\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4089 - acc: 0.8151 - val_loss: 0.2668 - val_acc: 0.9083\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4102 - acc: 0.8140 - val_loss: 0.2618 - val_acc: 0.9114\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4073 - acc: 0.8145 - val_loss: 0.2378 - val_acc: 0.9236\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4103 - acc: 0.8126 - val_loss: 0.3040 - val_acc: 0.8823\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4089 - acc: 0.8127 - val_loss: 0.2744 - val_acc: 0.9079\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4113 - acc: 0.8136 - val_loss: 0.2269 - val_acc: 0.9294\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4147 - acc: 0.8127 - val_loss: 0.2573 - val_acc: 0.9134\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4082 - acc: 0.8153 - val_loss: 0.2448 - val_acc: 0.9221\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4104 - acc: 0.8132 - val_loss: 0.2904 - val_acc: 0.9017\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4097 - acc: 0.8148 - val_loss: 0.2670 - val_acc: 0.9081\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4065 - acc: 0.8161 - val_loss: 0.2821 - val_acc: 0.9054\n",
      "Epoch 00011: early stopping\n",
      "6350\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4095 - acc: 0.8150 - val_loss: 0.2790 - val_acc: 0.9026\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4099 - acc: 0.8124 - val_loss: 0.2676 - val_acc: 0.9101\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4090 - acc: 0.8129 - val_loss: 0.2445 - val_acc: 0.9211\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4104 - acc: 0.8121 - val_loss: 0.2661 - val_acc: 0.9085\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4074 - acc: 0.8138 - val_loss: 0.2392 - val_acc: 0.9259\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4081 - acc: 0.8135 - val_loss: 0.2557 - val_acc: 0.9168\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4109 - acc: 0.8130 - val_loss: 0.2690 - val_acc: 0.9038\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4079 - acc: 0.8130 - val_loss: 0.2874 - val_acc: 0.8973\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4086 - acc: 0.8115 - val_loss: 0.2489 - val_acc: 0.9249\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4096 - acc: 0.8130 - val_loss: 0.2338 - val_acc: 0.9258\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4110 - acc: 0.8124 - val_loss: 0.3029 - val_acc: 0.8934\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4092 - acc: 0.8131 - val_loss: 0.2992 - val_acc: 0.8924\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4108 - acc: 0.8100 - val_loss: 0.2718 - val_acc: 0.9045\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4085 - acc: 0.8130 - val_loss: 0.2641 - val_acc: 0.9107\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4098 - acc: 0.8122 - val_loss: 0.2980 - val_acc: 0.8907\n",
      "Epoch 00015: early stopping\n",
      "6360\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4084 - acc: 0.8136 - val_loss: 0.2670 - val_acc: 0.9212\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4106 - acc: 0.8128 - val_loss: 0.3303 - val_acc: 0.8663\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4056 - acc: 0.8131 - val_loss: 0.2588 - val_acc: 0.9181\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4069 - acc: 0.8138 - val_loss: 0.3328 - val_acc: 0.8794\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4079 - acc: 0.8145 - val_loss: 0.2859 - val_acc: 0.9026\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4109 - acc: 0.8128 - val_loss: 0.2365 - val_acc: 0.9299\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4078 - acc: 0.8125 - val_loss: 0.2621 - val_acc: 0.9143\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4090 - acc: 0.8118 - val_loss: 0.2847 - val_acc: 0.9097\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4075 - acc: 0.8150 - val_loss: 0.2625 - val_acc: 0.9150\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4086 - acc: 0.8153 - val_loss: 0.2320 - val_acc: 0.9289\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4095 - acc: 0.8128 - val_loss: 0.2621 - val_acc: 0.9181\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4071 - acc: 0.8146 - val_loss: 0.2473 - val_acc: 0.9199\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4078 - acc: 0.8141 - val_loss: 0.3076 - val_acc: 0.8915\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4093 - acc: 0.8146 - val_loss: 0.2850 - val_acc: 0.8968\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4102 - acc: 0.8152 - val_loss: 0.2414 - val_acc: 0.9222\n",
      "Epoch 00015: early stopping\n",
      "6370\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4068 - acc: 0.8152 - val_loss: 0.2710 - val_acc: 0.9093\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4081 - acc: 0.8120 - val_loss: 0.3073 - val_acc: 0.8898\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4072 - acc: 0.8138 - val_loss: 0.2803 - val_acc: 0.9013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4084 - acc: 0.8139 - val_loss: 0.2787 - val_acc: 0.9057\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4133 - acc: 0.8117 - val_loss: 0.2720 - val_acc: 0.9111\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4076 - acc: 0.8147 - val_loss: 0.2833 - val_acc: 0.8952\n",
      "Epoch 00006: early stopping\n",
      "6380\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4075 - acc: 0.8120 - val_loss: 0.2699 - val_acc: 0.9071\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4065 - acc: 0.8128 - val_loss: 0.2845 - val_acc: 0.8929\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4066 - acc: 0.8152 - val_loss: 0.2863 - val_acc: 0.8975\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4059 - acc: 0.8165 - val_loss: 0.2827 - val_acc: 0.9021\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4048 - acc: 0.8155 - val_loss: 0.3094 - val_acc: 0.8826\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4045 - acc: 0.8157 - val_loss: 0.3444 - val_acc: 0.8595\n",
      "Epoch 00006: early stopping\n",
      "0.9115797262301147 6380\n",
      "6390\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4071 - acc: 0.8141 - val_loss: 0.2922 - val_acc: 0.8960\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4061 - acc: 0.8148 - val_loss: 0.2869 - val_acc: 0.8960\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4032 - acc: 0.8152 - val_loss: 0.3053 - val_acc: 0.8901\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4059 - acc: 0.8146 - val_loss: 0.2776 - val_acc: 0.8990\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4063 - acc: 0.8129 - val_loss: 0.3388 - val_acc: 0.8669\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4070 - acc: 0.8157 - val_loss: 0.2817 - val_acc: 0.9034\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4079 - acc: 0.8157 - val_loss: 0.3045 - val_acc: 0.8834\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4051 - acc: 0.8150 - val_loss: 0.2862 - val_acc: 0.8934\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4070 - acc: 0.8138 - val_loss: 0.2664 - val_acc: 0.9058\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4051 - acc: 0.8135 - val_loss: 0.3070 - val_acc: 0.8782\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4095 - acc: 0.8121 - val_loss: 0.2734 - val_acc: 0.9010\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4079 - acc: 0.8137 - val_loss: 0.2535 - val_acc: 0.9113\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4075 - acc: 0.8146 - val_loss: 0.3132 - val_acc: 0.8754\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4029 - acc: 0.8159 - val_loss: 0.2705 - val_acc: 0.9020\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4070 - acc: 0.8131 - val_loss: 0.2765 - val_acc: 0.9019\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4038 - acc: 0.8168 - val_loss: 0.2748 - val_acc: 0.9033\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4047 - acc: 0.8144 - val_loss: 0.2519 - val_acc: 0.9134\n",
      "Epoch 18/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4067 - acc: 0.8140 - val_loss: 0.2711 - val_acc: 0.9049\n",
      "Epoch 19/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4060 - acc: 0.8149 - val_loss: 0.2841 - val_acc: 0.8991\n",
      "Epoch 20/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4048 - acc: 0.8159 - val_loss: 0.2880 - val_acc: 0.8977\n",
      "Epoch 21/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4065 - acc: 0.8154 - val_loss: 0.2627 - val_acc: 0.9066\n",
      "Epoch 22/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4047 - acc: 0.8154 - val_loss: 0.3605 - val_acc: 0.8471\n",
      "Epoch 00022: early stopping\n",
      "6400\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4083 - acc: 0.8128 - val_loss: 0.2711 - val_acc: 0.9054\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4055 - acc: 0.8168 - val_loss: 0.3083 - val_acc: 0.8841\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4027 - acc: 0.8159 - val_loss: 0.2690 - val_acc: 0.9054\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4068 - acc: 0.8124 - val_loss: 0.3243 - val_acc: 0.8795\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4070 - acc: 0.8143 - val_loss: 0.2838 - val_acc: 0.9002\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4049 - acc: 0.8142 - val_loss: 0.3292 - val_acc: 0.8788\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4063 - acc: 0.8141 - val_loss: 0.2702 - val_acc: 0.9068\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4041 - acc: 0.8157 - val_loss: 0.2779 - val_acc: 0.9079\n",
      "Epoch 00008: early stopping\n",
      "6410\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4054 - acc: 0.8167 - val_loss: 0.3284 - val_acc: 0.8662\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4043 - acc: 0.8159 - val_loss: 0.2806 - val_acc: 0.9014\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4058 - acc: 0.8149 - val_loss: 0.2965 - val_acc: 0.8937\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4052 - acc: 0.8145 - val_loss: 0.3057 - val_acc: 0.8875\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4033 - acc: 0.8157 - val_loss: 0.3033 - val_acc: 0.8813\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4038 - acc: 0.8162 - val_loss: 0.3046 - val_acc: 0.8891\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4048 - acc: 0.8151 - val_loss: 0.3640 - val_acc: 0.8463\n",
      "Epoch 00007: early stopping\n",
      "6420\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4087 - acc: 0.8145 - val_loss: 0.2655 - val_acc: 0.9106\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4078 - acc: 0.8148 - val_loss: 0.3137 - val_acc: 0.8794\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4022 - acc: 0.8145 - val_loss: 0.2717 - val_acc: 0.9061\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4032 - acc: 0.8156 - val_loss: 0.2543 - val_acc: 0.9128\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4067 - acc: 0.8155 - val_loss: 0.2650 - val_acc: 0.9105\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4053 - acc: 0.8148 - val_loss: 0.2578 - val_acc: 0.9129\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4066 - acc: 0.8148 - val_loss: 0.3194 - val_acc: 0.8796\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4045 - acc: 0.8162 - val_loss: 0.2981 - val_acc: 0.8933\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4020 - acc: 0.8171 - val_loss: 0.2602 - val_acc: 0.9091\n",
      "Epoch 00009: early stopping\n",
      "6430\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4052 - acc: 0.8163 - val_loss: 0.2888 - val_acc: 0.8940\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4063 - acc: 0.8156 - val_loss: 0.2922 - val_acc: 0.8927\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4047 - acc: 0.8159 - val_loss: 0.2756 - val_acc: 0.9019\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4063 - acc: 0.8156 - val_loss: 0.3014 - val_acc: 0.8888\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4040 - acc: 0.8167 - val_loss: 0.2881 - val_acc: 0.8941\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4010 - acc: 0.8163 - val_loss: 0.2611 - val_acc: 0.9117\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4029 - acc: 0.8153 - val_loss: 0.2760 - val_acc: 0.9038\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4059 - acc: 0.8141 - val_loss: 0.2834 - val_acc: 0.8977\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4023 - acc: 0.8144 - val_loss: 0.2594 - val_acc: 0.9077\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4038 - acc: 0.8149 - val_loss: 0.2879 - val_acc: 0.8967\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4047 - acc: 0.8146 - val_loss: 0.2968 - val_acc: 0.8878\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4041 - acc: 0.8155 - val_loss: 0.2799 - val_acc: 0.9009\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4034 - acc: 0.8165 - val_loss: 0.2566 - val_acc: 0.9102\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4079 - acc: 0.8121 - val_loss: 0.2625 - val_acc: 0.9117\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4039 - acc: 0.8161 - val_loss: 0.2631 - val_acc: 0.9124\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4024 - acc: 0.8164 - val_loss: 0.3011 - val_acc: 0.8897\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4042 - acc: 0.8158 - val_loss: 0.2425 - val_acc: 0.9202\n",
      "Epoch 18/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4058 - acc: 0.8167 - val_loss: 0.2700 - val_acc: 0.9057\n",
      "Epoch 19/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4037 - acc: 0.8167 - val_loss: 0.2844 - val_acc: 0.8933\n",
      "Epoch 20/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4019 - acc: 0.8173 - val_loss: 0.3106 - val_acc: 0.8810\n",
      "Epoch 21/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4008 - acc: 0.8180 - val_loss: 0.2812 - val_acc: 0.8960\n",
      "Epoch 22/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4040 - acc: 0.8171 - val_loss: 0.2628 - val_acc: 0.9074\n",
      "Epoch 00022: early stopping\n",
      "6440\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4016 - acc: 0.8165 - val_loss: 0.2716 - val_acc: 0.9030\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4020 - acc: 0.8167 - val_loss: 0.2955 - val_acc: 0.8923\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4046 - acc: 0.8164 - val_loss: 0.3305 - val_acc: 0.8712\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4040 - acc: 0.8153 - val_loss: 0.2922 - val_acc: 0.8955\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4022 - acc: 0.8168 - val_loss: 0.3666 - val_acc: 0.8562\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3995 - acc: 0.8206 - val_loss: 0.3030 - val_acc: 0.8904\n",
      "Epoch 00006: early stopping\n",
      "6450\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4043 - acc: 0.8162 - val_loss: 0.2605 - val_acc: 0.9093\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4016 - acc: 0.8171 - val_loss: 0.3284 - val_acc: 0.8682\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4031 - acc: 0.8171 - val_loss: 0.3326 - val_acc: 0.8685\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4005 - acc: 0.8174 - val_loss: 0.3868 - val_acc: 0.8339\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4000 - acc: 0.8167 - val_loss: 0.3604 - val_acc: 0.8528\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4026 - acc: 0.8165 - val_loss: 0.2415 - val_acc: 0.9174\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4009 - acc: 0.8165 - val_loss: 0.3513 - val_acc: 0.8496\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4047 - acc: 0.8150 - val_loss: 0.2869 - val_acc: 0.8899\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3981 - acc: 0.8186 - val_loss: 0.3034 - val_acc: 0.8910\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4005 - acc: 0.8179 - val_loss: 0.2517 - val_acc: 0.9150\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4017 - acc: 0.8157 - val_loss: 0.2690 - val_acc: 0.9015\n",
      "Epoch 00011: early stopping\n",
      "6460\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4026 - acc: 0.8155 - val_loss: 0.2748 - val_acc: 0.9005\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4018 - acc: 0.8170 - val_loss: 0.3060 - val_acc: 0.8842\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4015 - acc: 0.8158 - val_loss: 0.2433 - val_acc: 0.9178\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4024 - acc: 0.8151 - val_loss: 0.2692 - val_acc: 0.8984\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4020 - acc: 0.8151 - val_loss: 0.3083 - val_acc: 0.8900\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3992 - acc: 0.8165 - val_loss: 0.2383 - val_acc: 0.9189\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3997 - acc: 0.8158 - val_loss: 0.2710 - val_acc: 0.9012\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8182 - val_loss: 0.3071 - val_acc: 0.8802\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3994 - acc: 0.8158 - val_loss: 0.2605 - val_acc: 0.9058\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4018 - acc: 0.8157 - val_loss: 0.3017 - val_acc: 0.8849\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 1s 5us/step - loss: 0.4030 - acc: 0.8156 - val_loss: 0.2784 - val_acc: 0.8967\n",
      "Epoch 00011: early stopping\n",
      "6470\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4024 - acc: 0.8148 - val_loss: 0.2702 - val_acc: 0.9007\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3999 - acc: 0.8169 - val_loss: 0.2818 - val_acc: 0.8872\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3998 - acc: 0.8155 - val_loss: 0.2585 - val_acc: 0.9077\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4046 - acc: 0.8139 - val_loss: 0.2752 - val_acc: 0.9042\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3996 - acc: 0.8180 - val_loss: 0.3064 - val_acc: 0.8799\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4004 - acc: 0.8165 - val_loss: 0.2925 - val_acc: 0.8955\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4015 - acc: 0.8157 - val_loss: 0.3002 - val_acc: 0.8815\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3992 - acc: 0.8167 - val_loss: 0.3046 - val_acc: 0.8840\n",
      "Epoch 00008: early stopping\n",
      "6480\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4022 - acc: 0.8148 - val_loss: 0.2887 - val_acc: 0.8933\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4044 - acc: 0.8142 - val_loss: 0.3090 - val_acc: 0.8837\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4013 - acc: 0.8179 - val_loss: 0.2542 - val_acc: 0.9170\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4010 - acc: 0.8162 - val_loss: 0.2579 - val_acc: 0.9088\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3993 - acc: 0.8166 - val_loss: 0.2901 - val_acc: 0.8875\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4002 - acc: 0.8163 - val_loss: 0.2720 - val_acc: 0.9081\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3986 - acc: 0.8163 - val_loss: 0.2526 - val_acc: 0.9109\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4004 - acc: 0.8175 - val_loss: 0.2691 - val_acc: 0.9032\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3983 - acc: 0.8179 - val_loss: 0.3392 - val_acc: 0.8657\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4016 - acc: 0.8165 - val_loss: 0.2409 - val_acc: 0.9196\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4022 - acc: 0.8167 - val_loss: 0.2711 - val_acc: 0.9049\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4006 - acc: 0.8147 - val_loss: 0.2912 - val_acc: 0.8931\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4013 - acc: 0.8149 - val_loss: 0.2724 - val_acc: 0.9051\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4017 - acc: 0.8150 - val_loss: 0.2516 - val_acc: 0.9119\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3977 - acc: 0.8166 - val_loss: 0.2830 - val_acc: 0.8939\n",
      "Epoch 00015: early stopping\n",
      "6490\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4008 - acc: 0.8147 - val_loss: 0.2734 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4005 - acc: 0.8171 - val_loss: 0.2868 - val_acc: 0.8931\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3995 - acc: 0.8171 - val_loss: 0.2847 - val_acc: 0.8966\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3986 - acc: 0.8164 - val_loss: 0.2418 - val_acc: 0.9177\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4007 - acc: 0.8167 - val_loss: 0.3202 - val_acc: 0.8755\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4008 - acc: 0.8163 - val_loss: 0.2530 - val_acc: 0.9120\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4032 - acc: 0.8159 - val_loss: 0.2725 - val_acc: 0.9095\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3999 - acc: 0.8165 - val_loss: 0.2991 - val_acc: 0.8887\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4015 - acc: 0.8157 - val_loss: 0.2594 - val_acc: 0.9120\n",
      "Epoch 00009: early stopping\n",
      "6500\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4026 - acc: 0.8151 - val_loss: 0.2571 - val_acc: 0.9167\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4010 - acc: 0.8159 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4003 - acc: 0.8164 - val_loss: 0.2460 - val_acc: 0.9177\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4006 - acc: 0.8156 - val_loss: 0.2538 - val_acc: 0.9160\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4013 - acc: 0.8166 - val_loss: 0.2734 - val_acc: 0.9010\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3969 - acc: 0.8174 - val_loss: 0.2594 - val_acc: 0.9099\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3991 - acc: 0.8164 - val_loss: 0.2627 - val_acc: 0.9082\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4010 - acc: 0.8153 - val_loss: 0.2925 - val_acc: 0.8938\n",
      "Epoch 00008: early stopping\n",
      "6510\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3969 - acc: 0.8184 - val_loss: 0.2777 - val_acc: 0.9010\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3988 - acc: 0.8165 - val_loss: 0.2714 - val_acc: 0.9056\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3998 - acc: 0.8151 - val_loss: 0.2657 - val_acc: 0.9034\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4002 - acc: 0.8164 - val_loss: 0.2707 - val_acc: 0.9058\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3979 - acc: 0.8151 - val_loss: 0.2511 - val_acc: 0.9126\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 1s 5us/step - loss: 0.4021 - acc: 0.8142 - val_loss: 0.2789 - val_acc: 0.8977\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3988 - acc: 0.8157 - val_loss: 0.2904 - val_acc: 0.8921\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4005 - acc: 0.8179 - val_loss: 0.2490 - val_acc: 0.9170\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3995 - acc: 0.8161 - val_loss: 0.2753 - val_acc: 0.9026\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3961 - acc: 0.8187 - val_loss: 0.3302 - val_acc: 0.8614\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4004 - acc: 0.8171 - val_loss: 0.2539 - val_acc: 0.9102\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3999 - acc: 0.8159 - val_loss: 0.2383 - val_acc: 0.9195\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4002 - acc: 0.8152 - val_loss: 0.2734 - val_acc: 0.9043\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3998 - acc: 0.8181 - val_loss: 0.2966 - val_acc: 0.8859\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4000 - acc: 0.8163 - val_loss: 0.2730 - val_acc: 0.9011\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3996 - acc: 0.8152 - val_loss: 0.3151 - val_acc: 0.8705\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4000 - acc: 0.8162 - val_loss: 0.3151 - val_acc: 0.8716\n",
      "Epoch 00017: early stopping\n",
      "6520\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8188 - val_loss: 0.2763 - val_acc: 0.8982\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3983 - acc: 0.8172 - val_loss: 0.2981 - val_acc: 0.8825\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4013 - acc: 0.8146 - val_loss: 0.2965 - val_acc: 0.8918\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8168 - val_loss: 0.2668 - val_acc: 0.9061\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3960 - acc: 0.8198 - val_loss: 0.2451 - val_acc: 0.9164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4000 - acc: 0.8175 - val_loss: 0.2566 - val_acc: 0.9111\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3969 - acc: 0.8185 - val_loss: 0.2720 - val_acc: 0.8986\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3983 - acc: 0.8181 - val_loss: 0.2672 - val_acc: 0.9082\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3996 - acc: 0.8172 - val_loss: 0.2713 - val_acc: 0.9029\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3970 - acc: 0.8177 - val_loss: 0.2767 - val_acc: 0.8979\n",
      "Epoch 00010: early stopping\n",
      "6530\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3982 - acc: 0.8177 - val_loss: 0.2985 - val_acc: 0.8829\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4005 - acc: 0.8173 - val_loss: 0.2858 - val_acc: 0.8947\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4012 - acc: 0.8149 - val_loss: 0.2668 - val_acc: 0.9075\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3987 - acc: 0.8178 - val_loss: 0.3392 - val_acc: 0.8660\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3985 - acc: 0.8174 - val_loss: 0.3050 - val_acc: 0.8812\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3986 - acc: 0.8156 - val_loss: 0.2483 - val_acc: 0.9160\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3973 - acc: 0.8157 - val_loss: 0.2713 - val_acc: 0.9038\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3973 - acc: 0.8170 - val_loss: 0.2496 - val_acc: 0.9124\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3990 - acc: 0.8187 - val_loss: 0.2985 - val_acc: 0.8895\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4013 - acc: 0.8174 - val_loss: 0.2831 - val_acc: 0.8910\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.4001 - acc: 0.8171 - val_loss: 0.2402 - val_acc: 0.9181\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3989 - acc: 0.8163 - val_loss: 0.2772 - val_acc: 0.8959\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3973 - acc: 0.8172 - val_loss: 0.2919 - val_acc: 0.8856\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3976 - acc: 0.8179 - val_loss: 0.2893 - val_acc: 0.8889\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3989 - acc: 0.8189 - val_loss: 0.2629 - val_acc: 0.9080\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3994 - acc: 0.8173 - val_loss: 0.2806 - val_acc: 0.8982\n",
      "Epoch 00016: early stopping\n",
      "6540\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8184 - val_loss: 0.2752 - val_acc: 0.9037\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3974 - acc: 0.8187 - val_loss: 0.2685 - val_acc: 0.9091\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3965 - acc: 0.8187 - val_loss: 0.2764 - val_acc: 0.9006\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3976 - acc: 0.8196 - val_loss: 0.2745 - val_acc: 0.9003\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8188 - val_loss: 0.2758 - val_acc: 0.9009\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3970 - acc: 0.8180 - val_loss: 0.2517 - val_acc: 0.9123\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3976 - acc: 0.8177 - val_loss: 0.2597 - val_acc: 0.9129\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.4006 - acc: 0.8156 - val_loss: 0.2883 - val_acc: 0.8955\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8191 - val_loss: 0.2562 - val_acc: 0.9128\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3966 - acc: 0.8183 - val_loss: 0.2850 - val_acc: 0.8968\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3969 - acc: 0.8161 - val_loss: 0.3045 - val_acc: 0.8865\n",
      "Epoch 00011: early stopping\n",
      "6550\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3987 - acc: 0.8183 - val_loss: 0.2694 - val_acc: 0.9031\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8181 - val_loss: 0.2932 - val_acc: 0.8846\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3943 - acc: 0.8208 - val_loss: 0.2813 - val_acc: 0.9006\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3973 - acc: 0.8164 - val_loss: 0.2882 - val_acc: 0.8864\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8169 - val_loss: 0.2500 - val_acc: 0.9103\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3985 - acc: 0.8172 - val_loss: 0.2545 - val_acc: 0.9136\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3965 - acc: 0.8185 - val_loss: 0.2631 - val_acc: 0.9108\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3977 - acc: 0.8188 - val_loss: 0.3005 - val_acc: 0.8849\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3956 - acc: 0.8197 - val_loss: 0.3003 - val_acc: 0.8835\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3980 - acc: 0.8166 - val_loss: 0.2535 - val_acc: 0.9133\n",
      "Epoch 00010: early stopping\n",
      "6560\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3962 - acc: 0.8166 - val_loss: 0.2340 - val_acc: 0.9204\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8183 - val_loss: 0.2578 - val_acc: 0.9113\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3960 - acc: 0.8166 - val_loss: 0.2843 - val_acc: 0.8974\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3993 - acc: 0.8163 - val_loss: 0.3048 - val_acc: 0.8788\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3967 - acc: 0.8172 - val_loss: 0.2779 - val_acc: 0.8981\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8182 - val_loss: 0.2567 - val_acc: 0.9092\n",
      "Epoch 00006: early stopping\n",
      "6570\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3960 - acc: 0.8191 - val_loss: 0.2630 - val_acc: 0.9093\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3989 - acc: 0.8180 - val_loss: 0.3141 - val_acc: 0.8768\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3969 - acc: 0.8166 - val_loss: 0.2630 - val_acc: 0.9036\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.3962 - acc: 0.8157 - val_loss: 0.2307 - val_acc: 0.9215\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3985 - acc: 0.8149 - val_loss: 0.2563 - val_acc: 0.9052\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8159 - val_loss: 0.2519 - val_acc: 0.9129\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3998 - acc: 0.8174 - val_loss: 0.3027 - val_acc: 0.8869\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3951 - acc: 0.8181 - val_loss: 0.2345 - val_acc: 0.9223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3977 - acc: 0.8165 - val_loss: 0.2440 - val_acc: 0.9168\n",
      "Epoch 00009: early stopping\n",
      "6580\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8188 - val_loss: 0.2544 - val_acc: 0.9124\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3983 - acc: 0.8164 - val_loss: 0.2579 - val_acc: 0.9069\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3968 - acc: 0.8184 - val_loss: 0.2420 - val_acc: 0.9197\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3951 - acc: 0.8178 - val_loss: 0.2870 - val_acc: 0.8932\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3938 - acc: 0.8188 - val_loss: 0.2309 - val_acc: 0.9230\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3946 - acc: 0.8182 - val_loss: 0.2907 - val_acc: 0.8866\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3954 - acc: 0.8188 - val_loss: 0.2490 - val_acc: 0.9148\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3963 - acc: 0.8185 - val_loss: 0.2960 - val_acc: 0.8950\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3968 - acc: 0.8174 - val_loss: 0.2556 - val_acc: 0.9124\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3977 - acc: 0.8172 - val_loss: 0.2695 - val_acc: 0.9029\n",
      "Epoch 00010: early stopping\n",
      "6590\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3967 - acc: 0.8187 - val_loss: 0.2997 - val_acc: 0.8849\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3951 - acc: 0.8203 - val_loss: 0.2816 - val_acc: 0.8977\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8165 - val_loss: 0.2652 - val_acc: 0.9102\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3958 - acc: 0.8185 - val_loss: 0.2279 - val_acc: 0.9267\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3996 - acc: 0.8165 - val_loss: 0.2762 - val_acc: 0.8980\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3954 - acc: 0.8168 - val_loss: 0.3749 - val_acc: 0.8390\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3982 - acc: 0.8160 - val_loss: 0.3007 - val_acc: 0.8813\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3993 - acc: 0.8182 - val_loss: 0.2723 - val_acc: 0.9003\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8173 - val_loss: 0.2713 - val_acc: 0.9046\n",
      "Epoch 00009: early stopping\n",
      "6600\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3980 - acc: 0.8161 - val_loss: 0.2447 - val_acc: 0.9180\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3939 - acc: 0.8184 - val_loss: 0.3176 - val_acc: 0.8714\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8165 - val_loss: 0.2761 - val_acc: 0.8977\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8188 - val_loss: 0.2822 - val_acc: 0.8901\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8164 - val_loss: 0.3148 - val_acc: 0.8747\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3946 - acc: 0.8175 - val_loss: 0.2981 - val_acc: 0.8842\n",
      "Epoch 00006: early stopping\n",
      "6610\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3959 - acc: 0.8176 - val_loss: 0.2425 - val_acc: 0.9179\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3957 - acc: 0.8182 - val_loss: 0.2586 - val_acc: 0.9100\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3944 - acc: 0.8183 - val_loss: 0.2651 - val_acc: 0.9049\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3944 - acc: 0.8204 - val_loss: 0.2530 - val_acc: 0.9143\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8187 - val_loss: 0.2498 - val_acc: 0.9138\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3947 - acc: 0.8177 - val_loss: 0.2469 - val_acc: 0.9135\n",
      "Epoch 00006: early stopping\n",
      "6620\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3961 - acc: 0.8179 - val_loss: 0.2443 - val_acc: 0.9186\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3948 - acc: 0.8182 - val_loss: 0.3406 - val_acc: 0.8590\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3961 - acc: 0.8170 - val_loss: 0.3489 - val_acc: 0.8629\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3970 - acc: 0.8168 - val_loss: 0.3508 - val_acc: 0.8489\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3983 - acc: 0.8171 - val_loss: 0.2755 - val_acc: 0.8963\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3948 - acc: 0.8182 - val_loss: 0.2529 - val_acc: 0.9142\n",
      "Epoch 00006: early stopping\n",
      "6630\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3958 - acc: 0.8175 - val_loss: 0.2569 - val_acc: 0.9147\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3942 - acc: 0.8196 - val_loss: 0.3044 - val_acc: 0.8881\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3971 - acc: 0.8200 - val_loss: 0.2709 - val_acc: 0.9056\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3974 - acc: 0.8166 - val_loss: 0.2563 - val_acc: 0.9122\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3956 - acc: 0.8190 - val_loss: 0.2657 - val_acc: 0.9049\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3964 - acc: 0.8174 - val_loss: 0.2670 - val_acc: 0.9027\n",
      "Epoch 00006: early stopping\n",
      "6640\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3960 - acc: 0.8188 - val_loss: 0.2483 - val_acc: 0.9169\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3963 - acc: 0.8177 - val_loss: 0.2555 - val_acc: 0.9156\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3924 - acc: 0.8188 - val_loss: 0.2483 - val_acc: 0.9161\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8163 - val_loss: 0.3215 - val_acc: 0.8736\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3949 - acc: 0.8200 - val_loss: 0.2407 - val_acc: 0.9171\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3933 - acc: 0.8203 - val_loss: 0.2976 - val_acc: 0.8886\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3953 - acc: 0.8183 - val_loss: 0.2642 - val_acc: 0.9031\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3954 - acc: 0.8181 - val_loss: 0.2381 - val_acc: 0.9171\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3937 - acc: 0.8203 - val_loss: 0.2184 - val_acc: 0.9331\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3946 - acc: 0.8197 - val_loss: 0.2617 - val_acc: 0.9068\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3961 - acc: 0.8181 - val_loss: 0.3352 - val_acc: 0.8730\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3944 - acc: 0.8189 - val_loss: 0.2993 - val_acc: 0.8856\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3959 - acc: 0.8178 - val_loss: 0.2462 - val_acc: 0.9152\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3959 - acc: 0.8181 - val_loss: 0.2382 - val_acc: 0.9194\n",
      "Epoch 00014: early stopping\n",
      "6650\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3972 - acc: 0.8173 - val_loss: 0.2645 - val_acc: 0.9042\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3941 - acc: 0.8190 - val_loss: 0.2400 - val_acc: 0.9188\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3946 - acc: 0.8193 - val_loss: 0.2591 - val_acc: 0.9106\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3905 - acc: 0.8214 - val_loss: 0.2956 - val_acc: 0.8858\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3942 - acc: 0.8203 - val_loss: 0.2533 - val_acc: 0.9153\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3917 - acc: 0.8205 - val_loss: 0.2385 - val_acc: 0.9210\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3946 - acc: 0.8190 - val_loss: 0.2290 - val_acc: 0.9237\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3943 - acc: 0.8192 - val_loss: 0.2366 - val_acc: 0.9181\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3931 - acc: 0.8195 - val_loss: 0.2478 - val_acc: 0.9105\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3922 - acc: 0.8202 - val_loss: 0.2860 - val_acc: 0.8933\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3933 - acc: 0.8213 - val_loss: 0.2674 - val_acc: 0.9047\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3954 - acc: 0.8194 - val_loss: 0.2672 - val_acc: 0.9084\n",
      "Epoch 00012: early stopping\n",
      "6660\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3906 - acc: 0.8204 - val_loss: 0.2445 - val_acc: 0.9147\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3920 - acc: 0.8218 - val_loss: 0.2405 - val_acc: 0.9237\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3936 - acc: 0.8202 - val_loss: 0.2384 - val_acc: 0.9239\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3927 - acc: 0.8205 - val_loss: 0.2578 - val_acc: 0.9157\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3933 - acc: 0.8211 - val_loss: 0.2302 - val_acc: 0.9283\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3911 - acc: 0.8210 - val_loss: 0.2440 - val_acc: 0.9199\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3925 - acc: 0.8233 - val_loss: 0.2508 - val_acc: 0.9166\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3940 - acc: 0.8213 - val_loss: 0.2322 - val_acc: 0.9254\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3903 - acc: 0.8235 - val_loss: 0.2748 - val_acc: 0.9030\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3907 - acc: 0.8211 - val_loss: 0.2506 - val_acc: 0.9160\n",
      "Epoch 00010: early stopping\n",
      "6670\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3915 - acc: 0.8214 - val_loss: 0.2596 - val_acc: 0.9121\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3932 - acc: 0.8197 - val_loss: 0.2251 - val_acc: 0.9263\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3914 - acc: 0.8198 - val_loss: 0.2593 - val_acc: 0.9104\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3904 - acc: 0.8219 - val_loss: 0.2937 - val_acc: 0.8860\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3903 - acc: 0.8222 - val_loss: 0.2263 - val_acc: 0.9269\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3897 - acc: 0.8216 - val_loss: 0.2577 - val_acc: 0.9092\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3882 - acc: 0.8222 - val_loss: 0.2435 - val_acc: 0.9204\n",
      "Epoch 00007: early stopping\n",
      "6680\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3894 - acc: 0.8230 - val_loss: 0.2512 - val_acc: 0.9104\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3884 - acc: 0.8223 - val_loss: 0.2492 - val_acc: 0.9152\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3889 - acc: 0.8214 - val_loss: 0.2376 - val_acc: 0.9206\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3888 - acc: 0.8215 - val_loss: 0.2368 - val_acc: 0.9217\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3884 - acc: 0.8216 - val_loss: 0.2272 - val_acc: 0.9273\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3924 - acc: 0.8204 - val_loss: 0.2296 - val_acc: 0.9294\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3905 - acc: 0.8215 - val_loss: 0.2233 - val_acc: 0.9326\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3899 - acc: 0.8209 - val_loss: 0.2622 - val_acc: 0.9085\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3867 - acc: 0.8217 - val_loss: 0.2416 - val_acc: 0.9214\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3880 - acc: 0.8229 - val_loss: 0.2501 - val_acc: 0.9135\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3906 - acc: 0.8209 - val_loss: 0.2229 - val_acc: 0.9323\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3843 - acc: 0.8246 - val_loss: 0.2640 - val_acc: 0.9041\n",
      "Epoch 00012: early stopping\n",
      "6690\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3847 - acc: 0.8236 - val_loss: 0.2509 - val_acc: 0.9154\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3870 - acc: 0.8220 - val_loss: 0.2835 - val_acc: 0.8899\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3834 - acc: 0.8236 - val_loss: 0.2541 - val_acc: 0.9101\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3833 - acc: 0.8241 - val_loss: 0.2775 - val_acc: 0.8978\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3804 - acc: 0.8255 - val_loss: 0.2422 - val_acc: 0.9272\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3855 - acc: 0.8225 - val_loss: 0.2418 - val_acc: 0.9204\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3865 - acc: 0.8239 - val_loss: 0.2135 - val_acc: 0.9334\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3851 - acc: 0.8245 - val_loss: 0.2227 - val_acc: 0.9330\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3841 - acc: 0.8240 - val_loss: 0.2219 - val_acc: 0.9326\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3860 - acc: 0.8228 - val_loss: 0.2131 - val_acc: 0.9324\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3829 - acc: 0.8229 - val_loss: 0.2436 - val_acc: 0.9204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3856 - acc: 0.8239 - val_loss: 0.2435 - val_acc: 0.9179\n",
      "Epoch 00012: early stopping\n",
      "6700\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3827 - acc: 0.8260 - val_loss: 0.2914 - val_acc: 0.8868\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3836 - acc: 0.8234 - val_loss: 0.2266 - val_acc: 0.9223\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3816 - acc: 0.8227 - val_loss: 0.2744 - val_acc: 0.9027\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3823 - acc: 0.8251 - val_loss: 0.2299 - val_acc: 0.9295\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3836 - acc: 0.8248 - val_loss: 0.2644 - val_acc: 0.8968\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3836 - acc: 0.8254 - val_loss: 0.2639 - val_acc: 0.9075\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3783 - acc: 0.8270 - val_loss: 0.2894 - val_acc: 0.8940\n",
      "Epoch 00007: early stopping\n",
      "6710\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3801 - acc: 0.8281 - val_loss: 0.2279 - val_acc: 0.9280\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3808 - acc: 0.8275 - val_loss: 0.2221 - val_acc: 0.9290\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3836 - acc: 0.8235 - val_loss: 0.2430 - val_acc: 0.9183\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3791 - acc: 0.8252 - val_loss: 0.2713 - val_acc: 0.8978\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3800 - acc: 0.8260 - val_loss: 0.2511 - val_acc: 0.9144\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3785 - acc: 0.8276 - val_loss: 0.2075 - val_acc: 0.9345\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3811 - acc: 0.8264 - val_loss: 0.2129 - val_acc: 0.9380\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3801 - acc: 0.8249 - val_loss: 0.2965 - val_acc: 0.8845\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3768 - acc: 0.8271 - val_loss: 0.2417 - val_acc: 0.9244\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3791 - acc: 0.8254 - val_loss: 0.3094 - val_acc: 0.8701\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3776 - acc: 0.8270 - val_loss: 0.2148 - val_acc: 0.9317\n",
      "Epoch 00011: early stopping\n",
      "6720\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3767 - acc: 0.8278 - val_loss: 0.2185 - val_acc: 0.9297\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3781 - acc: 0.8259 - val_loss: 0.2358 - val_acc: 0.9271\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3788 - acc: 0.8263 - val_loss: 0.2561 - val_acc: 0.9074\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3796 - acc: 0.8267 - val_loss: 0.2402 - val_acc: 0.9137\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3781 - acc: 0.8272 - val_loss: 0.2261 - val_acc: 0.9268\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3790 - acc: 0.8261 - val_loss: 0.2444 - val_acc: 0.9157\n",
      "Epoch 00006: early stopping\n",
      "6730\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3785 - acc: 0.8254 - val_loss: 0.2172 - val_acc: 0.9281\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3816 - acc: 0.8265 - val_loss: 0.2281 - val_acc: 0.9249\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3781 - acc: 0.8264 - val_loss: 0.2598 - val_acc: 0.9047\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3776 - acc: 0.8264 - val_loss: 0.2562 - val_acc: 0.9065\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3789 - acc: 0.8255 - val_loss: 0.2792 - val_acc: 0.9027\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3735 - acc: 0.8267 - val_loss: 0.2331 - val_acc: 0.9231\n",
      "Epoch 00006: early stopping\n",
      "6740\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3785 - acc: 0.8249 - val_loss: 0.2392 - val_acc: 0.9172\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3676 - acc: 0.8306 - val_loss: 0.2367 - val_acc: 0.9174\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3756 - acc: 0.8265 - val_loss: 0.2432 - val_acc: 0.9098\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3745 - acc: 0.8284 - val_loss: 0.2382 - val_acc: 0.9173\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3717 - acc: 0.8297 - val_loss: 0.2240 - val_acc: 0.9221\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3734 - acc: 0.8278 - val_loss: 0.2228 - val_acc: 0.9228\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3718 - acc: 0.8291 - val_loss: 0.2062 - val_acc: 0.9301\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3729 - acc: 0.8285 - val_loss: 0.2135 - val_acc: 0.9298\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3735 - acc: 0.8286 - val_loss: 0.2073 - val_acc: 0.9319\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3715 - acc: 0.8294 - val_loss: 0.2553 - val_acc: 0.9076\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3724 - acc: 0.8273 - val_loss: 0.2221 - val_acc: 0.9230\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3729 - acc: 0.8275 - val_loss: 0.2218 - val_acc: 0.9243\n",
      "Epoch 00012: early stopping\n",
      "6750\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3712 - acc: 0.8296 - val_loss: 0.2150 - val_acc: 0.9293\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3718 - acc: 0.8289 - val_loss: 0.2001 - val_acc: 0.9354\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3724 - acc: 0.8285 - val_loss: 0.2195 - val_acc: 0.9261\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3685 - acc: 0.8327 - val_loss: 0.2110 - val_acc: 0.9279\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3676 - acc: 0.8318 - val_loss: 0.2344 - val_acc: 0.9190\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3680 - acc: 0.8327 - val_loss: 0.2462 - val_acc: 0.9059\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3696 - acc: 0.8315 - val_loss: 0.2146 - val_acc: 0.9271\n",
      "Epoch 00007: early stopping\n",
      "6760\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3683 - acc: 0.8313 - val_loss: 0.2416 - val_acc: 0.9068\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3700 - acc: 0.8313 - val_loss: 0.2268 - val_acc: 0.9210\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3664 - acc: 0.8315 - val_loss: 0.2491 - val_acc: 0.9100\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3663 - acc: 0.8323 - val_loss: 0.1826 - val_acc: 0.9431\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3686 - acc: 0.8306 - val_loss: 0.2044 - val_acc: 0.9336\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3678 - acc: 0.8321 - val_loss: 0.2292 - val_acc: 0.9192\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3665 - acc: 0.8320 - val_loss: 0.2294 - val_acc: 0.9196\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3632 - acc: 0.8341 - val_loss: 0.2596 - val_acc: 0.9048\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3647 - acc: 0.8333 - val_loss: 0.2318 - val_acc: 0.9227\n",
      "Epoch 00009: early stopping\n",
      "6770\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3652 - acc: 0.8329 - val_loss: 0.2183 - val_acc: 0.9242\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3618 - acc: 0.8343 - val_loss: 0.2137 - val_acc: 0.9258\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3647 - acc: 0.8335 - val_loss: 0.1935 - val_acc: 0.9350\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3619 - acc: 0.8345 - val_loss: 0.2011 - val_acc: 0.9317\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3625 - acc: 0.8343 - val_loss: 0.2346 - val_acc: 0.9158\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3621 - acc: 0.8347 - val_loss: 0.2068 - val_acc: 0.9316\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3647 - acc: 0.8333 - val_loss: 0.2506 - val_acc: 0.9069\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3599 - acc: 0.8353 - val_loss: 0.2013 - val_acc: 0.9352\n",
      "Epoch 00008: early stopping\n",
      "6780\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3610 - acc: 0.8351 - val_loss: 0.2324 - val_acc: 0.9181\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3604 - acc: 0.8339 - val_loss: 0.2225 - val_acc: 0.9216\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3603 - acc: 0.8347 - val_loss: 0.2261 - val_acc: 0.9206\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3621 - acc: 0.8353 - val_loss: 0.2614 - val_acc: 0.9022\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3590 - acc: 0.8344 - val_loss: 0.1984 - val_acc: 0.9338\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3582 - acc: 0.8337 - val_loss: 0.2230 - val_acc: 0.9217\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3579 - acc: 0.8352 - val_loss: 0.1886 - val_acc: 0.9391\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3592 - acc: 0.8368 - val_loss: 0.2052 - val_acc: 0.9328\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3551 - acc: 0.8373 - val_loss: 0.2126 - val_acc: 0.9308\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3556 - acc: 0.8365 - val_loss: 0.2066 - val_acc: 0.9314\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3575 - acc: 0.8356 - val_loss: 0.1900 - val_acc: 0.9426\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3602 - acc: 0.8355 - val_loss: 0.2298 - val_acc: 0.9193\n",
      "Epoch 00012: early stopping\n",
      "0.9197188309285979 6780\n",
      "6790\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3592 - acc: 0.8354 - val_loss: 0.2496 - val_acc: 0.9046\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3556 - acc: 0.8388 - val_loss: 0.2033 - val_acc: 0.9312\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3536 - acc: 0.8383 - val_loss: 0.2640 - val_acc: 0.9057\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3546 - acc: 0.8384 - val_loss: 0.1965 - val_acc: 0.9350\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3535 - acc: 0.8380 - val_loss: 0.1976 - val_acc: 0.9338\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3536 - acc: 0.8392 - val_loss: 0.2494 - val_acc: 0.9093\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3566 - acc: 0.8379 - val_loss: 0.2148 - val_acc: 0.9291\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3557 - acc: 0.8393 - val_loss: 0.1814 - val_acc: 0.9417\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3590 - acc: 0.8374 - val_loss: 0.1946 - val_acc: 0.9355\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3543 - acc: 0.8381 - val_loss: 0.2148 - val_acc: 0.9267\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3528 - acc: 0.8393 - val_loss: 0.2339 - val_acc: 0.9172\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3560 - acc: 0.8354 - val_loss: 0.1897 - val_acc: 0.9397\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3560 - acc: 0.8369 - val_loss: 0.2223 - val_acc: 0.9214\n",
      "Epoch 00013: early stopping\n",
      "6800\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3540 - acc: 0.8372 - val_loss: 0.2294 - val_acc: 0.9170\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3538 - acc: 0.8376 - val_loss: 0.2103 - val_acc: 0.9314\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3502 - acc: 0.8396 - val_loss: 0.1986 - val_acc: 0.9363\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3536 - acc: 0.8382 - val_loss: 0.2418 - val_acc: 0.9191\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3541 - acc: 0.8385 - val_loss: 0.1941 - val_acc: 0.9363\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3492 - acc: 0.8390 - val_loss: 0.2003 - val_acc: 0.9356\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3503 - acc: 0.8394 - val_loss: 0.1895 - val_acc: 0.9373\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3526 - acc: 0.8388 - val_loss: 0.2299 - val_acc: 0.9174\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3481 - acc: 0.8410 - val_loss: 0.1741 - val_acc: 0.9450\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3505 - acc: 0.8376 - val_loss: 0.2377 - val_acc: 0.9116\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3513 - acc: 0.8397 - val_loss: 0.1783 - val_acc: 0.9414\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3501 - acc: 0.8391 - val_loss: 0.2262 - val_acc: 0.9186\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3500 - acc: 0.8404 - val_loss: 0.1922 - val_acc: 0.9330\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3513 - acc: 0.8390 - val_loss: 0.1917 - val_acc: 0.9362\n",
      "Epoch 00014: early stopping\n",
      "6810\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3532 - acc: 0.8388 - val_loss: 0.2035 - val_acc: 0.9266\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3488 - acc: 0.8394 - val_loss: 0.2120 - val_acc: 0.9222\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3515 - acc: 0.8382 - val_loss: 0.2456 - val_acc: 0.9064\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3502 - acc: 0.8385 - val_loss: 0.1958 - val_acc: 0.9360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3479 - acc: 0.8402 - val_loss: 0.2190 - val_acc: 0.9260\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3488 - acc: 0.8393 - val_loss: 0.1800 - val_acc: 0.9447\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3498 - acc: 0.8392 - val_loss: 0.2369 - val_acc: 0.9151\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3486 - acc: 0.8417 - val_loss: 0.2178 - val_acc: 0.9253\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3500 - acc: 0.8408 - val_loss: 0.1878 - val_acc: 0.9390\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3504 - acc: 0.8415 - val_loss: 0.1831 - val_acc: 0.9427\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3477 - acc: 0.8423 - val_loss: 0.2178 - val_acc: 0.9232\n",
      "Epoch 00011: early stopping\n",
      "0.9230484646688865 6810\n",
      "6820\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3494 - acc: 0.8417 - val_loss: 0.1938 - val_acc: 0.9380\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3494 - acc: 0.8409 - val_loss: 0.2317 - val_acc: 0.9161\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3477 - acc: 0.8406 - val_loss: 0.2088 - val_acc: 0.9276\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3488 - acc: 0.8410 - val_loss: 0.2051 - val_acc: 0.9312\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3459 - acc: 0.8424 - val_loss: 0.2043 - val_acc: 0.9323\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3501 - acc: 0.8413 - val_loss: 0.2094 - val_acc: 0.9293\n",
      "Epoch 00006: early stopping\n",
      "6830\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3468 - acc: 0.8404 - val_loss: 0.1851 - val_acc: 0.9379\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3480 - acc: 0.8411 - val_loss: 0.1914 - val_acc: 0.9363\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3469 - acc: 0.8408 - val_loss: 0.2077 - val_acc: 0.9297\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3452 - acc: 0.8421 - val_loss: 0.2423 - val_acc: 0.9111\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3481 - acc: 0.8403 - val_loss: 0.2146 - val_acc: 0.9273\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3473 - acc: 0.8400 - val_loss: 0.2078 - val_acc: 0.9264\n",
      "Epoch 00006: early stopping\n",
      "6840\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3466 - acc: 0.8414 - val_loss: 0.1742 - val_acc: 0.9451\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3474 - acc: 0.8402 - val_loss: 0.1936 - val_acc: 0.9352\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3477 - acc: 0.8412 - val_loss: 0.2035 - val_acc: 0.9346\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3473 - acc: 0.8422 - val_loss: 0.1742 - val_acc: 0.9446\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3463 - acc: 0.8435 - val_loss: 0.1881 - val_acc: 0.9402\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3429 - acc: 0.8427 - val_loss: 0.2578 - val_acc: 0.9068\n",
      "Epoch 00006: early stopping\n",
      "6850\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3445 - acc: 0.8420 - val_loss: 0.2008 - val_acc: 0.9296\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3476 - acc: 0.8403 - val_loss: 0.2075 - val_acc: 0.9284\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3478 - acc: 0.8396 - val_loss: 0.1963 - val_acc: 0.9343\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3499 - acc: 0.8397 - val_loss: 0.1928 - val_acc: 0.9356\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3450 - acc: 0.8421 - val_loss: 0.2285 - val_acc: 0.9162\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3501 - acc: 0.8391 - val_loss: 0.2340 - val_acc: 0.9171\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3452 - acc: 0.8408 - val_loss: 0.1930 - val_acc: 0.9392\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3455 - acc: 0.8427 - val_loss: 0.2256 - val_acc: 0.9217\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3459 - acc: 0.8421 - val_loss: 0.1932 - val_acc: 0.9361\n",
      "Epoch 00009: early stopping\n",
      "6860\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3489 - acc: 0.8412 - val_loss: 0.1683 - val_acc: 0.9486\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3470 - acc: 0.8416 - val_loss: 0.1917 - val_acc: 0.9337\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3466 - acc: 0.8427 - val_loss: 0.2456 - val_acc: 0.9112\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3501 - acc: 0.8410 - val_loss: 0.2281 - val_acc: 0.9186\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3460 - acc: 0.8411 - val_loss: 0.1855 - val_acc: 0.9415\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3480 - acc: 0.8407 - val_loss: 0.1861 - val_acc: 0.9399\n",
      "Epoch 00006: early stopping\n",
      "6870\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3444 - acc: 0.8418 - val_loss: 0.2215 - val_acc: 0.9206\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3482 - acc: 0.8411 - val_loss: 0.1703 - val_acc: 0.9486\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3469 - acc: 0.8435 - val_loss: 0.2305 - val_acc: 0.9170\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3480 - acc: 0.8416 - val_loss: 0.1930 - val_acc: 0.9383\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3462 - acc: 0.8407 - val_loss: 0.1745 - val_acc: 0.9472\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3445 - acc: 0.8423 - val_loss: 0.2006 - val_acc: 0.9287\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3453 - acc: 0.8400 - val_loss: 0.2075 - val_acc: 0.9346\n",
      "Epoch 00007: early stopping\n",
      "6880\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3432 - acc: 0.8415 - val_loss: 0.2021 - val_acc: 0.9291\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3441 - acc: 0.8434 - val_loss: 0.1803 - val_acc: 0.9412\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3430 - acc: 0.8426 - val_loss: 0.1909 - val_acc: 0.9399\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3424 - acc: 0.8427 - val_loss: 0.1940 - val_acc: 0.9371\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8425 - val_loss: 0.1863 - val_acc: 0.9400\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8432 - val_loss: 0.1912 - val_acc: 0.9363\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3451 - acc: 0.8407 - val_loss: 0.1877 - val_acc: 0.9372\n",
      "Epoch 00007: early stopping\n",
      "6890\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3420 - acc: 0.8430 - val_loss: 0.1972 - val_acc: 0.9340\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3457 - acc: 0.8420 - val_loss: 0.1792 - val_acc: 0.9407\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3437 - acc: 0.8413 - val_loss: 0.1810 - val_acc: 0.9410\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3425 - acc: 0.8430 - val_loss: 0.2006 - val_acc: 0.9314\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3477 - acc: 0.8412 - val_loss: 0.1872 - val_acc: 0.9411\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3430 - acc: 0.8423 - val_loss: 0.1670 - val_acc: 0.9473\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8423 - val_loss: 0.1666 - val_acc: 0.9481\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3442 - acc: 0.8415 - val_loss: 0.1983 - val_acc: 0.9302\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3413 - acc: 0.8410 - val_loss: 0.1742 - val_acc: 0.9424\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8413 - val_loss: 0.1854 - val_acc: 0.9396\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8412 - val_loss: 0.1894 - val_acc: 0.9382\n",
      "Epoch 00011: early stopping\n",
      "6900\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3438 - acc: 0.8403 - val_loss: 0.1937 - val_acc: 0.9332\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3427 - acc: 0.8404 - val_loss: 0.1810 - val_acc: 0.9400\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3433 - acc: 0.8421 - val_loss: 0.1853 - val_acc: 0.9390\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3447 - acc: 0.8422 - val_loss: 0.1746 - val_acc: 0.9418\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3436 - acc: 0.8422 - val_loss: 0.1814 - val_acc: 0.9428\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3422 - acc: 0.8432 - val_loss: 0.1933 - val_acc: 0.9339\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3441 - acc: 0.8419 - val_loss: 0.2033 - val_acc: 0.9284\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3421 - acc: 0.8425 - val_loss: 0.1844 - val_acc: 0.9413\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3465 - acc: 0.8410 - val_loss: 0.1928 - val_acc: 0.9366\n",
      "Epoch 00009: early stopping\n",
      "6910\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8412 - val_loss: 0.1819 - val_acc: 0.9396\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3457 - acc: 0.8410 - val_loss: 0.1777 - val_acc: 0.9419\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3419 - acc: 0.8432 - val_loss: 0.1899 - val_acc: 0.9358\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3406 - acc: 0.8412 - val_loss: 0.1932 - val_acc: 0.9352\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3442 - acc: 0.8421 - val_loss: 0.1916 - val_acc: 0.9372\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3438 - acc: 0.8418 - val_loss: 0.1879 - val_acc: 0.9382\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3449 - acc: 0.8423 - val_loss: 0.1950 - val_acc: 0.9354\n",
      "Epoch 00007: early stopping\n",
      "6920\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3425 - acc: 0.8430 - val_loss: 0.1947 - val_acc: 0.9351\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8429 - val_loss: 0.2159 - val_acc: 0.9235\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3443 - acc: 0.8425 - val_loss: 0.1959 - val_acc: 0.9352\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3402 - acc: 0.8437 - val_loss: 0.2074 - val_acc: 0.9284\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3415 - acc: 0.8435 - val_loss: 0.1807 - val_acc: 0.9396\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3437 - acc: 0.8413 - val_loss: 0.1703 - val_acc: 0.9453\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3442 - acc: 0.8417 - val_loss: 0.1930 - val_acc: 0.9367\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3418 - acc: 0.8428 - val_loss: 0.1923 - val_acc: 0.9346\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3432 - acc: 0.8410 - val_loss: 0.1900 - val_acc: 0.9367\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3434 - acc: 0.8426 - val_loss: 0.2086 - val_acc: 0.9289\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3419 - acc: 0.8409 - val_loss: 0.1806 - val_acc: 0.9442\n",
      "Epoch 00011: early stopping\n",
      "6930\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8412 - val_loss: 0.1661 - val_acc: 0.9493\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3446 - acc: 0.8423 - val_loss: 0.1687 - val_acc: 0.9469\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3430 - acc: 0.8413 - val_loss: 0.1820 - val_acc: 0.9422\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3437 - acc: 0.8415 - val_loss: 0.2037 - val_acc: 0.9301\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3444 - acc: 0.8421 - val_loss: 0.1784 - val_acc: 0.9444\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3422 - acc: 0.8424 - val_loss: 0.1799 - val_acc: 0.9438\n",
      "Epoch 00006: early stopping\n",
      "6940\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3420 - acc: 0.8414 - val_loss: 0.1700 - val_acc: 0.9465\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3402 - acc: 0.8429 - val_loss: 0.2050 - val_acc: 0.9293\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3427 - acc: 0.8428 - val_loss: 0.1660 - val_acc: 0.9467\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8426 - val_loss: 0.1776 - val_acc: 0.9448\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3413 - acc: 0.8427 - val_loss: 0.1709 - val_acc: 0.9436\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3398 - acc: 0.8451 - val_loss: 0.1905 - val_acc: 0.9385\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3418 - acc: 0.8429 - val_loss: 0.1812 - val_acc: 0.9411\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3408 - acc: 0.8424 - val_loss: 0.2252 - val_acc: 0.9212\n",
      "Epoch 00008: early stopping\n",
      "6950\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3403 - acc: 0.8434 - val_loss: 0.2647 - val_acc: 0.8949\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3444 - acc: 0.8413 - val_loss: 0.1714 - val_acc: 0.9461\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - ETA: 0s - loss: 0.3395 - acc: 0.842 - 0s 3us/step - loss: 0.3400 - acc: 0.8428 - val_loss: 0.1909 - val_acc: 0.9349\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3414 - acc: 0.8435 - val_loss: 0.1688 - val_acc: 0.9446\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3437 - acc: 0.8435 - val_loss: 0.1944 - val_acc: 0.9348\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3441 - acc: 0.8421 - val_loss: 0.2250 - val_acc: 0.9221\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3421 - acc: 0.8447 - val_loss: 0.1753 - val_acc: 0.9443\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3413 - acc: 0.8455 - val_loss: 0.1752 - val_acc: 0.9424\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3428 - acc: 0.8436 - val_loss: 0.1721 - val_acc: 0.9436\n",
      "Epoch 00009: early stopping\n",
      "6960\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3407 - acc: 0.8443 - val_loss: 0.1758 - val_acc: 0.9412\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3425 - acc: 0.8418 - val_loss: 0.2480 - val_acc: 0.9065\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3417 - acc: 0.8429 - val_loss: 0.1709 - val_acc: 0.9441\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3404 - acc: 0.8442 - val_loss: 0.2419 - val_acc: 0.9124\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3421 - acc: 0.8452 - val_loss: 0.1828 - val_acc: 0.9389\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3370 - acc: 0.8452 - val_loss: 0.1751 - val_acc: 0.9440\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3384 - acc: 0.8442 - val_loss: 0.1964 - val_acc: 0.9333\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3420 - acc: 0.8430 - val_loss: 0.1685 - val_acc: 0.9456\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3414 - acc: 0.8462 - val_loss: 0.1789 - val_acc: 0.9408\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8437 - val_loss: 0.1842 - val_acc: 0.9398\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3401 - acc: 0.8434 - val_loss: 0.1723 - val_acc: 0.9456\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3437 - acc: 0.8420 - val_loss: 0.2065 - val_acc: 0.9307\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3409 - acc: 0.8456 - val_loss: 0.2234 - val_acc: 0.9206\n",
      "Epoch 00013: early stopping\n",
      "6970\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3424 - acc: 0.8437 - val_loss: 0.2119 - val_acc: 0.9280\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3395 - acc: 0.8446 - val_loss: 0.2011 - val_acc: 0.9316\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8455 - val_loss: 0.2078 - val_acc: 0.9303\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3421 - acc: 0.8428 - val_loss: 0.1695 - val_acc: 0.9455\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3401 - acc: 0.8433 - val_loss: 0.2046 - val_acc: 0.9306\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3409 - acc: 0.8448 - val_loss: 0.1801 - val_acc: 0.9426\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3380 - acc: 0.8435 - val_loss: 0.1729 - val_acc: 0.9449\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3390 - acc: 0.8439 - val_loss: 0.1856 - val_acc: 0.9392\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3431 - acc: 0.8420 - val_loss: 0.1778 - val_acc: 0.9422\n",
      "Epoch 00009: early stopping\n",
      "6980\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3410 - acc: 0.8425 - val_loss: 0.1914 - val_acc: 0.9382\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3387 - acc: 0.8433 - val_loss: 0.1754 - val_acc: 0.9457\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3416 - acc: 0.8429 - val_loss: 0.1734 - val_acc: 0.9467\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3361 - acc: 0.8439 - val_loss: 0.1614 - val_acc: 0.9504\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3432 - acc: 0.8419 - val_loss: 0.1892 - val_acc: 0.9374\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3415 - acc: 0.8437 - val_loss: 0.1784 - val_acc: 0.9421\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3428 - acc: 0.8426 - val_loss: 0.1740 - val_acc: 0.9441\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3424 - acc: 0.8433 - val_loss: 0.1747 - val_acc: 0.9452\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3396 - acc: 0.8439 - val_loss: 0.1807 - val_acc: 0.9447\n",
      "Epoch 00009: early stopping\n",
      "6990\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3402 - acc: 0.8446 - val_loss: 0.2031 - val_acc: 0.9321\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3395 - acc: 0.8441 - val_loss: 0.1756 - val_acc: 0.9468\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3379 - acc: 0.8453 - val_loss: 0.2336 - val_acc: 0.9174\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3372 - acc: 0.8460 - val_loss: 0.1911 - val_acc: 0.9365\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3421 - acc: 0.8441 - val_loss: 0.1892 - val_acc: 0.9353\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3398 - acc: 0.8436 - val_loss: 0.1775 - val_acc: 0.9416\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3411 - acc: 0.8462 - val_loss: 0.2037 - val_acc: 0.9295\n",
      "Epoch 00007: early stopping\n",
      "7000\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8461 - val_loss: 0.2015 - val_acc: 0.9350\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3405 - acc: 0.8446 - val_loss: 0.2120 - val_acc: 0.9304\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8456 - val_loss: 0.2038 - val_acc: 0.9323\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3420 - acc: 0.8440 - val_loss: 0.1814 - val_acc: 0.9420\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3374 - acc: 0.8461 - val_loss: 0.1865 - val_acc: 0.9413\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3400 - acc: 0.8449 - val_loss: 0.2029 - val_acc: 0.9326\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3400 - acc: 0.8454 - val_loss: 0.1926 - val_acc: 0.9366\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3400 - acc: 0.8443 - val_loss: 0.1742 - val_acc: 0.9441\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3409 - acc: 0.8452 - val_loss: 0.1738 - val_acc: 0.9448\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3412 - acc: 0.8445 - val_loss: 0.2021 - val_acc: 0.9306\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3404 - acc: 0.8447 - val_loss: 0.1675 - val_acc: 0.9484\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3410 - acc: 0.8450 - val_loss: 0.1923 - val_acc: 0.9368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3403 - acc: 0.8434 - val_loss: 0.1741 - val_acc: 0.9450\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3367 - acc: 0.8467 - val_loss: 0.1903 - val_acc: 0.9373\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3397 - acc: 0.8436 - val_loss: 0.1888 - val_acc: 0.9382\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3387 - acc: 0.8424 - val_loss: 0.1731 - val_acc: 0.9429\n",
      "Epoch 00016: early stopping\n",
      "7010\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3368 - acc: 0.8448 - val_loss: 0.1789 - val_acc: 0.9429\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3407 - acc: 0.8454 - val_loss: 0.1796 - val_acc: 0.9435\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3373 - acc: 0.8440 - val_loss: 0.1715 - val_acc: 0.9481\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3373 - acc: 0.8447 - val_loss: 0.1791 - val_acc: 0.9414\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3382 - acc: 0.8459 - val_loss: 0.1786 - val_acc: 0.9438\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3379 - acc: 0.8463 - val_loss: 0.1680 - val_acc: 0.9454\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3378 - acc: 0.8449 - val_loss: 0.1599 - val_acc: 0.9507\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3398 - acc: 0.8443 - val_loss: 0.1903 - val_acc: 0.9389\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3386 - acc: 0.8431 - val_loss: 0.1752 - val_acc: 0.9436\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3371 - acc: 0.8461 - val_loss: 0.1650 - val_acc: 0.9481\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3366 - acc: 0.8461 - val_loss: 0.1683 - val_acc: 0.9463\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3384 - acc: 0.8446 - val_loss: 0.1781 - val_acc: 0.9400\n",
      "Epoch 00012: early stopping\n",
      "7020\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8445 - val_loss: 0.1750 - val_acc: 0.9445\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3397 - acc: 0.8442 - val_loss: 0.2147 - val_acc: 0.9271\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3375 - acc: 0.8451 - val_loss: 0.2072 - val_acc: 0.9308\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3372 - acc: 0.8463 - val_loss: 0.1996 - val_acc: 0.9330\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3405 - acc: 0.8442 - val_loss: 0.1685 - val_acc: 0.9490\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3374 - acc: 0.8467 - val_loss: 0.1854 - val_acc: 0.9407\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3398 - acc: 0.8443 - val_loss: 0.2016 - val_acc: 0.9321\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3385 - acc: 0.8438 - val_loss: 0.1728 - val_acc: 0.9458\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3393 - acc: 0.8437 - val_loss: 0.1835 - val_acc: 0.9405\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3363 - acc: 0.8461 - val_loss: 0.1865 - val_acc: 0.9390\n",
      "Epoch 00010: early stopping\n",
      "7030\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3415 - acc: 0.8430 - val_loss: 0.2013 - val_acc: 0.9304\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3400 - acc: 0.8431 - val_loss: 0.1588 - val_acc: 0.9514\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3399 - acc: 0.8429 - val_loss: 0.1893 - val_acc: 0.9357\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8463 - val_loss: 0.1700 - val_acc: 0.9455\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8458 - val_loss: 0.2082 - val_acc: 0.9281\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3396 - acc: 0.8437 - val_loss: 0.1599 - val_acc: 0.9518\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8449 - val_loss: 0.2016 - val_acc: 0.9315\n",
      "Epoch 00007: early stopping\n",
      "0.925638179800222 7030\n",
      "7040\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3372 - acc: 0.8446 - val_loss: 0.1787 - val_acc: 0.9418\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3386 - acc: 0.8466 - val_loss: 0.1727 - val_acc: 0.9442\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3386 - acc: 0.8452 - val_loss: 0.1929 - val_acc: 0.9377\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3374 - acc: 0.8440 - val_loss: 0.1715 - val_acc: 0.9455\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3385 - acc: 0.8427 - val_loss: 0.1653 - val_acc: 0.9509\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3378 - acc: 0.8439 - val_loss: 0.1690 - val_acc: 0.9451\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3358 - acc: 0.8458 - val_loss: 0.1607 - val_acc: 0.9491\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3407 - acc: 0.8433 - val_loss: 0.2079 - val_acc: 0.9275\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8451 - val_loss: 0.1693 - val_acc: 0.9463\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3383 - acc: 0.8435 - val_loss: 0.1705 - val_acc: 0.9467\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3370 - acc: 0.8444 - val_loss: 0.1697 - val_acc: 0.9453\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3377 - acc: 0.8430 - val_loss: 0.2189 - val_acc: 0.9230\n",
      "Epoch 00012: early stopping\n",
      "7050\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3355 - acc: 0.8441 - val_loss: 0.1661 - val_acc: 0.9486\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3360 - acc: 0.8447 - val_loss: 0.1713 - val_acc: 0.9447\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3371 - acc: 0.8450 - val_loss: 0.1783 - val_acc: 0.9406\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3405 - acc: 0.8430 - val_loss: 0.1965 - val_acc: 0.9340\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3379 - acc: 0.8438 - val_loss: 0.1706 - val_acc: 0.9451\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3373 - acc: 0.8450 - val_loss: 0.2245 - val_acc: 0.9211\n",
      "Epoch 00006: early stopping\n",
      "7060\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3393 - acc: 0.8448 - val_loss: 0.1759 - val_acc: 0.9456\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3384 - acc: 0.8446 - val_loss: 0.1717 - val_acc: 0.9476\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3418 - acc: 0.8440 - val_loss: 0.1723 - val_acc: 0.9480\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8469 - val_loss: 0.1821 - val_acc: 0.9446\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8464 - val_loss: 0.1632 - val_acc: 0.9477\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8460 - val_loss: 0.1845 - val_acc: 0.9408\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8472 - val_loss: 0.1710 - val_acc: 0.9446\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3396 - acc: 0.8451 - val_loss: 0.1749 - val_acc: 0.9454\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3398 - acc: 0.8458 - val_loss: 0.1682 - val_acc: 0.9483\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3381 - acc: 0.8464 - val_loss: 0.1659 - val_acc: 0.9508\n",
      "Epoch 00010: early stopping\n",
      "7070\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8457 - val_loss: 0.1924 - val_acc: 0.9390\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3380 - acc: 0.8463 - val_loss: 0.2004 - val_acc: 0.9364\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3396 - acc: 0.8433 - val_loss: 0.1959 - val_acc: 0.9372\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8475 - val_loss: 0.2227 - val_acc: 0.9230\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3365 - acc: 0.8468 - val_loss: 0.1622 - val_acc: 0.9481\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3390 - acc: 0.8435 - val_loss: 0.1683 - val_acc: 0.9449\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3374 - acc: 0.8457 - val_loss: 0.1696 - val_acc: 0.9468\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3408 - acc: 0.8442 - val_loss: 0.1773 - val_acc: 0.9425\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8465 - val_loss: 0.1928 - val_acc: 0.9336\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3380 - acc: 0.8446 - val_loss: 0.1726 - val_acc: 0.9467\n",
      "Epoch 00010: early stopping\n",
      "7080\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3382 - acc: 0.8457 - val_loss: 0.1974 - val_acc: 0.9353\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8459 - val_loss: 0.1850 - val_acc: 0.9397\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3395 - acc: 0.8454 - val_loss: 0.2145 - val_acc: 0.9258\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3390 - acc: 0.8454 - val_loss: 0.1960 - val_acc: 0.9368\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3384 - acc: 0.8472 - val_loss: 0.1982 - val_acc: 0.9350\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3412 - acc: 0.8422 - val_loss: 0.2161 - val_acc: 0.9255\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3382 - acc: 0.8441 - val_loss: 0.1758 - val_acc: 0.9437\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3367 - acc: 0.8465 - val_loss: 0.2093 - val_acc: 0.9282\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3378 - acc: 0.8461 - val_loss: 0.1574 - val_acc: 0.9517\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3400 - acc: 0.8455 - val_loss: 0.1991 - val_acc: 0.9319\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3351 - acc: 0.8461 - val_loss: 0.1817 - val_acc: 0.9417\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3352 - acc: 0.8474 - val_loss: 0.1761 - val_acc: 0.9444\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3374 - acc: 0.8461 - val_loss: 0.1726 - val_acc: 0.9460\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3379 - acc: 0.8434 - val_loss: 0.2128 - val_acc: 0.9276\n",
      "Epoch 00014: early stopping\n",
      "7090\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3380 - acc: 0.8446 - val_loss: 0.1646 - val_acc: 0.9498\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3363 - acc: 0.8467 - val_loss: 0.2067 - val_acc: 0.9294\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3355 - acc: 0.8452 - val_loss: 0.1878 - val_acc: 0.9411\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3353 - acc: 0.8460 - val_loss: 0.1980 - val_acc: 0.9330\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3366 - acc: 0.8449 - val_loss: 0.1849 - val_acc: 0.9411\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3402 - acc: 0.8457 - val_loss: 0.1860 - val_acc: 0.9401\n",
      "Epoch 00006: early stopping\n",
      "7100\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3388 - acc: 0.8464 - val_loss: 0.1816 - val_acc: 0.9432\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3399 - acc: 0.8435 - val_loss: 0.1892 - val_acc: 0.9372\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3351 - acc: 0.8480 - val_loss: 0.2311 - val_acc: 0.9181\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3391 - acc: 0.8452 - val_loss: 0.2391 - val_acc: 0.9087\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3369 - acc: 0.8473 - val_loss: 0.1779 - val_acc: 0.9432\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3393 - acc: 0.8447 - val_loss: 0.1705 - val_acc: 0.9477\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8454 - val_loss: 0.2136 - val_acc: 0.9266\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3373 - acc: 0.8442 - val_loss: 0.1849 - val_acc: 0.9364\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3359 - acc: 0.8468 - val_loss: 0.2045 - val_acc: 0.9304\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8460 - val_loss: 0.2498 - val_acc: 0.9076\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3389 - acc: 0.8442 - val_loss: 0.1855 - val_acc: 0.9341\n",
      "Epoch 00011: early stopping\n",
      "7110\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3397 - acc: 0.8438 - val_loss: 0.1832 - val_acc: 0.9444\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3367 - acc: 0.8449 - val_loss: 0.2117 - val_acc: 0.9282\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3386 - acc: 0.8431 - val_loss: 0.1708 - val_acc: 0.9451\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3378 - acc: 0.8439 - val_loss: 0.2108 - val_acc: 0.9252\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3368 - acc: 0.8470 - val_loss: 0.2013 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8446 - val_loss: 0.1708 - val_acc: 0.9453\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8462 - val_loss: 0.1947 - val_acc: 0.9348\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3399 - acc: 0.8455 - val_loss: 0.2603 - val_acc: 0.9068\n",
      "Epoch 00008: early stopping\n",
      "7120\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8460 - val_loss: 0.2534 - val_acc: 0.9044\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3355 - acc: 0.8461 - val_loss: 0.1883 - val_acc: 0.9407\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3395 - acc: 0.8433 - val_loss: 0.1996 - val_acc: 0.9324\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3390 - acc: 0.8454 - val_loss: 0.2389 - val_acc: 0.9168\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3372 - acc: 0.8459 - val_loss: 0.1822 - val_acc: 0.9437\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3375 - acc: 0.8459 - val_loss: 0.2780 - val_acc: 0.9009\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3365 - acc: 0.8463 - val_loss: 0.1972 - val_acc: 0.9314\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3358 - acc: 0.8451 - val_loss: 0.1648 - val_acc: 0.9492\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3365 - acc: 0.8455 - val_loss: 0.1762 - val_acc: 0.9426\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3382 - acc: 0.8441 - val_loss: 0.1613 - val_acc: 0.9502\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3353 - acc: 0.8464 - val_loss: 0.2037 - val_acc: 0.9315\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3359 - acc: 0.8447 - val_loss: 0.2081 - val_acc: 0.9263\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3352 - acc: 0.8462 - val_loss: 0.2026 - val_acc: 0.9332\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3458 - acc: 0.8421 - val_loss: 0.1778 - val_acc: 0.9410\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3345 - acc: 0.8441 - val_loss: 0.2147 - val_acc: 0.9270\n",
      "Epoch 00015: early stopping\n",
      "7130\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3357 - acc: 0.8458 - val_loss: 0.2275 - val_acc: 0.9215\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8445 - val_loss: 0.1971 - val_acc: 0.9341\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3364 - acc: 0.8454 - val_loss: 0.1702 - val_acc: 0.9460\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3345 - acc: 0.8462 - val_loss: 0.1915 - val_acc: 0.9362\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3349 - acc: 0.8445 - val_loss: 0.1925 - val_acc: 0.9344\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3359 - acc: 0.8457 - val_loss: 0.1872 - val_acc: 0.9360\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3358 - acc: 0.8460 - val_loss: 0.1862 - val_acc: 0.9370\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3358 - acc: 0.8454 - val_loss: 0.2116 - val_acc: 0.9274\n",
      "Epoch 00008: early stopping\n",
      "7140\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8457 - val_loss: 0.1855 - val_acc: 0.9360\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3351 - acc: 0.8440 - val_loss: 0.1943 - val_acc: 0.9347\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8457 - val_loss: 0.1819 - val_acc: 0.9403\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8465 - val_loss: 0.1614 - val_acc: 0.9509\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8461 - val_loss: 0.1773 - val_acc: 0.9413\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8473 - val_loss: 0.1693 - val_acc: 0.9469\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8471 - val_loss: 0.1815 - val_acc: 0.9405\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8457 - val_loss: 0.2035 - val_acc: 0.9321\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8469 - val_loss: 0.1839 - val_acc: 0.9382\n",
      "Epoch 00009: early stopping\n",
      "7150\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3356 - acc: 0.8443 - val_loss: 0.2137 - val_acc: 0.9232\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8464 - val_loss: 0.1703 - val_acc: 0.9441\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8464 - val_loss: 0.1895 - val_acc: 0.9345\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8438 - val_loss: 0.2126 - val_acc: 0.9216\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3323 - acc: 0.8458 - val_loss: 0.2026 - val_acc: 0.9277\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3331 - acc: 0.8465 - val_loss: 0.1860 - val_acc: 0.9399\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3350 - acc: 0.8467 - val_loss: 0.1995 - val_acc: 0.9312\n",
      "Epoch 00007: early stopping\n",
      "7160\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3381 - acc: 0.8446 - val_loss: 0.1841 - val_acc: 0.9392\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8456 - val_loss: 0.1615 - val_acc: 0.9486\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3358 - acc: 0.8448 - val_loss: 0.2083 - val_acc: 0.9281\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3376 - acc: 0.8440 - val_loss: 0.1747 - val_acc: 0.9432\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3319 - acc: 0.8467 - val_loss: 0.1936 - val_acc: 0.9335\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3363 - acc: 0.8449 - val_loss: 0.1989 - val_acc: 0.9313\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3364 - acc: 0.8446 - val_loss: 0.1999 - val_acc: 0.9309\n",
      "Epoch 00007: early stopping\n",
      "7170\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8458 - val_loss: 0.2011 - val_acc: 0.9311\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3319 - acc: 0.8456 - val_loss: 0.2055 - val_acc: 0.9300\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8443 - val_loss: 0.1893 - val_acc: 0.9375\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3323 - acc: 0.8463 - val_loss: 0.1928 - val_acc: 0.9330\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8484 - val_loss: 0.2303 - val_acc: 0.9179\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3323 - acc: 0.8476 - val_loss: 0.2246 - val_acc: 0.9165\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3343 - acc: 0.8448 - val_loss: 0.1898 - val_acc: 0.9349\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3365 - acc: 0.8450 - val_loss: 0.1720 - val_acc: 0.9438\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3359 - acc: 0.8473 - val_loss: 0.2365 - val_acc: 0.9144\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3368 - acc: 0.8463 - val_loss: 0.2030 - val_acc: 0.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8449 - val_loss: 0.1804 - val_acc: 0.9398\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8462 - val_loss: 0.1689 - val_acc: 0.9449\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3370 - acc: 0.8444 - val_loss: 0.1882 - val_acc: 0.9371\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8465 - val_loss: 0.1940 - val_acc: 0.9340\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3357 - acc: 0.8444 - val_loss: 0.2489 - val_acc: 0.9094\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3364 - acc: 0.8456 - val_loss: 0.1960 - val_acc: 0.9350\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8469 - val_loss: 0.1783 - val_acc: 0.9431\n",
      "Epoch 00017: early stopping\n",
      "7180\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8459 - val_loss: 0.2173 - val_acc: 0.9247\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3355 - acc: 0.8462 - val_loss: 0.2323 - val_acc: 0.9204\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8474 - val_loss: 0.1959 - val_acc: 0.9367\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8463 - val_loss: 0.1831 - val_acc: 0.9379\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3326 - acc: 0.8492 - val_loss: 0.2178 - val_acc: 0.9214\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8471 - val_loss: 0.2028 - val_acc: 0.9306\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8472 - val_loss: 0.2182 - val_acc: 0.9263\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8483 - val_loss: 0.1824 - val_acc: 0.9415\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3351 - acc: 0.8473 - val_loss: 0.1715 - val_acc: 0.9440\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8490 - val_loss: 0.1854 - val_acc: 0.9395\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8465 - val_loss: 0.1968 - val_acc: 0.9350\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8470 - val_loss: 0.1858 - val_acc: 0.9384\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8465 - val_loss: 0.1947 - val_acc: 0.9329\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8478 - val_loss: 0.1998 - val_acc: 0.9299\n",
      "Epoch 00014: early stopping\n",
      "7190\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8494 - val_loss: 0.1998 - val_acc: 0.9298\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8471 - val_loss: 0.1723 - val_acc: 0.9420\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8473 - val_loss: 0.2247 - val_acc: 0.9216\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3360 - acc: 0.8454 - val_loss: 0.2165 - val_acc: 0.9230\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8461 - val_loss: 0.2082 - val_acc: 0.9264\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3354 - acc: 0.8468 - val_loss: 0.1905 - val_acc: 0.9347\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8459 - val_loss: 0.2177 - val_acc: 0.9235\n",
      "Epoch 00007: early stopping\n",
      "7200\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3379 - acc: 0.8445 - val_loss: 0.1906 - val_acc: 0.9342\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8472 - val_loss: 0.2157 - val_acc: 0.9221\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8457 - val_loss: 0.2036 - val_acc: 0.9295\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8453 - val_loss: 0.1668 - val_acc: 0.9459\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8469 - val_loss: 0.1875 - val_acc: 0.9368\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8461 - val_loss: 0.1913 - val_acc: 0.9347\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8452 - val_loss: 0.1802 - val_acc: 0.9400\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8476 - val_loss: 0.1777 - val_acc: 0.9418\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8470 - val_loss: 0.1843 - val_acc: 0.9392\n",
      "Epoch 00009: early stopping\n",
      "7210\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8460 - val_loss: 0.1767 - val_acc: 0.9425\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8480 - val_loss: 0.2173 - val_acc: 0.9239\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8467 - val_loss: 0.1742 - val_acc: 0.9457\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8460 - val_loss: 0.1912 - val_acc: 0.9348\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8467 - val_loss: 0.1876 - val_acc: 0.9352\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8471 - val_loss: 0.2061 - val_acc: 0.9276\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8465 - val_loss: 0.1704 - val_acc: 0.9443\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8449 - val_loss: 0.2002 - val_acc: 0.9329\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8470 - val_loss: 0.2170 - val_acc: 0.9236\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8471 - val_loss: 0.1643 - val_acc: 0.9476\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8442 - val_loss: 0.2160 - val_acc: 0.9229\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8455 - val_loss: 0.2204 - val_acc: 0.9211\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8467 - val_loss: 0.2051 - val_acc: 0.9294\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3345 - acc: 0.8457 - val_loss: 0.1919 - val_acc: 0.9344\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8455 - val_loss: 0.1895 - val_acc: 0.9349\n",
      "Epoch 00015: early stopping\n",
      "7220\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8457 - val_loss: 0.1648 - val_acc: 0.9472\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3340 - acc: 0.8456 - val_loss: 0.1887 - val_acc: 0.9375\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8473 - val_loss: 0.1730 - val_acc: 0.9454\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8470 - val_loss: 0.2629 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8470 - val_loss: 0.2295 - val_acc: 0.9150\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3349 - acc: 0.8461 - val_loss: 0.1798 - val_acc: 0.9433\n",
      "Epoch 00006: early stopping\n",
      "7230\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3350 - acc: 0.8472 - val_loss: 0.2167 - val_acc: 0.9234\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8475 - val_loss: 0.2048 - val_acc: 0.9313\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8473 - val_loss: 0.1881 - val_acc: 0.9373\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8474 - val_loss: 0.2290 - val_acc: 0.9188\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3335 - acc: 0.8458 - val_loss: 0.1861 - val_acc: 0.9394\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3345 - acc: 0.8451 - val_loss: 0.1783 - val_acc: 0.9444\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8472 - val_loss: 0.1964 - val_acc: 0.9361\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8462 - val_loss: 0.2023 - val_acc: 0.9304\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8482 - val_loss: 0.2010 - val_acc: 0.9311\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8472 - val_loss: 0.1837 - val_acc: 0.9370\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3341 - acc: 0.8469 - val_loss: 0.1794 - val_acc: 0.9437\n",
      "Epoch 00011: early stopping\n",
      "7240\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8469 - val_loss: 0.1951 - val_acc: 0.9349\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8473 - val_loss: 0.1943 - val_acc: 0.9341\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8464 - val_loss: 0.2236 - val_acc: 0.9227\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3361 - acc: 0.8462 - val_loss: 0.2023 - val_acc: 0.9294\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3351 - acc: 0.8451 - val_loss: 0.1789 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8464 - val_loss: 0.1855 - val_acc: 0.9379\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8484 - val_loss: 0.2092 - val_acc: 0.9257\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3323 - acc: 0.8470 - val_loss: 0.2172 - val_acc: 0.9239\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8479 - val_loss: 0.2702 - val_acc: 0.8997\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8471 - val_loss: 0.1912 - val_acc: 0.9366\n",
      "Epoch 00010: early stopping\n",
      "7250\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3354 - acc: 0.8472 - val_loss: 0.1848 - val_acc: 0.9404\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8483 - val_loss: 0.2114 - val_acc: 0.9269\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8461 - val_loss: 0.1984 - val_acc: 0.9298\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8465 - val_loss: 0.2507 - val_acc: 0.9080\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8463 - val_loss: 0.1674 - val_acc: 0.9441\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8463 - val_loss: 0.1669 - val_acc: 0.9467\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3340 - acc: 0.8467 - val_loss: 0.2010 - val_acc: 0.9325\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8459 - val_loss: 0.1929 - val_acc: 0.9359\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8485 - val_loss: 0.2194 - val_acc: 0.9229\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3367 - acc: 0.8461 - val_loss: 0.2179 - val_acc: 0.9232\n",
      "Epoch 00010: early stopping\n",
      "7260\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8469 - val_loss: 0.1711 - val_acc: 0.9444\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8490 - val_loss: 0.2093 - val_acc: 0.9284\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8476 - val_loss: 0.2002 - val_acc: 0.9337\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8464 - val_loss: 0.1887 - val_acc: 0.9367\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8491 - val_loss: 0.2288 - val_acc: 0.9171\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3341 - acc: 0.8477 - val_loss: 0.2127 - val_acc: 0.9245\n",
      "Epoch 00006: early stopping\n",
      "7270\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8459 - val_loss: 0.1930 - val_acc: 0.9356\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8464 - val_loss: 0.1974 - val_acc: 0.9294\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3297 - acc: 0.8477 - val_loss: 0.2289 - val_acc: 0.9190\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3350 - acc: 0.8463 - val_loss: 0.1920 - val_acc: 0.9386\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8486 - val_loss: 0.1960 - val_acc: 0.9308\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8469 - val_loss: 0.1691 - val_acc: 0.9461\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3362 - acc: 0.8460 - val_loss: 0.1847 - val_acc: 0.9400\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3357 - acc: 0.8464 - val_loss: 0.1842 - val_acc: 0.9390\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8466 - val_loss: 0.1848 - val_acc: 0.9384\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8469 - val_loss: 0.2020 - val_acc: 0.9289\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8457 - val_loss: 0.1850 - val_acc: 0.9365\n",
      "Epoch 00011: early stopping\n",
      "7280\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3326 - acc: 0.8467 - val_loss: 0.2412 - val_acc: 0.9146\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3340 - acc: 0.8474 - val_loss: 0.1598 - val_acc: 0.9492\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3357 - acc: 0.8471 - val_loss: 0.2107 - val_acc: 0.9235\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8470 - val_loss: 0.1934 - val_acc: 0.9359\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8468 - val_loss: 0.2005 - val_acc: 0.9316\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8480 - val_loss: 0.2144 - val_acc: 0.9256\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8475 - val_loss: 0.2043 - val_acc: 0.9294\n",
      "Epoch 00007: early stopping\n",
      "7290\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8476 - val_loss: 0.1940 - val_acc: 0.9350\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3331 - acc: 0.8451 - val_loss: 0.1872 - val_acc: 0.9371\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8480 - val_loss: 0.1720 - val_acc: 0.9439\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8481 - val_loss: 0.1749 - val_acc: 0.9432\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8475 - val_loss: 0.1931 - val_acc: 0.9341\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8469 - val_loss: 0.1825 - val_acc: 0.9403\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8471 - val_loss: 0.1697 - val_acc: 0.9454\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8474 - val_loss: 0.1913 - val_acc: 0.9364\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3318 - acc: 0.8490 - val_loss: 0.2185 - val_acc: 0.9248\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3348 - acc: 0.8461 - val_loss: 0.1998 - val_acc: 0.9352\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3336 - acc: 0.8478 - val_loss: 0.1789 - val_acc: 0.9425\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8486 - val_loss: 0.2213 - val_acc: 0.9244\n",
      "Epoch 00012: early stopping\n",
      "7300\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8479 - val_loss: 0.1715 - val_acc: 0.9453\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8469 - val_loss: 0.1874 - val_acc: 0.9395\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8476 - val_loss: 0.1958 - val_acc: 0.9358\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3352 - acc: 0.8474 - val_loss: 0.1853 - val_acc: 0.9386\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8475 - val_loss: 0.2068 - val_acc: 0.9280\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8474 - val_loss: 0.1983 - val_acc: 0.9337\n",
      "Epoch 00006: early stopping\n",
      "7310\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3336 - acc: 0.8471 - val_loss: 0.1776 - val_acc: 0.9426\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8476 - val_loss: 0.1788 - val_acc: 0.9422\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8478 - val_loss: 0.1827 - val_acc: 0.9403\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8477 - val_loss: 0.1792 - val_acc: 0.9448\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8492 - val_loss: 0.2143 - val_acc: 0.9245\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3355 - acc: 0.8473 - val_loss: 0.1719 - val_acc: 0.9434\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8475 - val_loss: 0.1674 - val_acc: 0.9471\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8462 - val_loss: 0.1767 - val_acc: 0.9409\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8476 - val_loss: 0.2151 - val_acc: 0.9260\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3323 - acc: 0.8488 - val_loss: 0.1738 - val_acc: 0.9432\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8478 - val_loss: 0.1761 - val_acc: 0.9448\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8471 - val_loss: 0.1904 - val_acc: 0.9371\n",
      "Epoch 00012: early stopping\n",
      "7320\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8460 - val_loss: 0.1885 - val_acc: 0.9365\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8484 - val_loss: 0.1804 - val_acc: 0.9403\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8488 - val_loss: 0.2132 - val_acc: 0.9237\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8491 - val_loss: 0.2044 - val_acc: 0.9281\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8481 - val_loss: 0.1773 - val_acc: 0.9404\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8489 - val_loss: 0.1968 - val_acc: 0.9302\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8483 - val_loss: 0.1824 - val_acc: 0.9425\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8477 - val_loss: 0.1753 - val_acc: 0.9438\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8465 - val_loss: 0.1781 - val_acc: 0.9422\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8470 - val_loss: 0.2008 - val_acc: 0.9293\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8480 - val_loss: 0.1790 - val_acc: 0.9393\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8461 - val_loss: 0.1802 - val_acc: 0.9421\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8472 - val_loss: 0.1762 - val_acc: 0.9460\n",
      "Epoch 00013: early stopping\n",
      "7330\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8470 - val_loss: 0.1764 - val_acc: 0.9455\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8469 - val_loss: 0.2064 - val_acc: 0.9258\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8489 - val_loss: 0.1938 - val_acc: 0.9344\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8481 - val_loss: 0.1737 - val_acc: 0.9437\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8462 - val_loss: 0.1867 - val_acc: 0.9406\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3341 - acc: 0.8459 - val_loss: 0.1952 - val_acc: 0.9355\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8467 - val_loss: 0.2099 - val_acc: 0.9306\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8476 - val_loss: 0.1812 - val_acc: 0.9412\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8493 - val_loss: 0.1854 - val_acc: 0.9389\n",
      "Epoch 00009: early stopping\n",
      "7340\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8488 - val_loss: 0.1998 - val_acc: 0.9309\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3335 - acc: 0.8475 - val_loss: 0.1748 - val_acc: 0.9432\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8478 - val_loss: 0.1947 - val_acc: 0.9347\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8485 - val_loss: 0.1778 - val_acc: 0.9408\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8504 - val_loss: 0.2142 - val_acc: 0.9245\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8462 - val_loss: 0.1877 - val_acc: 0.9356\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8492 - val_loss: 0.1865 - val_acc: 0.9368\n",
      "Epoch 00007: early stopping\n",
      "7350\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8469 - val_loss: 0.1916 - val_acc: 0.9362\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3335 - acc: 0.8484 - val_loss: 0.1800 - val_acc: 0.9391\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8474 - val_loss: 0.1979 - val_acc: 0.9328\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8483 - val_loss: 0.1800 - val_acc: 0.9395\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8478 - val_loss: 0.2081 - val_acc: 0.9262\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8478 - val_loss: 0.1754 - val_acc: 0.9414\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3347 - acc: 0.8471 - val_loss: 0.1729 - val_acc: 0.9446\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8478 - val_loss: 0.1955 - val_acc: 0.9331\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8477 - val_loss: 0.1861 - val_acc: 0.9360\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8473 - val_loss: 0.2015 - val_acc: 0.9332\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8454 - val_loss: 0.1842 - val_acc: 0.9396\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8473 - val_loss: 0.1880 - val_acc: 0.9352\n",
      "Epoch 00012: early stopping\n",
      "7360\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8462 - val_loss: 0.2044 - val_acc: 0.9295\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8473 - val_loss: 0.2158 - val_acc: 0.9270\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8473 - val_loss: 0.2101 - val_acc: 0.9279\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8480 - val_loss: 0.1829 - val_acc: 0.9416\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3350 - acc: 0.8472 - val_loss: 0.1856 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8473 - val_loss: 0.1831 - val_acc: 0.9390\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8469 - val_loss: 0.2066 - val_acc: 0.9274\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8474 - val_loss: 0.1901 - val_acc: 0.9333\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3326 - acc: 0.8469 - val_loss: 0.1674 - val_acc: 0.9481\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8461 - val_loss: 0.1900 - val_acc: 0.9384\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8484 - val_loss: 0.1727 - val_acc: 0.9433\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8470 - val_loss: 0.2267 - val_acc: 0.9197\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8497 - val_loss: 0.1829 - val_acc: 0.9384\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8465 - val_loss: 0.1817 - val_acc: 0.9387\n",
      "Epoch 00014: early stopping\n",
      "7370\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8475 - val_loss: 0.1797 - val_acc: 0.9408\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8479 - val_loss: 0.1755 - val_acc: 0.9464\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8467 - val_loss: 0.2239 - val_acc: 0.9214\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8481 - val_loss: 0.1795 - val_acc: 0.9407\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8481 - val_loss: 0.1903 - val_acc: 0.9378\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3336 - acc: 0.8460 - val_loss: 0.1877 - val_acc: 0.9373\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8485 - val_loss: 0.1923 - val_acc: 0.9294\n",
      "Epoch 00007: early stopping\n",
      "7380\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8477 - val_loss: 0.1905 - val_acc: 0.9362\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8487 - val_loss: 0.2187 - val_acc: 0.9219\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3297 - acc: 0.8483 - val_loss: 0.1816 - val_acc: 0.9402\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8484 - val_loss: 0.1809 - val_acc: 0.9404\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8483 - val_loss: 0.1624 - val_acc: 0.9485\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8460 - val_loss: 0.1616 - val_acc: 0.9485\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8483 - val_loss: 0.1779 - val_acc: 0.9401\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8499 - val_loss: 0.1863 - val_acc: 0.9393\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8467 - val_loss: 0.1627 - val_acc: 0.9483\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8484 - val_loss: 0.1770 - val_acc: 0.9435\n",
      "Epoch 00010: early stopping\n",
      "7390\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8484 - val_loss: 0.2072 - val_acc: 0.9281\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8473 - val_loss: 0.1643 - val_acc: 0.9483\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8456 - val_loss: 0.2089 - val_acc: 0.9248\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8455 - val_loss: 0.1983 - val_acc: 0.9338\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8486 - val_loss: 0.1784 - val_acc: 0.9407\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8475 - val_loss: 0.1866 - val_acc: 0.9402\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8486 - val_loss: 0.1934 - val_acc: 0.9347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n",
      "7400\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8479 - val_loss: 0.1936 - val_acc: 0.9352\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8480 - val_loss: 0.1579 - val_acc: 0.9508\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8478 - val_loss: 0.1925 - val_acc: 0.9343\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8465 - val_loss: 0.1893 - val_acc: 0.9383\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3333 - acc: 0.8482 - val_loss: 0.1817 - val_acc: 0.9407\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8464 - val_loss: 0.1725 - val_acc: 0.9452\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8457 - val_loss: 0.1955 - val_acc: 0.9356\n",
      "Epoch 00007: early stopping\n",
      "7410\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8456 - val_loss: 0.1705 - val_acc: 0.9455\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3326 - acc: 0.8481 - val_loss: 0.1840 - val_acc: 0.9414\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8479 - val_loss: 0.1687 - val_acc: 0.9453\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3337 - acc: 0.8440 - val_loss: 0.1650 - val_acc: 0.9475\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8474 - val_loss: 0.1887 - val_acc: 0.9356\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8481 - val_loss: 0.1796 - val_acc: 0.9406\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8485 - val_loss: 0.1792 - val_acc: 0.9375\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8478 - val_loss: 0.1883 - val_acc: 0.9350\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3350 - acc: 0.8459 - val_loss: 0.1738 - val_acc: 0.9428\n",
      "Epoch 00009: early stopping\n",
      "7420\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8479 - val_loss: 0.1675 - val_acc: 0.9478\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8487 - val_loss: 0.1616 - val_acc: 0.9495\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8482 - val_loss: 0.1698 - val_acc: 0.9481\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8478 - val_loss: 0.1695 - val_acc: 0.9476\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8481 - val_loss: 0.1710 - val_acc: 0.9442\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8471 - val_loss: 0.2025 - val_acc: 0.9314\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3331 - acc: 0.8466 - val_loss: 0.1836 - val_acc: 0.9391\n",
      "Epoch 00007: early stopping\n",
      "7430\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8489 - val_loss: 0.1870 - val_acc: 0.9375\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8465 - val_loss: 0.1773 - val_acc: 0.9409\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8483 - val_loss: 0.1764 - val_acc: 0.9405\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8487 - val_loss: 0.1744 - val_acc: 0.9425\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8491 - val_loss: 0.1873 - val_acc: 0.9395\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8493 - val_loss: 0.1892 - val_acc: 0.9367\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8485 - val_loss: 0.1589 - val_acc: 0.9492\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8473 - val_loss: 0.1686 - val_acc: 0.9451\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8484 - val_loss: 0.1739 - val_acc: 0.9415\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3290 - acc: 0.8493 - val_loss: 0.2110 - val_acc: 0.9225\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8469 - val_loss: 0.1597 - val_acc: 0.9504\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8462 - val_loss: 0.1777 - val_acc: 0.9410\n",
      "Epoch 00012: early stopping\n",
      "7440\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8485 - val_loss: 0.1782 - val_acc: 0.9400\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8487 - val_loss: 0.1681 - val_acc: 0.9452\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8482 - val_loss: 0.2099 - val_acc: 0.9273\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3331 - acc: 0.8454 - val_loss: 0.1812 - val_acc: 0.9406\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8457 - val_loss: 0.1741 - val_acc: 0.9362\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8485 - val_loss: 0.1648 - val_acc: 0.9444\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8465 - val_loss: 0.1779 - val_acc: 0.9404\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8502 - val_loss: 0.1827 - val_acc: 0.9387\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8476 - val_loss: 0.1753 - val_acc: 0.9452\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8463 - val_loss: 0.1720 - val_acc: 0.9430\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8471 - val_loss: 0.1724 - val_acc: 0.9431\n",
      "Epoch 00011: early stopping\n",
      "7450\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8479 - val_loss: 0.1789 - val_acc: 0.9422\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8467 - val_loss: 0.1680 - val_acc: 0.9461\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8461 - val_loss: 0.1798 - val_acc: 0.9397\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3338 - acc: 0.8449 - val_loss: 0.1751 - val_acc: 0.9426\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8465 - val_loss: 0.1658 - val_acc: 0.9457\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8466 - val_loss: 0.1738 - val_acc: 0.9443\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8482 - val_loss: 0.1744 - val_acc: 0.9431\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8475 - val_loss: 0.1942 - val_acc: 0.9353\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8480 - val_loss: 0.1679 - val_acc: 0.9447\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8483 - val_loss: 0.2085 - val_acc: 0.9268\n",
      "Epoch 00010: early stopping\n",
      "7460\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8486 - val_loss: 0.1872 - val_acc: 0.9361\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8485 - val_loss: 0.1829 - val_acc: 0.9378\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3339 - acc: 0.8461 - val_loss: 0.1969 - val_acc: 0.9353\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8497 - val_loss: 0.1731 - val_acc: 0.9436\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8472 - val_loss: 0.1650 - val_acc: 0.9457\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8486 - val_loss: 0.1851 - val_acc: 0.9371\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8489 - val_loss: 0.1744 - val_acc: 0.9411\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8490 - val_loss: 0.1617 - val_acc: 0.9507\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8501 - val_loss: 0.1956 - val_acc: 0.9329\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8485 - val_loss: 0.1785 - val_acc: 0.9422\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8484 - val_loss: 0.1819 - val_acc: 0.9406\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8463 - val_loss: 0.1732 - val_acc: 0.9465\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8471 - val_loss: 0.1762 - val_acc: 0.9438\n",
      "Epoch 00013: early stopping\n",
      "7470\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8475 - val_loss: 0.2018 - val_acc: 0.9304\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8467 - val_loss: 0.1849 - val_acc: 0.9412\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3297 - acc: 0.8484 - val_loss: 0.1726 - val_acc: 0.9445\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8490 - val_loss: 0.1778 - val_acc: 0.9412\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8489 - val_loss: 0.1770 - val_acc: 0.9424\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3329 - acc: 0.8458 - val_loss: 0.2134 - val_acc: 0.9251\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8488 - val_loss: 0.1722 - val_acc: 0.9433\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3290 - acc: 0.8487 - val_loss: 0.1673 - val_acc: 0.9473\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8490 - val_loss: 0.1967 - val_acc: 0.9337\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3328 - acc: 0.8477 - val_loss: 0.1964 - val_acc: 0.9382\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8501 - val_loss: 0.1911 - val_acc: 0.9394\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8476 - val_loss: 0.1805 - val_acc: 0.9416\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3297 - acc: 0.8471 - val_loss: 0.1882 - val_acc: 0.9410\n",
      "Epoch 00013: early stopping\n",
      "7480\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8463 - val_loss: 0.1521 - val_acc: 0.9550\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8486 - val_loss: 0.1699 - val_acc: 0.9457\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8497 - val_loss: 0.1837 - val_acc: 0.9400\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3313 - acc: 0.8475 - val_loss: 0.1776 - val_acc: 0.9427\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3301 - acc: 0.8492 - val_loss: 0.1559 - val_acc: 0.9536\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8493 - val_loss: 0.1747 - val_acc: 0.9447\n",
      "Epoch 00006: early stopping\n",
      "7490\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8503 - val_loss: 0.1837 - val_acc: 0.9393\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8478 - val_loss: 0.1656 - val_acc: 0.9486\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8473 - val_loss: 0.1823 - val_acc: 0.9417\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3315 - acc: 0.8486 - val_loss: 0.1708 - val_acc: 0.9451\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8489 - val_loss: 0.1879 - val_acc: 0.9345\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8474 - val_loss: 0.1650 - val_acc: 0.9485\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8477 - val_loss: 0.1721 - val_acc: 0.9459\n",
      "Epoch 00007: early stopping\n",
      "7500\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8466 - val_loss: 0.1665 - val_acc: 0.9449\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3326 - acc: 0.8464 - val_loss: 0.1859 - val_acc: 0.9364\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8490 - val_loss: 0.1726 - val_acc: 0.9458\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8478 - val_loss: 0.1613 - val_acc: 0.9500\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8470 - val_loss: 0.1996 - val_acc: 0.9318\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8484 - val_loss: 0.1796 - val_acc: 0.9426\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8497 - val_loss: 0.1780 - val_acc: 0.9425\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8487 - val_loss: 0.1690 - val_acc: 0.9474\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8477 - val_loss: 0.1795 - val_acc: 0.9393\n",
      "Epoch 00009: early stopping\n",
      "7510\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3344 - acc: 0.8453 - val_loss: 0.1957 - val_acc: 0.9324\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8474 - val_loss: 0.2289 - val_acc: 0.9198\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8485 - val_loss: 0.1639 - val_acc: 0.9503\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3325 - acc: 0.8459 - val_loss: 0.1684 - val_acc: 0.9464\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8478 - val_loss: 0.1801 - val_acc: 0.9408\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8484 - val_loss: 0.1608 - val_acc: 0.9515\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8461 - val_loss: 0.1965 - val_acc: 0.9312\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8487 - val_loss: 0.1779 - val_acc: 0.9420\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8492 - val_loss: 0.1797 - val_acc: 0.9424\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8483 - val_loss: 0.1471 - val_acc: 0.9542\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3335 - acc: 0.8464 - val_loss: 0.1782 - val_acc: 0.9372\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8486 - val_loss: 0.1852 - val_acc: 0.9376\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8493 - val_loss: 0.1634 - val_acc: 0.9467\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8483 - val_loss: 0.1746 - val_acc: 0.9424\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3279 - acc: 0.8493 - val_loss: 0.2017 - val_acc: 0.9274\n",
      "Epoch 00015: early stopping\n",
      "7520\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8461 - val_loss: 0.2191 - val_acc: 0.9201\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8481 - val_loss: 0.1890 - val_acc: 0.9392\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8493 - val_loss: 0.2174 - val_acc: 0.9239\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8480 - val_loss: 0.1835 - val_acc: 0.9394\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8486 - val_loss: 0.1841 - val_acc: 0.9392\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8490 - val_loss: 0.1875 - val_acc: 0.9379\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8499 - val_loss: 0.1633 - val_acc: 0.9472\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8472 - val_loss: 0.1750 - val_acc: 0.9438\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3288 - acc: 0.8483 - val_loss: 0.1892 - val_acc: 0.9367\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3297 - acc: 0.8482 - val_loss: 0.1621 - val_acc: 0.9509\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8485 - val_loss: 0.1818 - val_acc: 0.9423\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8488 - val_loss: 0.1693 - val_acc: 0.9476\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3298 - acc: 0.8484 - val_loss: 0.1684 - val_acc: 0.9478\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3330 - acc: 0.8483 - val_loss: 0.2019 - val_acc: 0.9332\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3298 - acc: 0.8489 - val_loss: 0.1907 - val_acc: 0.9383\n",
      "Epoch 00015: early stopping\n",
      "7530\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3324 - acc: 0.8483 - val_loss: 0.1860 - val_acc: 0.9402\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8502 - val_loss: 0.2007 - val_acc: 0.9363\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8494 - val_loss: 0.1861 - val_acc: 0.9382\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8475 - val_loss: 0.1914 - val_acc: 0.9362\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8499 - val_loss: 0.1578 - val_acc: 0.9520\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3341 - acc: 0.8471 - val_loss: 0.1588 - val_acc: 0.9522\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8481 - val_loss: 0.1656 - val_acc: 0.9497\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3332 - acc: 0.8475 - val_loss: 0.1919 - val_acc: 0.9348\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8477 - val_loss: 0.1648 - val_acc: 0.9492\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8481 - val_loss: 0.2042 - val_acc: 0.9314\n",
      "Epoch 00010: early stopping\n",
      "7540\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8505 - val_loss: 0.1573 - val_acc: 0.9513\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8482 - val_loss: 0.1833 - val_acc: 0.9372\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8485 - val_loss: 0.1854 - val_acc: 0.9385\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 1s 4us/step - loss: 0.3276 - acc: 0.8485 - val_loss: 0.1510 - val_acc: 0.9533\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3318 - acc: 0.8474 - val_loss: 0.1555 - val_acc: 0.9538\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8480 - val_loss: 0.1720 - val_acc: 0.9408\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8466 - val_loss: 0.1855 - val_acc: 0.9393\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8477 - val_loss: 0.1929 - val_acc: 0.9366\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8492 - val_loss: 0.1548 - val_acc: 0.9520\n",
      "Epoch 00009: early stopping\n",
      "7550\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8474 - val_loss: 0.2042 - val_acc: 0.9331\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8473 - val_loss: 0.2057 - val_acc: 0.9311\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3340 - acc: 0.8474 - val_loss: 0.1848 - val_acc: 0.9389\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8489 - val_loss: 0.1834 - val_acc: 0.9401\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8488 - val_loss: 0.1726 - val_acc: 0.9456\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8471 - val_loss: 0.1933 - val_acc: 0.9367\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3319 - acc: 0.8475 - val_loss: 0.1725 - val_acc: 0.9463\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8463 - val_loss: 0.2216 - val_acc: 0.9162\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8456 - val_loss: 0.2104 - val_acc: 0.9296\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8504 - val_loss: 0.1914 - val_acc: 0.9366\n",
      "Epoch 00010: early stopping\n",
      "7560\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8492 - val_loss: 0.2167 - val_acc: 0.9252\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3290 - acc: 0.8481 - val_loss: 0.1929 - val_acc: 0.9362\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8477 - val_loss: 0.1807 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8472 - val_loss: 0.2298 - val_acc: 0.9165\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8493 - val_loss: 0.2069 - val_acc: 0.9295\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8479 - val_loss: 0.2074 - val_acc: 0.9290\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8484 - val_loss: 0.2251 - val_acc: 0.9181\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8477 - val_loss: 0.2209 - val_acc: 0.9204\n",
      "Epoch 00008: early stopping\n",
      "7570\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8468 - val_loss: 0.1845 - val_acc: 0.9418\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8473 - val_loss: 0.1935 - val_acc: 0.9470\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8480 - val_loss: 0.2226 - val_acc: 0.9213\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8508 - val_loss: 0.1921 - val_acc: 0.9351\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8472 - val_loss: 0.1696 - val_acc: 0.9460\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8483 - val_loss: 0.1966 - val_acc: 0.9343\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3312 - acc: 0.8490 - val_loss: 0.1940 - val_acc: 0.9361\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3334 - acc: 0.8471 - val_loss: 0.1799 - val_acc: 0.9406\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8487 - val_loss: 0.2032 - val_acc: 0.9317\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8476 - val_loss: 0.1888 - val_acc: 0.9366\n",
      "Epoch 00010: early stopping\n",
      "7580\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3318 - acc: 0.8477 - val_loss: 0.1777 - val_acc: 0.9439\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8489 - val_loss: 0.2092 - val_acc: 0.9284\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8488 - val_loss: 0.1798 - val_acc: 0.9415\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8471 - val_loss: 0.1843 - val_acc: 0.9410\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8477 - val_loss: 0.1821 - val_acc: 0.9403\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8479 - val_loss: 0.1805 - val_acc: 0.9395\n",
      "Epoch 00006: early stopping\n",
      "7590\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8470 - val_loss: 0.1948 - val_acc: 0.9350\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8471 - val_loss: 0.1631 - val_acc: 0.9503\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8471 - val_loss: 0.2058 - val_acc: 0.9277\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8483 - val_loss: 0.1673 - val_acc: 0.9467\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8476 - val_loss: 0.2048 - val_acc: 0.9300\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8466 - val_loss: 0.2161 - val_acc: 0.9242\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8486 - val_loss: 0.1816 - val_acc: 0.9403\n",
      "Epoch 00007: early stopping\n",
      "7600\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8492 - val_loss: 0.1705 - val_acc: 0.9468\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8483 - val_loss: 0.1851 - val_acc: 0.9398\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3311 - acc: 0.8472 - val_loss: 0.1886 - val_acc: 0.9385\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8494 - val_loss: 0.1750 - val_acc: 0.9446\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8499 - val_loss: 0.1912 - val_acc: 0.9365\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8477 - val_loss: 0.1984 - val_acc: 0.9350\n",
      "Epoch 00006: early stopping\n",
      "7610\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3304 - acc: 0.8490 - val_loss: 0.1973 - val_acc: 0.9331\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8481 - val_loss: 0.1902 - val_acc: 0.9366\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8472 - val_loss: 0.2147 - val_acc: 0.9248\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8493 - val_loss: 0.1817 - val_acc: 0.9415\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3320 - acc: 0.8471 - val_loss: 0.1959 - val_acc: 0.9327\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8471 - val_loss: 0.1845 - val_acc: 0.9389\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8470 - val_loss: 0.1746 - val_acc: 0.9446\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8478 - val_loss: 0.1858 - val_acc: 0.9379\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8483 - val_loss: 0.1839 - val_acc: 0.9408\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8474 - val_loss: 0.1778 - val_acc: 0.9434\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8480 - val_loss: 0.1794 - val_acc: 0.9446\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8500 - val_loss: 0.2152 - val_acc: 0.9260\n",
      "Epoch 00012: early stopping\n",
      "7620\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8482 - val_loss: 0.2082 - val_acc: 0.9291\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3316 - acc: 0.8465 - val_loss: 0.2188 - val_acc: 0.9241\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8497 - val_loss: 0.2066 - val_acc: 0.9298\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8508 - val_loss: 0.1776 - val_acc: 0.9447\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8485 - val_loss: 0.1735 - val_acc: 0.9455\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3288 - acc: 0.8488 - val_loss: 0.1739 - val_acc: 0.9456\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8481 - val_loss: 0.2045 - val_acc: 0.9246\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3314 - acc: 0.8466 - val_loss: 0.2053 - val_acc: 0.9305\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8480 - val_loss: 0.1986 - val_acc: 0.9372\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8483 - val_loss: 0.1950 - val_acc: 0.9362\n",
      "Epoch 00010: early stopping\n",
      "7630\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8485 - val_loss: 0.1904 - val_acc: 0.9391\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8510 - val_loss: 0.2082 - val_acc: 0.9301\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8508 - val_loss: 0.1686 - val_acc: 0.9469\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8482 - val_loss: 0.1778 - val_acc: 0.9426\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8486 - val_loss: 0.2046 - val_acc: 0.9304\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8490 - val_loss: 0.1719 - val_acc: 0.9448\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8484 - val_loss: 0.2144 - val_acc: 0.9242\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8496 - val_loss: 0.1862 - val_acc: 0.9378\n",
      "Epoch 00008: early stopping\n",
      "7640\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8472 - val_loss: 0.1704 - val_acc: 0.9461\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8482 - val_loss: 0.1991 - val_acc: 0.9296\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8501 - val_loss: 0.1830 - val_acc: 0.9417\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8477 - val_loss: 0.2283 - val_acc: 0.9205\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8485 - val_loss: 0.1963 - val_acc: 0.9324\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8490 - val_loss: 0.1851 - val_acc: 0.9414\n",
      "Epoch 00006: early stopping\n",
      "7650\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8494 - val_loss: 0.1877 - val_acc: 0.9408\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8493 - val_loss: 0.1742 - val_acc: 0.9458\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8509 - val_loss: 0.1810 - val_acc: 0.9424\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8490 - val_loss: 0.1839 - val_acc: 0.9408\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8480 - val_loss: 0.1815 - val_acc: 0.9407\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8475 - val_loss: 0.1763 - val_acc: 0.9412\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8497 - val_loss: 0.1841 - val_acc: 0.9383\n",
      "Epoch 00007: early stopping\n",
      "7660\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8486 - val_loss: 0.1946 - val_acc: 0.9348\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8482 - val_loss: 0.1876 - val_acc: 0.9375\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8477 - val_loss: 0.1663 - val_acc: 0.9464\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8468 - val_loss: 0.1799 - val_acc: 0.9389\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8500 - val_loss: 0.1742 - val_acc: 0.9419\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8487 - val_loss: 0.2039 - val_acc: 0.9278\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8503 - val_loss: 0.1858 - val_acc: 0.9380\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8492 - val_loss: 0.1927 - val_acc: 0.9335\n",
      "Epoch 00008: early stopping\n",
      "7670\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8484 - val_loss: 0.1819 - val_acc: 0.9391\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8494 - val_loss: 0.2148 - val_acc: 0.9232\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8476 - val_loss: 0.1905 - val_acc: 0.9337\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3308 - acc: 0.8475 - val_loss: 0.1714 - val_acc: 0.9442\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8479 - val_loss: 0.1816 - val_acc: 0.9407\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8489 - val_loss: 0.1825 - val_acc: 0.9368\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8482 - val_loss: 0.1902 - val_acc: 0.9360\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8513 - val_loss: 0.2067 - val_acc: 0.9267\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8479 - val_loss: 0.1770 - val_acc: 0.9402\n",
      "Epoch 00009: early stopping\n",
      "7680\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8489 - val_loss: 0.1666 - val_acc: 0.9487\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8471 - val_loss: 0.1772 - val_acc: 0.9401\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8510 - val_loss: 0.1929 - val_acc: 0.9331\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8502 - val_loss: 0.1758 - val_acc: 0.9414\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8491 - val_loss: 0.1938 - val_acc: 0.9321\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8502 - val_loss: 0.1685 - val_acc: 0.9457\n",
      "Epoch 00006: early stopping\n",
      "7690\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8471 - val_loss: 0.2355 - val_acc: 0.9141\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8490 - val_loss: 0.1798 - val_acc: 0.9400\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8476 - val_loss: 0.1725 - val_acc: 0.9424\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8487 - val_loss: 0.1770 - val_acc: 0.9404\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8476 - val_loss: 0.1852 - val_acc: 0.9376\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3293 - acc: 0.8482 - val_loss: 0.1939 - val_acc: 0.9335\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8488 - val_loss: 0.1898 - val_acc: 0.9386\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8483 - val_loss: 0.1792 - val_acc: 0.9388\n",
      "Epoch 00008: early stopping\n",
      "7700\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8493 - val_loss: 0.2295 - val_acc: 0.9150\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8483 - val_loss: 0.2116 - val_acc: 0.9231\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8476 - val_loss: 0.1743 - val_acc: 0.9421\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8499 - val_loss: 0.2022 - val_acc: 0.9290\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8500 - val_loss: 0.1827 - val_acc: 0.9366\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8486 - val_loss: 0.1694 - val_acc: 0.9446\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8477 - val_loss: 0.1720 - val_acc: 0.9433\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8497 - val_loss: 0.1801 - val_acc: 0.9401\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8506 - val_loss: 0.1734 - val_acc: 0.9437\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8490 - val_loss: 0.2204 - val_acc: 0.9210\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8509 - val_loss: 0.1954 - val_acc: 0.9353\n",
      "Epoch 00011: early stopping\n",
      "7710\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8495 - val_loss: 0.1855 - val_acc: 0.9349\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8482 - val_loss: 0.1819 - val_acc: 0.9396\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8492 - val_loss: 0.1642 - val_acc: 0.9502\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8492 - val_loss: 0.1881 - val_acc: 0.9355\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8485 - val_loss: 0.1725 - val_acc: 0.9435\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8481 - val_loss: 0.1739 - val_acc: 0.9424\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8505 - val_loss: 0.1687 - val_acc: 0.9457\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8489 - val_loss: 0.1955 - val_acc: 0.9302\n",
      "Epoch 00008: early stopping\n",
      "7720\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8495 - val_loss: 0.2009 - val_acc: 0.9302\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8475 - val_loss: 0.2001 - val_acc: 0.9346\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3342 - acc: 0.8481 - val_loss: 0.1749 - val_acc: 0.9426\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8494 - val_loss: 0.1978 - val_acc: 0.9316\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8490 - val_loss: 0.1892 - val_acc: 0.9365\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8491 - val_loss: 0.1822 - val_acc: 0.9369\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8497 - val_loss: 0.1752 - val_acc: 0.9412\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8484 - val_loss: 0.1612 - val_acc: 0.9485\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3291 - acc: 0.8488 - val_loss: 0.1966 - val_acc: 0.9324\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8490 - val_loss: 0.1816 - val_acc: 0.9404\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8500 - val_loss: 0.1641 - val_acc: 0.9475\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8487 - val_loss: 0.1735 - val_acc: 0.9427\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8485 - val_loss: 0.1807 - val_acc: 0.9418\n",
      "Epoch 00013: early stopping\n",
      "7730\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8472 - val_loss: 0.1775 - val_acc: 0.9377\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8485 - val_loss: 0.1780 - val_acc: 0.9374\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8499 - val_loss: 0.1961 - val_acc: 0.9308\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3286 - acc: 0.8481 - val_loss: 0.1981 - val_acc: 0.9307\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8483 - val_loss: 0.1949 - val_acc: 0.9321\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8483 - val_loss: 0.1775 - val_acc: 0.9381\n",
      "Epoch 00006: early stopping\n",
      "7740\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8497 - val_loss: 0.2184 - val_acc: 0.9211\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8477 - val_loss: 0.2148 - val_acc: 0.9237\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3279 - acc: 0.8490 - val_loss: 0.1990 - val_acc: 0.9301\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8490 - val_loss: 0.1888 - val_acc: 0.9352\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8470 - val_loss: 0.1976 - val_acc: 0.9307\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8489 - val_loss: 0.1730 - val_acc: 0.9425\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8470 - val_loss: 0.1714 - val_acc: 0.9446\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8496 - val_loss: 0.1771 - val_acc: 0.9446\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8514 - val_loss: 0.1632 - val_acc: 0.9489\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8486 - val_loss: 0.1860 - val_acc: 0.9361\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8493 - val_loss: 0.2064 - val_acc: 0.9269\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8491 - val_loss: 0.2158 - val_acc: 0.9244\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8496 - val_loss: 0.2104 - val_acc: 0.9262\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8479 - val_loss: 0.1885 - val_acc: 0.9336\n",
      "Epoch 00014: early stopping\n",
      "7750\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8466 - val_loss: 0.1939 - val_acc: 0.9324\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8484 - val_loss: 0.1914 - val_acc: 0.9332\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8490 - val_loss: 0.1649 - val_acc: 0.9485\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8487 - val_loss: 0.1956 - val_acc: 0.9296\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8498 - val_loss: 0.1879 - val_acc: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8476 - val_loss: 0.1984 - val_acc: 0.9288\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3279 - acc: 0.8486 - val_loss: 0.2264 - val_acc: 0.9152\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8482 - val_loss: 0.1786 - val_acc: 0.9403\n",
      "Epoch 00008: early stopping\n",
      "7760\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8491 - val_loss: 0.1710 - val_acc: 0.9417\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8493 - val_loss: 0.1787 - val_acc: 0.9405\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8504 - val_loss: 0.2391 - val_acc: 0.9144\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8493 - val_loss: 0.1916 - val_acc: 0.9335\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8497 - val_loss: 0.2229 - val_acc: 0.9219\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8488 - val_loss: 0.2090 - val_acc: 0.9256\n",
      "Epoch 00006: early stopping\n",
      "7770\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8490 - val_loss: 0.2046 - val_acc: 0.9220\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8483 - val_loss: 0.2026 - val_acc: 0.9284\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8492 - val_loss: 0.1606 - val_acc: 0.9506\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8489 - val_loss: 0.1822 - val_acc: 0.9403\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8492 - val_loss: 0.1626 - val_acc: 0.9495\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8480 - val_loss: 0.1819 - val_acc: 0.9417\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8485 - val_loss: 0.1919 - val_acc: 0.9336\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8494 - val_loss: 0.1739 - val_acc: 0.9435\n",
      "Epoch 00008: early stopping\n",
      "7780\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8486 - val_loss: 0.1961 - val_acc: 0.9323\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8494 - val_loss: 0.1895 - val_acc: 0.9351\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8480 - val_loss: 0.1967 - val_acc: 0.9307\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8509 - val_loss: 0.1764 - val_acc: 0.9408\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8490 - val_loss: 0.2091 - val_acc: 0.9259\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8495 - val_loss: 0.1947 - val_acc: 0.9325\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3289 - acc: 0.8482 - val_loss: 0.1911 - val_acc: 0.9347\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8497 - val_loss: 0.1828 - val_acc: 0.9412\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8495 - val_loss: 0.1733 - val_acc: 0.9449\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8506 - val_loss: 0.1586 - val_acc: 0.9509\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8509 - val_loss: 0.1990 - val_acc: 0.9320\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8495 - val_loss: 0.1800 - val_acc: 0.9411\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3298 - acc: 0.8489 - val_loss: 0.1876 - val_acc: 0.9359\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8500 - val_loss: 0.1942 - val_acc: 0.9354\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8494 - val_loss: 0.1950 - val_acc: 0.9358\n",
      "Epoch 00015: early stopping\n",
      "7790\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8476 - val_loss: 0.1917 - val_acc: 0.9349\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8502 - val_loss: 0.1875 - val_acc: 0.9391\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8504 - val_loss: 0.1705 - val_acc: 0.9461\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8505 - val_loss: 0.1614 - val_acc: 0.9511\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8498 - val_loss: 0.1826 - val_acc: 0.9402\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8507 - val_loss: 0.2023 - val_acc: 0.9308\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8505 - val_loss: 0.1756 - val_acc: 0.9426\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8498 - val_loss: 0.1762 - val_acc: 0.9424\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8478 - val_loss: 0.1918 - val_acc: 0.9341\n",
      "Epoch 00009: early stopping\n",
      "7800\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3309 - acc: 0.8474 - val_loss: 0.2066 - val_acc: 0.9306\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8488 - val_loss: 0.1898 - val_acc: 0.9375\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8495 - val_loss: 0.1952 - val_acc: 0.9320\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8481 - val_loss: 0.1775 - val_acc: 0.9434\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8502 - val_loss: 0.2061 - val_acc: 0.9267\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3296 - acc: 0.8490 - val_loss: 0.1625 - val_acc: 0.9474\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8490 - val_loss: 0.1719 - val_acc: 0.9454\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8501 - val_loss: 0.1698 - val_acc: 0.9463\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8511 - val_loss: 0.1897 - val_acc: 0.9362\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3306 - acc: 0.8488 - val_loss: 0.2004 - val_acc: 0.9343\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8484 - val_loss: 0.1854 - val_acc: 0.9411\n",
      "Epoch 00011: early stopping\n",
      "7810\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8501 - val_loss: 0.1800 - val_acc: 0.9412\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8522 - val_loss: 0.1991 - val_acc: 0.9322\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8516 - val_loss: 0.1785 - val_acc: 0.9418\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8494 - val_loss: 0.1998 - val_acc: 0.9345\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8503 - val_loss: 0.1999 - val_acc: 0.9358\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8495 - val_loss: 0.2077 - val_acc: 0.9297\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8493 - val_loss: 0.2212 - val_acc: 0.9237\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8494 - val_loss: 0.1675 - val_acc: 0.9458\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8517 - val_loss: 0.1864 - val_acc: 0.9373\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8494 - val_loss: 0.1958 - val_acc: 0.9333\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8494 - val_loss: 0.1953 - val_acc: 0.9327\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8515 - val_loss: 0.1698 - val_acc: 0.9455\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8500 - val_loss: 0.1719 - val_acc: 0.9406\n",
      "Epoch 00013: early stopping\n",
      "7820\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8477 - val_loss: 0.1706 - val_acc: 0.9462\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8477 - val_loss: 0.1937 - val_acc: 0.9362\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8503 - val_loss: 0.1635 - val_acc: 0.9491\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8487 - val_loss: 0.1795 - val_acc: 0.9404\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8491 - val_loss: 0.1760 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8521 - val_loss: 0.1685 - val_acc: 0.9470\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8501 - val_loss: 0.2004 - val_acc: 0.9281\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8502 - val_loss: 0.1966 - val_acc: 0.9319\n",
      "Epoch 00008: early stopping\n",
      "7830\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8495 - val_loss: 0.2175 - val_acc: 0.9266\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8481 - val_loss: 0.1617 - val_acc: 0.9481\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8489 - val_loss: 0.1859 - val_acc: 0.9364\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8520 - val_loss: 0.2136 - val_acc: 0.9259\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8493 - val_loss: 0.1760 - val_acc: 0.9437\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8496 - val_loss: 0.1982 - val_acc: 0.9328\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8487 - val_loss: 0.1880 - val_acc: 0.9377\n",
      "Epoch 00007: early stopping\n",
      "7840\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8495 - val_loss: 0.1910 - val_acc: 0.9349\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8485 - val_loss: 0.1760 - val_acc: 0.9414\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8503 - val_loss: 0.2031 - val_acc: 0.9302\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8511 - val_loss: 0.1816 - val_acc: 0.9393\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8506 - val_loss: 0.1773 - val_acc: 0.9405\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8505 - val_loss: 0.1751 - val_acc: 0.9432\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8508 - val_loss: 0.1981 - val_acc: 0.9345\n",
      "Epoch 00007: early stopping\n",
      "7850\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8502 - val_loss: 0.1753 - val_acc: 0.9435\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8488 - val_loss: 0.2129 - val_acc: 0.9260\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8497 - val_loss: 0.1672 - val_acc: 0.9476\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8488 - val_loss: 0.1716 - val_acc: 0.9444\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8486 - val_loss: 0.1746 - val_acc: 0.9420\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8489 - val_loss: 0.1800 - val_acc: 0.9415\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8499 - val_loss: 0.1794 - val_acc: 0.9418\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8493 - val_loss: 0.2076 - val_acc: 0.9296\n",
      "Epoch 00008: early stopping\n",
      "0.9260081391046985 7850\n",
      "7860\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8493 - val_loss: 0.1749 - val_acc: 0.9427\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8521 - val_loss: 0.1846 - val_acc: 0.9398\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8498 - val_loss: 0.2105 - val_acc: 0.9293\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8495 - val_loss: 0.2040 - val_acc: 0.9325\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8504 - val_loss: 0.1851 - val_acc: 0.9404\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8504 - val_loss: 0.1811 - val_acc: 0.9414\n",
      "Epoch 00006: early stopping\n",
      "7870\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8513 - val_loss: 0.1952 - val_acc: 0.9366\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8508 - val_loss: 0.1979 - val_acc: 0.9336\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8505 - val_loss: 0.2133 - val_acc: 0.9294\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8504 - val_loss: 0.1765 - val_acc: 0.9420\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8503 - val_loss: 0.1821 - val_acc: 0.9410\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8501 - val_loss: 0.1841 - val_acc: 0.9403\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3288 - acc: 0.8475 - val_loss: 0.1771 - val_acc: 0.9430\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3279 - acc: 0.8487 - val_loss: 0.1763 - val_acc: 0.9442\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8516 - val_loss: 0.2136 - val_acc: 0.9281\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7880\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8495 - val_loss: 0.1816 - val_acc: 0.9423\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8504 - val_loss: 0.2024 - val_acc: 0.9329\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8509 - val_loss: 0.2304 - val_acc: 0.9199\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8495 - val_loss: 0.2028 - val_acc: 0.9295\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8501 - val_loss: 0.1889 - val_acc: 0.9379\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8500 - val_loss: 0.1893 - val_acc: 0.9375\n",
      "Epoch 00006: early stopping\n",
      "7890\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8501 - val_loss: 0.2019 - val_acc: 0.9314\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8501 - val_loss: 0.1631 - val_acc: 0.9497\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8511 - val_loss: 0.1895 - val_acc: 0.9365\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8500 - val_loss: 0.1656 - val_acc: 0.9496\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8488 - val_loss: 0.1912 - val_acc: 0.9351\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8488 - val_loss: 0.1664 - val_acc: 0.9459\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8502 - val_loss: 0.1840 - val_acc: 0.9349\n",
      "Epoch 00007: early stopping\n",
      "7900\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8491 - val_loss: 0.1862 - val_acc: 0.9372\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8499 - val_loss: 0.1783 - val_acc: 0.9424\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8498 - val_loss: 0.1904 - val_acc: 0.9357\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8523 - val_loss: 0.1900 - val_acc: 0.9377\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8487 - val_loss: 0.2012 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8508 - val_loss: 0.1808 - val_acc: 0.9438\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8525 - val_loss: 0.2064 - val_acc: 0.9283\n",
      "Epoch 00007: early stopping\n",
      "0.9304476507584166 7900\n",
      "7910\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8487 - val_loss: 0.1798 - val_acc: 0.9412\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8520 - val_loss: 0.1774 - val_acc: 0.9414\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8496 - val_loss: 0.1676 - val_acc: 0.9467\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8502 - val_loss: 0.1968 - val_acc: 0.9294\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3287 - acc: 0.8477 - val_loss: 0.1881 - val_acc: 0.9346\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3302 - acc: 0.8489 - val_loss: 0.1756 - val_acc: 0.9432\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8508 - val_loss: 0.1725 - val_acc: 0.9444\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8479 - val_loss: 0.1975 - val_acc: 0.9329\n",
      "Epoch 00008: early stopping\n",
      "7920\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8490 - val_loss: 0.1884 - val_acc: 0.9379\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8486 - val_loss: 0.1737 - val_acc: 0.9423\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8486 - val_loss: 0.1746 - val_acc: 0.9409\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8480 - val_loss: 0.1814 - val_acc: 0.9397\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8493 - val_loss: 0.2027 - val_acc: 0.9296\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8515 - val_loss: 0.1732 - val_acc: 0.9454\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8515 - val_loss: 0.1982 - val_acc: 0.9304\n",
      "Epoch 00007: early stopping\n",
      "7930\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8505 - val_loss: 0.1745 - val_acc: 0.9435\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8505 - val_loss: 0.1754 - val_acc: 0.9436\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8495 - val_loss: 0.1708 - val_acc: 0.9449\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8510 - val_loss: 0.1739 - val_acc: 0.9433\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8520 - val_loss: 0.1907 - val_acc: 0.9358\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8502 - val_loss: 0.1768 - val_acc: 0.9428\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8512 - val_loss: 0.1893 - val_acc: 0.9348\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8492 - val_loss: 0.1782 - val_acc: 0.9423\n",
      "Epoch 00008: early stopping\n",
      "7940\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8490 - val_loss: 0.1721 - val_acc: 0.9451\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8512 - val_loss: 0.1708 - val_acc: 0.9441\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8493 - val_loss: 0.1840 - val_acc: 0.9400\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8493 - val_loss: 0.1925 - val_acc: 0.9359\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8511 - val_loss: 0.1691 - val_acc: 0.9456\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8503 - val_loss: 0.1746 - val_acc: 0.9428\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8505 - val_loss: 0.1677 - val_acc: 0.9447\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8491 - val_loss: 0.1897 - val_acc: 0.9349\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8497 - val_loss: 0.2191 - val_acc: 0.9250\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8501 - val_loss: 0.1910 - val_acc: 0.9367\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8505 - val_loss: 0.1744 - val_acc: 0.9443\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8506 - val_loss: 0.1911 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: early stopping\n",
      "7950\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8516 - val_loss: 0.1714 - val_acc: 0.9441\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8503 - val_loss: 0.1798 - val_acc: 0.9409\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8498 - val_loss: 0.1740 - val_acc: 0.9440\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8494 - val_loss: 0.1969 - val_acc: 0.9343\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8490 - val_loss: 0.1714 - val_acc: 0.9424\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8506 - val_loss: 0.2147 - val_acc: 0.9235\n",
      "Epoch 00006: early stopping\n",
      "7960\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8490 - val_loss: 0.1753 - val_acc: 0.9421\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8500 - val_loss: 0.1763 - val_acc: 0.9442\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8492 - val_loss: 0.1624 - val_acc: 0.9491\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8490 - val_loss: 0.1823 - val_acc: 0.9411\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8491 - val_loss: 0.1763 - val_acc: 0.9438\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8479 - val_loss: 0.1930 - val_acc: 0.9322\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8495 - val_loss: 0.1771 - val_acc: 0.9410\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8507 - val_loss: 0.1735 - val_acc: 0.9454\n",
      "Epoch 00008: early stopping\n",
      "7970\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8505 - val_loss: 0.2280 - val_acc: 0.9212\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8504 - val_loss: 0.1669 - val_acc: 0.9478\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8495 - val_loss: 0.1710 - val_acc: 0.9444\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8486 - val_loss: 0.2011 - val_acc: 0.9342\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8493 - val_loss: 0.1582 - val_acc: 0.9500\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8502 - val_loss: 0.1780 - val_acc: 0.9425\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8507 - val_loss: 0.1663 - val_acc: 0.9479\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8502 - val_loss: 0.1888 - val_acc: 0.9361\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8500 - val_loss: 0.1785 - val_acc: 0.9407\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8501 - val_loss: 0.1716 - val_acc: 0.9445\n",
      "Epoch 00010: early stopping\n",
      "7980\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8507 - val_loss: 0.2007 - val_acc: 0.9362\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8487 - val_loss: 0.1610 - val_acc: 0.9505\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8515 - val_loss: 0.1911 - val_acc: 0.9341\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8509 - val_loss: 0.1830 - val_acc: 0.9388\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8501 - val_loss: 0.1843 - val_acc: 0.9372\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8487 - val_loss: 0.1942 - val_acc: 0.9394\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8504 - val_loss: 0.1849 - val_acc: 0.9392\n",
      "Epoch 00007: early stopping\n",
      "7990\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8500 - val_loss: 0.1510 - val_acc: 0.9553\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8516 - val_loss: 0.1769 - val_acc: 0.9424\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8506 - val_loss: 0.1745 - val_acc: 0.9445\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8497 - val_loss: 0.1614 - val_acc: 0.9467\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8483 - val_loss: 0.1580 - val_acc: 0.9504\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8509 - val_loss: 0.1756 - val_acc: 0.9442\n",
      "Epoch 00006: early stopping\n",
      "8000\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8494 - val_loss: 0.1676 - val_acc: 0.9440\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8492 - val_loss: 0.1679 - val_acc: 0.9463\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8504 - val_loss: 0.1669 - val_acc: 0.9483\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8478 - val_loss: 0.1797 - val_acc: 0.9428\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8485 - val_loss: 0.2098 - val_acc: 0.9294\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8495 - val_loss: 0.2072 - val_acc: 0.9259\n",
      "Epoch 00006: early stopping\n",
      "8010\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8511 - val_loss: 0.2080 - val_acc: 0.9294\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8491 - val_loss: 0.1580 - val_acc: 0.9533\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8506 - val_loss: 0.1634 - val_acc: 0.9504\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8511 - val_loss: 0.1866 - val_acc: 0.9405\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8497 - val_loss: 0.1634 - val_acc: 0.9476\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8492 - val_loss: 0.1696 - val_acc: 0.9474\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8518 - val_loss: 0.1766 - val_acc: 0.9435\n",
      "Epoch 00007: early stopping\n",
      "8020\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8480 - val_loss: 0.1830 - val_acc: 0.9386\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8502 - val_loss: 0.1645 - val_acc: 0.9508\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8513 - val_loss: 0.1657 - val_acc: 0.9481\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.3245 - acc: 0.8497 - val_loss: 0.1691 - val_acc: 0.9486\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3322 - acc: 0.8497 - val_loss: 0.1588 - val_acc: 0.9520\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8503 - val_loss: 0.1688 - val_acc: 0.9459\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8507 - val_loss: 0.1813 - val_acc: 0.9399\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.3284 - acc: 0.8488 - val_loss: 0.1796 - val_acc: 0.9415\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8500 - val_loss: 0.1618 - val_acc: 0.9519\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8511 - val_loss: 0.1670 - val_acc: 0.9453\n",
      "Epoch 00010: early stopping\n",
      "8030\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8514 - val_loss: 0.1656 - val_acc: 0.9495\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8498 - val_loss: 0.1683 - val_acc: 0.9470\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8512 - val_loss: 0.1735 - val_acc: 0.9469\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8510 - val_loss: 0.1769 - val_acc: 0.9432\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8486 - val_loss: 0.1607 - val_acc: 0.9492\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8483 - val_loss: 0.1609 - val_acc: 0.9486\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8500 - val_loss: 0.1801 - val_acc: 0.9378\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8508 - val_loss: 0.1698 - val_acc: 0.9428\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8495 - val_loss: 0.1826 - val_acc: 0.9389\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8496 - val_loss: 0.1971 - val_acc: 0.9317\n",
      "Epoch 00010: early stopping\n",
      "8040\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8503 - val_loss: 0.2081 - val_acc: 0.9253\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8510 - val_loss: 0.1631 - val_acc: 0.9478\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3295 - acc: 0.8487 - val_loss: 0.1565 - val_acc: 0.9529\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8495 - val_loss: 0.1722 - val_acc: 0.9413\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8517 - val_loss: 0.1637 - val_acc: 0.9479\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8508 - val_loss: 0.2002 - val_acc: 0.9310\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8506 - val_loss: 0.1898 - val_acc: 0.9369\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3305 - acc: 0.8469 - val_loss: 0.1521 - val_acc: 0.9531\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8499 - val_loss: 0.1565 - val_acc: 0.9508\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3290 - acc: 0.8487 - val_loss: 0.1736 - val_acc: 0.9437\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8490 - val_loss: 0.2029 - val_acc: 0.9306\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8490 - val_loss: 0.1635 - val_acc: 0.9503\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3282 - acc: 0.8498 - val_loss: 0.1763 - val_acc: 0.9444\n",
      "Epoch 00013: early stopping\n",
      "8050\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8512 - val_loss: 0.2380 - val_acc: 0.9124\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8490 - val_loss: 0.1596 - val_acc: 0.9511\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8494 - val_loss: 0.1885 - val_acc: 0.9370\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8509 - val_loss: 0.1688 - val_acc: 0.9471\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8486 - val_loss: 0.1496 - val_acc: 0.9537\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3292 - acc: 0.8465 - val_loss: 0.1830 - val_acc: 0.9378\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3271 - acc: 0.8482 - val_loss: 0.1732 - val_acc: 0.9414\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8498 - val_loss: 0.1963 - val_acc: 0.9361\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8494 - val_loss: 0.2289 - val_acc: 0.9129\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8488 - val_loss: 0.1704 - val_acc: 0.9453\n",
      "Epoch 00010: early stopping\n",
      "8060\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8467 - val_loss: 0.1774 - val_acc: 0.9442\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8500 - val_loss: 0.1494 - val_acc: 0.9544\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3300 - acc: 0.8470 - val_loss: 0.2034 - val_acc: 0.9294\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8493 - val_loss: 0.2277 - val_acc: 0.9162\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8490 - val_loss: 0.1901 - val_acc: 0.9341\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8491 - val_loss: 0.2224 - val_acc: 0.9189\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8490 - val_loss: 0.1656 - val_acc: 0.9526\n",
      "Epoch 00007: early stopping\n",
      "8070\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3299 - acc: 0.8472 - val_loss: 0.2164 - val_acc: 0.9211\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8490 - val_loss: 0.1847 - val_acc: 0.9378\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8498 - val_loss: 0.1709 - val_acc: 0.9460\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8484 - val_loss: 0.2309 - val_acc: 0.9165\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8478 - val_loss: 0.1874 - val_acc: 0.9376\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8461 - val_loss: 0.1973 - val_acc: 0.9351\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3307 - acc: 0.8478 - val_loss: 0.2152 - val_acc: 0.9278\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8490 - val_loss: 0.2348 - val_acc: 0.9083\n",
      "Epoch 00008: early stopping\n",
      "8080\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8481 - val_loss: 0.1959 - val_acc: 0.9316\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8491 - val_loss: 0.1720 - val_acc: 0.9457\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8494 - val_loss: 0.2126 - val_acc: 0.9263\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8494 - val_loss: 0.1776 - val_acc: 0.9413\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8492 - val_loss: 0.1871 - val_acc: 0.9368\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8484 - val_loss: 0.1904 - val_acc: 0.9349\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8509 - val_loss: 0.2105 - val_acc: 0.9268\n",
      "Epoch 00007: early stopping\n",
      "8090\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8497 - val_loss: 0.1793 - val_acc: 0.9424\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8496 - val_loss: 0.2033 - val_acc: 0.9290\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8496 - val_loss: 0.2025 - val_acc: 0.9298\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8497 - val_loss: 0.2117 - val_acc: 0.9248\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3283 - acc: 0.8487 - val_loss: 0.1984 - val_acc: 0.9290\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8505 - val_loss: 0.1963 - val_acc: 0.9316\n",
      "Epoch 00006: early stopping\n",
      "8100\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8494 - val_loss: 0.2149 - val_acc: 0.9260\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8517 - val_loss: 0.1715 - val_acc: 0.9422\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8490 - val_loss: 0.2018 - val_acc: 0.9303\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8518 - val_loss: 0.2200 - val_acc: 0.9207\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8490 - val_loss: 0.1949 - val_acc: 0.9314\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8498 - val_loss: 0.1704 - val_acc: 0.9447\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8507 - val_loss: 0.1854 - val_acc: 0.9384\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8488 - val_loss: 0.1862 - val_acc: 0.9368\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8500 - val_loss: 0.2288 - val_acc: 0.9184\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8507 - val_loss: 0.1993 - val_acc: 0.9316\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8498 - val_loss: 0.2261 - val_acc: 0.9209\n",
      "Epoch 00011: early stopping\n",
      "8110\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8502 - val_loss: 0.1641 - val_acc: 0.9470\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8485 - val_loss: 0.2136 - val_acc: 0.9266\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8495 - val_loss: 0.1894 - val_acc: 0.9364\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8497 - val_loss: 0.1999 - val_acc: 0.9312\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8522 - val_loss: 0.1974 - val_acc: 0.9295\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8527 - val_loss: 0.2502 - val_acc: 0.9079\n",
      "Epoch 00006: early stopping\n",
      "8120\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3275 - acc: 0.8507 - val_loss: 0.2353 - val_acc: 0.9163\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8509 - val_loss: 0.1926 - val_acc: 0.9347\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8502 - val_loss: 0.1957 - val_acc: 0.9345\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3215 - acc: 0.8531 - val_loss: 0.1731 - val_acc: 0.9427\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8509 - val_loss: 0.1762 - val_acc: 0.9452\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.3217 - acc: 0.8522 - val_loss: 0.1795 - val_acc: 0.9391\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8497 - val_loss: 0.2035 - val_acc: 0.9279\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8496 - val_loss: 0.2117 - val_acc: 0.9220\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8509 - val_loss: 0.1962 - val_acc: 0.9321\n",
      "Epoch 00009: early stopping\n",
      "8130\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8509 - val_loss: 0.2229 - val_acc: 0.9198\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8516 - val_loss: 0.1934 - val_acc: 0.9366\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8505 - val_loss: 0.2004 - val_acc: 0.9296\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8496 - val_loss: 0.1943 - val_acc: 0.9321\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8494 - val_loss: 0.1922 - val_acc: 0.9299\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8482 - val_loss: 0.1886 - val_acc: 0.9336\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8486 - val_loss: 0.1981 - val_acc: 0.9291\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8493 - val_loss: 0.2044 - val_acc: 0.9284\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8485 - val_loss: 0.2213 - val_acc: 0.9257\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8501 - val_loss: 0.2155 - val_acc: 0.9220\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8513 - val_loss: 0.1750 - val_acc: 0.9419\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8498 - val_loss: 0.1887 - val_acc: 0.9368\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8501 - val_loss: 0.1748 - val_acc: 0.9438\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8482 - val_loss: 0.1797 - val_acc: 0.9411\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3280 - acc: 0.8483 - val_loss: 0.1812 - val_acc: 0.9387\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8514 - val_loss: 0.1845 - val_acc: 0.9389\n",
      "Epoch 00016: early stopping\n",
      "8140\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8508 - val_loss: 0.2244 - val_acc: 0.9184\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8514 - val_loss: 0.1804 - val_acc: 0.9408\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8496 - val_loss: 0.1672 - val_acc: 0.9485\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8510 - val_loss: 0.1840 - val_acc: 0.9381\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8495 - val_loss: 0.1961 - val_acc: 0.9341\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8510 - val_loss: 0.1849 - val_acc: 0.9402\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8491 - val_loss: 0.2139 - val_acc: 0.9259\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8513 - val_loss: 0.2184 - val_acc: 0.9248\n",
      "Epoch 00008: early stopping\n",
      "8150\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8502 - val_loss: 0.2176 - val_acc: 0.9185\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8506 - val_loss: 0.2168 - val_acc: 0.9239\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8500 - val_loss: 0.1689 - val_acc: 0.9487\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8491 - val_loss: 0.1918 - val_acc: 0.9380\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8497 - val_loss: 0.1913 - val_acc: 0.9382\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8501 - val_loss: 0.1912 - val_acc: 0.9394\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3263 - acc: 0.8489 - val_loss: 0.1723 - val_acc: 0.9459\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8478 - val_loss: 0.1846 - val_acc: 0.9419\n",
      "Epoch 00008: early stopping\n",
      "8160\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8514 - val_loss: 0.1865 - val_acc: 0.9388\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8505 - val_loss: 0.1876 - val_acc: 0.9358\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8499 - val_loss: 0.2120 - val_acc: 0.9223\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8502 - val_loss: 0.2190 - val_acc: 0.9223\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8490 - val_loss: 0.1973 - val_acc: 0.9322\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 4us/step - loss: 0.3238 - acc: 0.8509 - val_loss: 0.2074 - val_acc: 0.9296\n",
      "Epoch 00006: early stopping\n",
      "8170\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8501 - val_loss: 0.1911 - val_acc: 0.9348\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8508 - val_loss: 0.1756 - val_acc: 0.9428\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8513 - val_loss: 0.2234 - val_acc: 0.9226\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8495 - val_loss: 0.1866 - val_acc: 0.9374\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8491 - val_loss: 0.1815 - val_acc: 0.9422\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8507 - val_loss: 0.1592 - val_acc: 0.9494\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8510 - val_loss: 0.1779 - val_acc: 0.9421\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8491 - val_loss: 0.2009 - val_acc: 0.9295\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8504 - val_loss: 0.1710 - val_acc: 0.9444\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3276 - acc: 0.8480 - val_loss: 0.2432 - val_acc: 0.9159\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8504 - val_loss: 0.1984 - val_acc: 0.9317\n",
      "Epoch 00011: early stopping\n",
      "8180\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8505 - val_loss: 0.1656 - val_acc: 0.9474\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8486 - val_loss: 0.1820 - val_acc: 0.9373\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8488 - val_loss: 0.1987 - val_acc: 0.9314\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8479 - val_loss: 0.1692 - val_acc: 0.9478\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8517 - val_loss: 0.2020 - val_acc: 0.9305\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8505 - val_loss: 0.2012 - val_acc: 0.9295\n",
      "Epoch 00006: early stopping\n",
      "8190\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8494 - val_loss: 0.1666 - val_acc: 0.9460\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8505 - val_loss: 0.1645 - val_acc: 0.9475\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8506 - val_loss: 0.1901 - val_acc: 0.9355\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8498 - val_loss: 0.1681 - val_acc: 0.9463\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8508 - val_loss: 0.1712 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8524 - val_loss: 0.1857 - val_acc: 0.9391\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8486 - val_loss: 0.1823 - val_acc: 0.9367\n",
      "Epoch 00007: early stopping\n",
      "8200\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3197 - acc: 0.8526 - val_loss: 0.1979 - val_acc: 0.9329\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8487 - val_loss: 0.1702 - val_acc: 0.9436\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8495 - val_loss: 0.1551 - val_acc: 0.9532\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8507 - val_loss: 0.1644 - val_acc: 0.9458\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8484 - val_loss: 0.2073 - val_acc: 0.9250\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8508 - val_loss: 0.1831 - val_acc: 0.9385\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8501 - val_loss: 0.1894 - val_acc: 0.9378\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8475 - val_loss: 0.1756 - val_acc: 0.9435\n",
      "Epoch 00008: early stopping\n",
      "8210\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8495 - val_loss: 0.2047 - val_acc: 0.9294\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8503 - val_loss: 0.1912 - val_acc: 0.9372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8495 - val_loss: 0.2013 - val_acc: 0.9318\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8508 - val_loss: 0.1794 - val_acc: 0.9423\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8487 - val_loss: 0.1990 - val_acc: 0.9298\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8512 - val_loss: 0.2086 - val_acc: 0.9276\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8504 - val_loss: 0.1955 - val_acc: 0.9309\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8507 - val_loss: 0.1861 - val_acc: 0.9385\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8506 - val_loss: 0.1699 - val_acc: 0.9424\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8494 - val_loss: 0.1906 - val_acc: 0.9343\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8506 - val_loss: 0.1823 - val_acc: 0.9421\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8501 - val_loss: 0.1738 - val_acc: 0.9437\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8499 - val_loss: 0.1793 - val_acc: 0.9408\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8496 - val_loss: 0.1762 - val_acc: 0.9408\n",
      "Epoch 00014: early stopping\n",
      "8220\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8506 - val_loss: 0.1946 - val_acc: 0.9375\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8511 - val_loss: 0.1995 - val_acc: 0.9300\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8504 - val_loss: 0.1745 - val_acc: 0.9430\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8508 - val_loss: 0.1663 - val_acc: 0.9492\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8506 - val_loss: 0.2095 - val_acc: 0.9274\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8498 - val_loss: 0.1755 - val_acc: 0.9429\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8495 - val_loss: 0.1697 - val_acc: 0.9412\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8498 - val_loss: 0.1926 - val_acc: 0.9333\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8476 - val_loss: 0.1728 - val_acc: 0.9457\n",
      "Epoch 00009: early stopping\n",
      "8230\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8503 - val_loss: 0.1966 - val_acc: 0.9322\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8515 - val_loss: 0.1667 - val_acc: 0.9460\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8506 - val_loss: 0.1898 - val_acc: 0.9348\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8506 - val_loss: 0.1880 - val_acc: 0.9355\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8490 - val_loss: 0.1874 - val_acc: 0.9374\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8496 - val_loss: 0.1655 - val_acc: 0.9464\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8500 - val_loss: 0.1761 - val_acc: 0.9407\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8509 - val_loss: 0.1753 - val_acc: 0.9424\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8501 - val_loss: 0.1785 - val_acc: 0.9414\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8515 - val_loss: 0.1695 - val_acc: 0.9446\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8493 - val_loss: 0.1806 - val_acc: 0.9398\n",
      "Epoch 00011: early stopping\n",
      "8240\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8522 - val_loss: 0.1788 - val_acc: 0.9392\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8503 - val_loss: 0.1771 - val_acc: 0.9404\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8503 - val_loss: 0.1992 - val_acc: 0.9326\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8489 - val_loss: 0.2004 - val_acc: 0.9310\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8507 - val_loss: 0.1740 - val_acc: 0.9429\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8507 - val_loss: 0.1725 - val_acc: 0.9431\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8487 - val_loss: 0.1827 - val_acc: 0.9354\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3186 - acc: 0.8539 - val_loss: 0.2038 - val_acc: 0.9301\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8489 - val_loss: 0.1808 - val_acc: 0.9411\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8477 - val_loss: 0.1676 - val_acc: 0.9458\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8495 - val_loss: 0.1830 - val_acc: 0.9375\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8516 - val_loss: 0.1818 - val_acc: 0.9394\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8503 - val_loss: 0.2337 - val_acc: 0.9142\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8488 - val_loss: 0.1817 - val_acc: 0.9390\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8488 - val_loss: 0.1755 - val_acc: 0.9410\n",
      "Epoch 00015: early stopping\n",
      "8250\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8497 - val_loss: 0.1971 - val_acc: 0.9330\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8499 - val_loss: 0.1883 - val_acc: 0.9377\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8509 - val_loss: 0.1804 - val_acc: 0.9376\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8494 - val_loss: 0.1609 - val_acc: 0.9459\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8493 - val_loss: 0.1897 - val_acc: 0.9336\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8493 - val_loss: 0.1805 - val_acc: 0.9388\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8482 - val_loss: 0.1936 - val_acc: 0.9327\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8486 - val_loss: 0.2174 - val_acc: 0.9214\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8493 - val_loss: 0.2078 - val_acc: 0.9269\n",
      "Epoch 00009: early stopping\n",
      "8260\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8513 - val_loss: 0.1787 - val_acc: 0.9392\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8484 - val_loss: 0.1806 - val_acc: 0.9382\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8483 - val_loss: 0.2251 - val_acc: 0.9172\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8508 - val_loss: 0.1723 - val_acc: 0.9411\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8475 - val_loss: 0.1704 - val_acc: 0.9410\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8486 - val_loss: 0.1765 - val_acc: 0.9397\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8515 - val_loss: 0.1928 - val_acc: 0.9321\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8499 - val_loss: 0.1887 - val_acc: 0.9350\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8484 - val_loss: 0.1791 - val_acc: 0.9415\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8501 - val_loss: 0.1754 - val_acc: 0.9421\n",
      "Epoch 00010: early stopping\n",
      "8270\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8519 - val_loss: 0.1832 - val_acc: 0.9354\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3202 - acc: 0.8514 - val_loss: 0.1819 - val_acc: 0.9362\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8498 - val_loss: 0.1790 - val_acc: 0.9402\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8494 - val_loss: 0.1767 - val_acc: 0.9438\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8486 - val_loss: 0.1790 - val_acc: 0.9399\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8509 - val_loss: 0.1671 - val_acc: 0.9478\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8512 - val_loss: 0.1945 - val_acc: 0.9324\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8500 - val_loss: 0.1945 - val_acc: 0.9311\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3197 - acc: 0.8511 - val_loss: 0.1865 - val_acc: 0.9350\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8499 - val_loss: 0.1810 - val_acc: 0.9379\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8497 - val_loss: 0.1756 - val_acc: 0.9407\n",
      "Epoch 00011: early stopping\n",
      "8280\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8502 - val_loss: 0.1574 - val_acc: 0.9501\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8510 - val_loss: 0.1944 - val_acc: 0.9327\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8510 - val_loss: 0.2018 - val_acc: 0.9275\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8512 - val_loss: 0.1629 - val_acc: 0.9459\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8500 - val_loss: 0.1809 - val_acc: 0.9389\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8504 - val_loss: 0.1739 - val_acc: 0.9423\n",
      "Epoch 00006: early stopping\n",
      "8290\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8510 - val_loss: 0.1912 - val_acc: 0.9366\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8518 - val_loss: 0.1895 - val_acc: 0.9336\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8494 - val_loss: 0.1770 - val_acc: 0.9427\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8510 - val_loss: 0.1798 - val_acc: 0.9396\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8497 - val_loss: 0.1807 - val_acc: 0.9403\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8503 - val_loss: 0.1953 - val_acc: 0.9348\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8522 - val_loss: 0.2003 - val_acc: 0.9296\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3209 - acc: 0.8522 - val_loss: 0.1771 - val_acc: 0.9418\n",
      "Epoch 00008: early stopping\n",
      "8300\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8488 - val_loss: 0.1598 - val_acc: 0.9513\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3270 - acc: 0.8488 - val_loss: 0.1778 - val_acc: 0.9398\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8501 - val_loss: 0.1792 - val_acc: 0.9363\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8504 - val_loss: 0.1715 - val_acc: 0.9407\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3210 - acc: 0.8497 - val_loss: 0.1813 - val_acc: 0.9387\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8498 - val_loss: 0.1833 - val_acc: 0.9403\n",
      "Epoch 00006: early stopping\n",
      "8310\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8499 - val_loss: 0.1903 - val_acc: 0.9359\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8495 - val_loss: 0.2117 - val_acc: 0.9221\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8488 - val_loss: 0.1722 - val_acc: 0.9409\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8496 - val_loss: 0.1813 - val_acc: 0.9378\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8503 - val_loss: 0.1821 - val_acc: 0.9369\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8501 - val_loss: 0.1865 - val_acc: 0.9350\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8492 - val_loss: 0.1728 - val_acc: 0.9419\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8493 - val_loss: 0.1776 - val_acc: 0.9415\n",
      "Epoch 00008: early stopping\n",
      "8320\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8502 - val_loss: 0.1766 - val_acc: 0.9432\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8511 - val_loss: 0.1756 - val_acc: 0.9430\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8507 - val_loss: 0.1950 - val_acc: 0.9350\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8514 - val_loss: 0.1936 - val_acc: 0.9342\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3269 - acc: 0.8499 - val_loss: 0.1929 - val_acc: 0.9342\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8517 - val_loss: 0.2026 - val_acc: 0.9298\n",
      "Epoch 00006: early stopping\n",
      "8330\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8493 - val_loss: 0.1700 - val_acc: 0.9450\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8502 - val_loss: 0.2061 - val_acc: 0.9262\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3204 - acc: 0.8525 - val_loss: 0.1931 - val_acc: 0.9328\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8516 - val_loss: 0.2065 - val_acc: 0.9312\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8499 - val_loss: 0.2064 - val_acc: 0.9291\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8487 - val_loss: 0.1744 - val_acc: 0.9417\n",
      "Epoch 00006: early stopping\n",
      "8340\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8509 - val_loss: 0.1787 - val_acc: 0.9429\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3268 - acc: 0.8493 - val_loss: 0.1845 - val_acc: 0.9390\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8491 - val_loss: 0.1700 - val_acc: 0.9441\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8501 - val_loss: 0.1824 - val_acc: 0.9398\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8497 - val_loss: 0.1934 - val_acc: 0.9345\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8492 - val_loss: 0.1834 - val_acc: 0.9381\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8492 - val_loss: 0.1982 - val_acc: 0.9280\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3210 - acc: 0.8487 - val_loss: 0.1949 - val_acc: 0.9312\n",
      "Epoch 00008: early stopping\n",
      "8350\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8517 - val_loss: 0.1773 - val_acc: 0.9408\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8501 - val_loss: 0.1805 - val_acc: 0.9400\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8498 - val_loss: 0.1696 - val_acc: 0.9478\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8492 - val_loss: 0.1778 - val_acc: 0.9412\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8504 - val_loss: 0.1878 - val_acc: 0.9353\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8503 - val_loss: 0.1680 - val_acc: 0.9474\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8503 - val_loss: 0.1757 - val_acc: 0.9425\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8492 - val_loss: 0.1790 - val_acc: 0.9433\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8494 - val_loss: 0.1685 - val_acc: 0.9453\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8501 - val_loss: 0.1567 - val_acc: 0.9530\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8494 - val_loss: 0.2056 - val_acc: 0.9308\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8505 - val_loss: 0.1759 - val_acc: 0.9434\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8508 - val_loss: 0.2038 - val_acc: 0.9294\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8513 - val_loss: 0.1852 - val_acc: 0.9387\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8508 - val_loss: 0.1813 - val_acc: 0.9394\n",
      "Epoch 00015: early stopping\n",
      "8360\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3195 - acc: 0.8526 - val_loss: 0.1893 - val_acc: 0.9355\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8509 - val_loss: 0.1935 - val_acc: 0.9350\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8489 - val_loss: 0.1954 - val_acc: 0.9361\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8501 - val_loss: 0.1782 - val_acc: 0.9390\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8479 - val_loss: 0.1696 - val_acc: 0.9465\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8482 - val_loss: 0.1724 - val_acc: 0.9416\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8517 - val_loss: 0.1742 - val_acc: 0.9418\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8500 - val_loss: 0.1720 - val_acc: 0.9423\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8493 - val_loss: 0.2030 - val_acc: 0.9304\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8507 - val_loss: 0.2026 - val_acc: 0.9321\n",
      "Epoch 00010: early stopping\n",
      "8370\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8504 - val_loss: 0.1684 - val_acc: 0.9471\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8494 - val_loss: 0.1705 - val_acc: 0.9456\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8496 - val_loss: 0.1656 - val_acc: 0.9470\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8511 - val_loss: 0.1574 - val_acc: 0.9525\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8515 - val_loss: 0.1915 - val_acc: 0.9341\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8488 - val_loss: 0.1610 - val_acc: 0.9494\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8499 - val_loss: 0.1811 - val_acc: 0.9386\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8502 - val_loss: 0.1662 - val_acc: 0.9461\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8492 - val_loss: 0.1711 - val_acc: 0.9426\n",
      "Epoch 00009: early stopping\n",
      "8380\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8490 - val_loss: 0.1712 - val_acc: 0.9433\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8494 - val_loss: 0.1727 - val_acc: 0.9418\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8489 - val_loss: 0.1803 - val_acc: 0.9381\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3210 - acc: 0.8498 - val_loss: 0.1534 - val_acc: 0.9499\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8498 - val_loss: 0.2348 - val_acc: 0.9132\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3273 - acc: 0.8478 - val_loss: 0.1572 - val_acc: 0.9493\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8496 - val_loss: 0.1796 - val_acc: 0.9401\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8488 - val_loss: 0.1812 - val_acc: 0.9403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8493 - val_loss: 0.1585 - val_acc: 0.9508\n",
      "Epoch 00009: early stopping\n",
      "8390\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8494 - val_loss: 0.1671 - val_acc: 0.9478\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8511 - val_loss: 0.1868 - val_acc: 0.9373\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8513 - val_loss: 0.1847 - val_acc: 0.9340\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8482 - val_loss: 0.2097 - val_acc: 0.9268\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8493 - val_loss: 0.1797 - val_acc: 0.9392\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8492 - val_loss: 0.1707 - val_acc: 0.9475\n",
      "Epoch 00006: early stopping\n",
      "8400\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8506 - val_loss: 0.1769 - val_acc: 0.9385\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8498 - val_loss: 0.1901 - val_acc: 0.9327\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8499 - val_loss: 0.1552 - val_acc: 0.9538\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8495 - val_loss: 0.1619 - val_acc: 0.9491\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8501 - val_loss: 0.1587 - val_acc: 0.9499\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8501 - val_loss: 0.2412 - val_acc: 0.9150\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8499 - val_loss: 0.1755 - val_acc: 0.9406\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8512 - val_loss: 0.1876 - val_acc: 0.9364\n",
      "Epoch 00008: early stopping\n",
      "8410\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8485 - val_loss: 0.1654 - val_acc: 0.9439\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3200 - acc: 0.8517 - val_loss: 0.1758 - val_acc: 0.9429\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8481 - val_loss: 0.1597 - val_acc: 0.9502\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8481 - val_loss: 0.1604 - val_acc: 0.9479\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8493 - val_loss: 0.1673 - val_acc: 0.9457\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3200 - acc: 0.8498 - val_loss: 0.1828 - val_acc: 0.9400\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3174 - acc: 0.8520 - val_loss: 0.2041 - val_acc: 0.9248\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8524 - val_loss: 0.1680 - val_acc: 0.9465\n",
      "Epoch 00008: early stopping\n",
      "8420\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8504 - val_loss: 0.1675 - val_acc: 0.9471\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8494 - val_loss: 0.1692 - val_acc: 0.9452\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8498 - val_loss: 0.1847 - val_acc: 0.9366\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8514 - val_loss: 0.2198 - val_acc: 0.9251\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8514 - val_loss: 0.2035 - val_acc: 0.9303\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8515 - val_loss: 0.1658 - val_acc: 0.9491\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3274 - acc: 0.8478 - val_loss: 0.1935 - val_acc: 0.9365\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8508 - val_loss: 0.1778 - val_acc: 0.9450\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8491 - val_loss: 0.1815 - val_acc: 0.9407\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8513 - val_loss: 0.1822 - val_acc: 0.9399\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3214 - acc: 0.8504 - val_loss: 0.2031 - val_acc: 0.9305\n",
      "Epoch 00011: early stopping\n",
      "8430\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8509 - val_loss: 0.1784 - val_acc: 0.9389\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8501 - val_loss: 0.1816 - val_acc: 0.9397\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8512 - val_loss: 0.1848 - val_acc: 0.9387\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8498 - val_loss: 0.1881 - val_acc: 0.9387\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8499 - val_loss: 0.1691 - val_acc: 0.9458\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8491 - val_loss: 0.1745 - val_acc: 0.9453\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3198 - acc: 0.8506 - val_loss: 0.1625 - val_acc: 0.9515\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8504 - val_loss: 0.1850 - val_acc: 0.9409\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8495 - val_loss: 0.1851 - val_acc: 0.9401\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8514 - val_loss: 0.1784 - val_acc: 0.9428\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8508 - val_loss: 0.1950 - val_acc: 0.9326\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8498 - val_loss: 0.1837 - val_acc: 0.9360\n",
      "Epoch 00012: early stopping\n",
      "8440\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8490 - val_loss: 0.1788 - val_acc: 0.9433\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8507 - val_loss: 0.1573 - val_acc: 0.9521\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8491 - val_loss: 0.1808 - val_acc: 0.9379\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8503 - val_loss: 0.1882 - val_acc: 0.9342\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8499 - val_loss: 0.1892 - val_acc: 0.9350\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3278 - acc: 0.8487 - val_loss: 0.2174 - val_acc: 0.9282\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8498 - val_loss: 0.1732 - val_acc: 0.9444\n",
      "Epoch 00007: early stopping\n",
      "8450\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8494 - val_loss: 0.1953 - val_acc: 0.9362\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8484 - val_loss: 0.1927 - val_acc: 0.9370\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8511 - val_loss: 0.1571 - val_acc: 0.9499\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8504 - val_loss: 0.1672 - val_acc: 0.9465\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8501 - val_loss: 0.1886 - val_acc: 0.9370\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8517 - val_loss: 0.1783 - val_acc: 0.9422\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8507 - val_loss: 0.2273 - val_acc: 0.9194\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8487 - val_loss: 0.1935 - val_acc: 0.9349\n",
      "Epoch 00008: early stopping\n",
      "8460\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3202 - acc: 0.8531 - val_loss: 0.1890 - val_acc: 0.9353\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8501 - val_loss: 0.1893 - val_acc: 0.9358\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8495 - val_loss: 0.1985 - val_acc: 0.9333\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8498 - val_loss: 0.1738 - val_acc: 0.9449\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8494 - val_loss: 0.1646 - val_acc: 0.9480\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3205 - acc: 0.8522 - val_loss: 0.1817 - val_acc: 0.9406\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8509 - val_loss: 0.1722 - val_acc: 0.9448\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8491 - val_loss: 0.2089 - val_acc: 0.9288\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8501 - val_loss: 0.1714 - val_acc: 0.9443\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8528 - val_loss: 0.1836 - val_acc: 0.9399\n",
      "Epoch 00010: early stopping\n",
      "8470\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8516 - val_loss: 0.1846 - val_acc: 0.9373\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8497 - val_loss: 0.1681 - val_acc: 0.9451\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8523 - val_loss: 0.1828 - val_acc: 0.9393\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8530 - val_loss: 0.1610 - val_acc: 0.9490\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8523 - val_loss: 0.1588 - val_acc: 0.9506\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8498 - val_loss: 0.1716 - val_acc: 0.9452\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8499 - val_loss: 0.1786 - val_acc: 0.9440\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3253 - acc: 0.8509 - val_loss: 0.1676 - val_acc: 0.9476\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3210 - acc: 0.8520 - val_loss: 0.1700 - val_acc: 0.9438\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8516 - val_loss: 0.1831 - val_acc: 0.9419\n",
      "Epoch 00010: early stopping\n",
      "8480\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8518 - val_loss: 0.1902 - val_acc: 0.9360\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8499 - val_loss: 0.1656 - val_acc: 0.9495\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8530 - val_loss: 0.1735 - val_acc: 0.9451\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8496 - val_loss: 0.1874 - val_acc: 0.9383\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8527 - val_loss: 0.1651 - val_acc: 0.9473\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8492 - val_loss: 0.1554 - val_acc: 0.9524\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8503 - val_loss: 0.1873 - val_acc: 0.9364\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8514 - val_loss: 0.1992 - val_acc: 0.9316\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3189 - acc: 0.8517 - val_loss: 0.1631 - val_acc: 0.9485\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8494 - val_loss: 0.1631 - val_acc: 0.9480\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3209 - acc: 0.8533 - val_loss: 0.1647 - val_acc: 0.9457\n",
      "Epoch 00011: early stopping\n",
      "8490\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8490 - val_loss: 0.2065 - val_acc: 0.9309\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8515 - val_loss: 0.1803 - val_acc: 0.9436\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8511 - val_loss: 0.1685 - val_acc: 0.9434\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8516 - val_loss: 0.1799 - val_acc: 0.9397\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8499 - val_loss: 0.1564 - val_acc: 0.9509\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8494 - val_loss: 0.1686 - val_acc: 0.9478\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8518 - val_loss: 0.1936 - val_acc: 0.9336\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3206 - acc: 0.8517 - val_loss: 0.1589 - val_acc: 0.9494\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8495 - val_loss: 0.1755 - val_acc: 0.9436\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8505 - val_loss: 0.1975 - val_acc: 0.9325\n",
      "Epoch 00010: early stopping\n",
      "8500\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3259 - acc: 0.8493 - val_loss: 0.1718 - val_acc: 0.9440\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8492 - val_loss: 0.1666 - val_acc: 0.9463\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3179 - acc: 0.8533 - val_loss: 0.1744 - val_acc: 0.9453\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3208 - acc: 0.8519 - val_loss: 0.1987 - val_acc: 0.9316\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3251 - acc: 0.8509 - val_loss: 0.1741 - val_acc: 0.9453\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8516 - val_loss: 0.1722 - val_acc: 0.9454\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8508 - val_loss: 0.1623 - val_acc: 0.9488\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8518 - val_loss: 0.1612 - val_acc: 0.9501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8510 - val_loss: 0.1660 - val_acc: 0.9484\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8506 - val_loss: 0.1533 - val_acc: 0.9500\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8506 - val_loss: 0.1594 - val_acc: 0.9519\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8517 - val_loss: 0.1652 - val_acc: 0.9482\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8493 - val_loss: 0.1797 - val_acc: 0.9419\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8499 - val_loss: 0.1757 - val_acc: 0.9432\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8503 - val_loss: 0.1739 - val_acc: 0.9415\n",
      "Epoch 00015: early stopping\n",
      "8510\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8489 - val_loss: 0.1627 - val_acc: 0.9498\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8493 - val_loss: 0.1682 - val_acc: 0.9460\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8522 - val_loss: 0.1982 - val_acc: 0.9310\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8502 - val_loss: 0.1857 - val_acc: 0.9414\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8520 - val_loss: 0.2025 - val_acc: 0.9302\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8505 - val_loss: 0.1611 - val_acc: 0.9509\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8525 - val_loss: 0.1841 - val_acc: 0.9389\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8511 - val_loss: 0.1574 - val_acc: 0.9515\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8522 - val_loss: 0.1654 - val_acc: 0.9497\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8528 - val_loss: 0.1750 - val_acc: 0.9433\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8522 - val_loss: 0.1790 - val_acc: 0.9421\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8501 - val_loss: 0.1747 - val_acc: 0.9464\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8511 - val_loss: 0.1639 - val_acc: 0.9488\n",
      "Epoch 00013: early stopping\n",
      "8520\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8511 - val_loss: 0.1620 - val_acc: 0.9500\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8483 - val_loss: 0.1725 - val_acc: 0.9457\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8508 - val_loss: 0.1642 - val_acc: 0.9467\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3208 - acc: 0.8510 - val_loss: 0.1650 - val_acc: 0.9490\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8491 - val_loss: 0.1716 - val_acc: 0.9442\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8513 - val_loss: 0.1697 - val_acc: 0.9470\n",
      "Epoch 00006: early stopping\n",
      "8530\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8517 - val_loss: 0.1763 - val_acc: 0.9428\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3201 - acc: 0.8536 - val_loss: 0.1711 - val_acc: 0.9479\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3213 - acc: 0.8521 - val_loss: 0.1992 - val_acc: 0.9265\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8492 - val_loss: 0.1664 - val_acc: 0.9478\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8512 - val_loss: 0.1635 - val_acc: 0.9500\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3200 - acc: 0.8519 - val_loss: 0.1831 - val_acc: 0.9400\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8502 - val_loss: 0.1553 - val_acc: 0.9512\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8495 - val_loss: 0.1605 - val_acc: 0.9475\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8504 - val_loss: 0.1704 - val_acc: 0.9442\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8517 - val_loss: 0.2124 - val_acc: 0.9274\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8519 - val_loss: 0.1602 - val_acc: 0.9476\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3249 - acc: 0.8501 - val_loss: 0.1779 - val_acc: 0.9404\n",
      "Epoch 00012: early stopping\n",
      "8540\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8508 - val_loss: 0.1884 - val_acc: 0.9384\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3215 - acc: 0.8526 - val_loss: 0.1765 - val_acc: 0.9424\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8525 - val_loss: 0.1620 - val_acc: 0.9499\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8527 - val_loss: 0.2066 - val_acc: 0.9296\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8497 - val_loss: 0.1967 - val_acc: 0.9315\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8503 - val_loss: 0.1756 - val_acc: 0.9442\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8520 - val_loss: 0.1658 - val_acc: 0.9450\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8499 - val_loss: 0.1836 - val_acc: 0.9390\n",
      "Epoch 00008: early stopping\n",
      "8550\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3205 - acc: 0.8509 - val_loss: 0.1507 - val_acc: 0.9532\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8494 - val_loss: 0.1460 - val_acc: 0.9551\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8509 - val_loss: 0.1711 - val_acc: 0.9436\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8501 - val_loss: 0.1723 - val_acc: 0.9424\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8503 - val_loss: 0.1861 - val_acc: 0.9372\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8478 - val_loss: 0.1640 - val_acc: 0.9464\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8509 - val_loss: 0.1507 - val_acc: 0.9542\n",
      "Epoch 00007: early stopping\n",
      "8560\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8517 - val_loss: 0.1862 - val_acc: 0.9355\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8513 - val_loss: 0.1594 - val_acc: 0.9478\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8503 - val_loss: 0.1776 - val_acc: 0.9428\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3188 - acc: 0.8516 - val_loss: 0.1570 - val_acc: 0.9516\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8505 - val_loss: 0.1703 - val_acc: 0.9459\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8513 - val_loss: 0.1726 - val_acc: 0.9429\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8515 - val_loss: 0.1672 - val_acc: 0.9452\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8512 - val_loss: 0.1911 - val_acc: 0.9378\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3284 - acc: 0.8479 - val_loss: 0.1721 - val_acc: 0.9429\n",
      "Epoch 00009: early stopping\n",
      "8570\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3215 - acc: 0.8511 - val_loss: 0.1993 - val_acc: 0.9316\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8510 - val_loss: 0.1913 - val_acc: 0.9360\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8508 - val_loss: 0.1609 - val_acc: 0.9486\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3207 - acc: 0.8517 - val_loss: 0.2157 - val_acc: 0.9230\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8510 - val_loss: 0.1765 - val_acc: 0.9417\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8500 - val_loss: 0.1831 - val_acc: 0.9375\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3209 - acc: 0.8522 - val_loss: 0.1878 - val_acc: 0.9365\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8513 - val_loss: 0.1944 - val_acc: 0.9330\n",
      "Epoch 00008: early stopping\n",
      "8580\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8507 - val_loss: 0.1703 - val_acc: 0.9478\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8504 - val_loss: 0.2157 - val_acc: 0.9244\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8535 - val_loss: 0.2061 - val_acc: 0.9297\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8510 - val_loss: 0.1571 - val_acc: 0.9530\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8512 - val_loss: 0.1562 - val_acc: 0.9491\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3258 - acc: 0.8508 - val_loss: 0.1606 - val_acc: 0.9496\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8498 - val_loss: 0.1700 - val_acc: 0.9431\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8490 - val_loss: 0.2444 - val_acc: 0.9123\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8511 - val_loss: 0.1489 - val_acc: 0.9572\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8487 - val_loss: 0.2071 - val_acc: 0.9272\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8499 - val_loss: 0.1509 - val_acc: 0.9556\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8508 - val_loss: 0.1591 - val_acc: 0.9476\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8511 - val_loss: 0.1552 - val_acc: 0.9524\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8504 - val_loss: 0.2128 - val_acc: 0.9257\n",
      "Epoch 00014: early stopping\n",
      "8590\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8509 - val_loss: 0.1872 - val_acc: 0.9381\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8504 - val_loss: 0.1705 - val_acc: 0.9422\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8515 - val_loss: 0.1922 - val_acc: 0.9305\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8493 - val_loss: 0.1646 - val_acc: 0.9434\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8499 - val_loss: 0.1648 - val_acc: 0.9466\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3260 - acc: 0.8482 - val_loss: 0.1556 - val_acc: 0.9536\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8510 - val_loss: 0.1565 - val_acc: 0.9478\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8501 - val_loss: 0.1719 - val_acc: 0.9400\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8511 - val_loss: 0.1583 - val_acc: 0.9507\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3209 - acc: 0.8520 - val_loss: 0.1983 - val_acc: 0.9272\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8511 - val_loss: 0.1523 - val_acc: 0.9568\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8494 - val_loss: 0.1494 - val_acc: 0.9554\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8502 - val_loss: 0.1691 - val_acc: 0.9445\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3205 - acc: 0.8513 - val_loss: 0.1646 - val_acc: 0.9465\n",
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3262 - acc: 0.8485 - val_loss: 0.1664 - val_acc: 0.9462\n",
      "Epoch 16/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8515 - val_loss: 0.1618 - val_acc: 0.9477\n",
      "Epoch 17/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8510 - val_loss: 0.1714 - val_acc: 0.9406\n",
      "Epoch 00017: early stopping\n",
      "8600\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3188 - acc: 0.8514 - val_loss: 0.1456 - val_acc: 0.9538\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8519 - val_loss: 0.2323 - val_acc: 0.9148\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8506 - val_loss: 0.1665 - val_acc: 0.9460\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8490 - val_loss: 0.1665 - val_acc: 0.9454\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3213 - acc: 0.8513 - val_loss: 0.1540 - val_acc: 0.9497\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8513 - val_loss: 0.1483 - val_acc: 0.9561\n",
      "Epoch 00006: early stopping\n",
      "8610\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3180 - acc: 0.8536 - val_loss: 0.1599 - val_acc: 0.9481\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8507 - val_loss: 0.1548 - val_acc: 0.9511\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8506 - val_loss: 0.1665 - val_acc: 0.9462\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8511 - val_loss: 0.1476 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3261 - acc: 0.8488 - val_loss: 0.1714 - val_acc: 0.9461\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8499 - val_loss: 0.1772 - val_acc: 0.9389\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3255 - acc: 0.8503 - val_loss: 0.1594 - val_acc: 0.9527\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8516 - val_loss: 0.1766 - val_acc: 0.9402\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8507 - val_loss: 0.1611 - val_acc: 0.9545\n",
      "Epoch 00009: early stopping\n",
      "8620\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8504 - val_loss: 0.2031 - val_acc: 0.9282\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8507 - val_loss: 0.2429 - val_acc: 0.9131\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8504 - val_loss: 0.1635 - val_acc: 0.9454\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8493 - val_loss: 0.1649 - val_acc: 0.9478\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3199 - acc: 0.8516 - val_loss: 0.1535 - val_acc: 0.9518\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3254 - acc: 0.8490 - val_loss: 0.1773 - val_acc: 0.9407\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3207 - acc: 0.8521 - val_loss: 0.1607 - val_acc: 0.9492\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3241 - acc: 0.8498 - val_loss: 0.1537 - val_acc: 0.9529\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8498 - val_loss: 0.1750 - val_acc: 0.9414\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3205 - acc: 0.8511 - val_loss: 0.2270 - val_acc: 0.9211\n",
      "Epoch 00010: early stopping\n",
      "8630\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8506 - val_loss: 0.1680 - val_acc: 0.9473\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8495 - val_loss: 0.1782 - val_acc: 0.9411\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3200 - acc: 0.8521 - val_loss: 0.2392 - val_acc: 0.9120\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8505 - val_loss: 0.1538 - val_acc: 0.9508\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3208 - acc: 0.8524 - val_loss: 0.1909 - val_acc: 0.9350\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8506 - val_loss: 0.1876 - val_acc: 0.9407\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8507 - val_loss: 0.2363 - val_acc: 0.9134\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8535 - val_loss: 0.2140 - val_acc: 0.9291\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3243 - acc: 0.8517 - val_loss: 0.2438 - val_acc: 0.9150\n",
      "Epoch 00009: early stopping\n",
      "8640\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8521 - val_loss: 0.2213 - val_acc: 0.9253\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8522 - val_loss: 0.1895 - val_acc: 0.9346\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8510 - val_loss: 0.1869 - val_acc: 0.9339\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3277 - acc: 0.8486 - val_loss: 0.2579 - val_acc: 0.9091\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8512 - val_loss: 0.1668 - val_acc: 0.9474\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8497 - val_loss: 0.1916 - val_acc: 0.9371\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8503 - val_loss: 0.1794 - val_acc: 0.9432\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3215 - acc: 0.8515 - val_loss: 0.2289 - val_acc: 0.9200\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3290 - acc: 0.8488 - val_loss: 0.1888 - val_acc: 0.9367\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3214 - acc: 0.8505 - val_loss: 0.1808 - val_acc: 0.9376\n",
      "Epoch 00010: early stopping\n",
      "8650\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8516 - val_loss: 0.2179 - val_acc: 0.9233\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3230 - acc: 0.8511 - val_loss: 0.2219 - val_acc: 0.9230\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8509 - val_loss: 0.1727 - val_acc: 0.9471\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8512 - val_loss: 0.2201 - val_acc: 0.9241\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3200 - acc: 0.8520 - val_loss: 0.2016 - val_acc: 0.9323\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8511 - val_loss: 0.2015 - val_acc: 0.9326\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3238 - acc: 0.8509 - val_loss: 0.1900 - val_acc: 0.9399\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8510 - val_loss: 0.2030 - val_acc: 0.9317\n",
      "Epoch 00008: early stopping\n",
      "8660\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8484 - val_loss: 0.1783 - val_acc: 0.9420\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3212 - acc: 0.8514 - val_loss: 0.1826 - val_acc: 0.9404\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8487 - val_loss: 0.2165 - val_acc: 0.9237\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8510 - val_loss: 0.1780 - val_acc: 0.9438\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8509 - val_loss: 0.1744 - val_acc: 0.9442\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3242 - acc: 0.8491 - val_loss: 0.2229 - val_acc: 0.9208\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3203 - acc: 0.8528 - val_loss: 0.1964 - val_acc: 0.9319\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8513 - val_loss: 0.2065 - val_acc: 0.9263\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3204 - acc: 0.8516 - val_loss: 0.1866 - val_acc: 0.9375\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3192 - acc: 0.8517 - val_loss: 0.1588 - val_acc: 0.9526\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8501 - val_loss: 0.1667 - val_acc: 0.9449\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8498 - val_loss: 0.1883 - val_acc: 0.9363\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8497 - val_loss: 0.1739 - val_acc: 0.9443\n",
      "Epoch 14/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3246 - acc: 0.8507 - val_loss: 0.1806 - val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3247 - acc: 0.8509 - val_loss: 0.2044 - val_acc: 0.9312\n",
      "Epoch 00015: early stopping\n",
      "8670\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8510 - val_loss: 0.1910 - val_acc: 0.9331\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3208 - acc: 0.8515 - val_loss: 0.1892 - val_acc: 0.9387\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3206 - acc: 0.8508 - val_loss: 0.1581 - val_acc: 0.9517\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3266 - acc: 0.8490 - val_loss: 0.1909 - val_acc: 0.9400\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3207 - acc: 0.8528 - val_loss: 0.2173 - val_acc: 0.9232\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8489 - val_loss: 0.1749 - val_acc: 0.9437\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8503 - val_loss: 0.1621 - val_acc: 0.9493\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3237 - acc: 0.8486 - val_loss: 0.2006 - val_acc: 0.9295\n",
      "Epoch 00008: early stopping\n",
      "8680\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3207 - acc: 0.8517 - val_loss: 0.1660 - val_acc: 0.9463\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8491 - val_loss: 0.2041 - val_acc: 0.9303\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8518 - val_loss: 0.1786 - val_acc: 0.9397\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8521 - val_loss: 0.1813 - val_acc: 0.9407\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3215 - acc: 0.8513 - val_loss: 0.1732 - val_acc: 0.9439\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8506 - val_loss: 0.1731 - val_acc: 0.9463\n",
      "Epoch 00006: early stopping\n",
      "8690\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8513 - val_loss: 0.1687 - val_acc: 0.9482\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8520 - val_loss: 0.2187 - val_acc: 0.9256\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3244 - acc: 0.8511 - val_loss: 0.1890 - val_acc: 0.9396\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8510 - val_loss: 0.1851 - val_acc: 0.9401\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3216 - acc: 0.8504 - val_loss: 0.1925 - val_acc: 0.9336\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3203 - acc: 0.8513 - val_loss: 0.1872 - val_acc: 0.9390\n",
      "Epoch 00006: early stopping\n",
      "8700\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8508 - val_loss: 0.1985 - val_acc: 0.9326\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8490 - val_loss: 0.1653 - val_acc: 0.9472\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8498 - val_loss: 0.1965 - val_acc: 0.9326\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3229 - acc: 0.8512 - val_loss: 0.1923 - val_acc: 0.9377\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8518 - val_loss: 0.1680 - val_acc: 0.9453\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8518 - val_loss: 0.2059 - val_acc: 0.9260\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8484 - val_loss: 0.1847 - val_acc: 0.9391\n",
      "Epoch 00007: early stopping\n",
      "8710\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3199 - acc: 0.8504 - val_loss: 0.1861 - val_acc: 0.9341\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3265 - acc: 0.8487 - val_loss: 0.1676 - val_acc: 0.9489\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3252 - acc: 0.8498 - val_loss: 0.1662 - val_acc: 0.9442\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3256 - acc: 0.8485 - val_loss: 0.1752 - val_acc: 0.9421\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8490 - val_loss: 0.1829 - val_acc: 0.9369\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3234 - acc: 0.8496 - val_loss: 0.1822 - val_acc: 0.9374\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8512 - val_loss: 0.2166 - val_acc: 0.9227\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3220 - acc: 0.8521 - val_loss: 0.1933 - val_acc: 0.9304\n",
      "Epoch 00008: early stopping\n",
      "0.9367369589345172 8710\n",
      "8720\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3213 - acc: 0.8509 - val_loss: 0.1907 - val_acc: 0.9344\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8510 - val_loss: 0.1976 - val_acc: 0.9370\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8502 - val_loss: 0.1974 - val_acc: 0.9297\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8513 - val_loss: 0.1971 - val_acc: 0.9293\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3236 - acc: 0.8513 - val_loss: 0.1983 - val_acc: 0.9331\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3180 - acc: 0.8523 - val_loss: 0.1670 - val_acc: 0.9467\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8517 - val_loss: 0.2142 - val_acc: 0.9220\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3206 - acc: 0.8514 - val_loss: 0.1621 - val_acc: 0.9483\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8502 - val_loss: 0.1879 - val_acc: 0.9390\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3189 - acc: 0.8521 - val_loss: 0.2106 - val_acc: 0.9243\n",
      "Epoch 11/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3219 - acc: 0.8508 - val_loss: 0.1861 - val_acc: 0.9353\n",
      "Epoch 12/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8506 - val_loss: 0.1841 - val_acc: 0.9397\n",
      "Epoch 13/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8505 - val_loss: 0.1660 - val_acc: 0.9494\n",
      "Epoch 00013: early stopping\n",
      "8730\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8512 - val_loss: 0.1972 - val_acc: 0.9337\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3250 - acc: 0.8481 - val_loss: 0.1808 - val_acc: 0.9394\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3245 - acc: 0.8502 - val_loss: 0.1852 - val_acc: 0.9402\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8508 - val_loss: 0.1901 - val_acc: 0.9376\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3228 - acc: 0.8512 - val_loss: 0.1578 - val_acc: 0.9522\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3205 - acc: 0.8514 - val_loss: 0.1801 - val_acc: 0.9396\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8512 - val_loss: 0.1680 - val_acc: 0.9468\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3203 - acc: 0.8523 - val_loss: 0.1590 - val_acc: 0.9509\n",
      "Epoch 9/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8503 - val_loss: 0.1753 - val_acc: 0.9439\n",
      "Epoch 10/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3232 - acc: 0.8513 - val_loss: 0.1879 - val_acc: 0.9379\n",
      "Epoch 00010: early stopping\n",
      "8740\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8534 - val_loss: 0.1639 - val_acc: 0.9488\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3227 - acc: 0.8525 - val_loss: 0.1782 - val_acc: 0.9415\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3231 - acc: 0.8509 - val_loss: 0.1869 - val_acc: 0.9367\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8489 - val_loss: 0.2000 - val_acc: 0.9321\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8512 - val_loss: 0.1653 - val_acc: 0.9474\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3189 - acc: 0.8518 - val_loss: 0.1680 - val_acc: 0.9477\n",
      "Epoch 00006: early stopping\n",
      "8750\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3213 - acc: 0.8498 - val_loss: 0.1699 - val_acc: 0.9442\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3194 - acc: 0.8520 - val_loss: 0.2257 - val_acc: 0.9213\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3223 - acc: 0.8515 - val_loss: 0.1888 - val_acc: 0.9362\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8501 - val_loss: 0.2095 - val_acc: 0.9237\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8500 - val_loss: 0.2179 - val_acc: 0.9215\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3186 - acc: 0.8525 - val_loss: 0.1928 - val_acc: 0.9320\n",
      "Epoch 00006: early stopping\n",
      "8760\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3202 - acc: 0.8507 - val_loss: 0.1828 - val_acc: 0.9381\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3209 - acc: 0.8538 - val_loss: 0.1723 - val_acc: 0.9422\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3187 - acc: 0.8531 - val_loss: 0.1735 - val_acc: 0.9403\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3222 - acc: 0.8498 - val_loss: 0.1812 - val_acc: 0.9381\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8498 - val_loss: 0.1849 - val_acc: 0.9350\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8509 - val_loss: 0.1872 - val_acc: 0.9358\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3218 - acc: 0.8497 - val_loss: 0.1741 - val_acc: 0.9442\n",
      "Epoch 00007: early stopping\n",
      "8770\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3208 - acc: 0.8504 - val_loss: 0.1627 - val_acc: 0.9488\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3217 - acc: 0.8506 - val_loss: 0.1978 - val_acc: 0.9286\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3240 - acc: 0.8502 - val_loss: 0.2059 - val_acc: 0.9243\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3189 - acc: 0.8499 - val_loss: 0.1812 - val_acc: 0.9407\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3204 - acc: 0.8513 - val_loss: 0.1677 - val_acc: 0.9467\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8506 - val_loss: 0.1760 - val_acc: 0.9450\n",
      "Epoch 00006: early stopping\n",
      "8780\n",
      "Train on 121146 samples, validate on 10177 samples\n",
      "Epoch 1/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3190 - acc: 0.8518 - val_loss: 0.1840 - val_acc: 0.9344\n",
      "Epoch 2/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3225 - acc: 0.8514 - val_loss: 0.1821 - val_acc: 0.9352\n",
      "Epoch 3/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3224 - acc: 0.8501 - val_loss: 0.1763 - val_acc: 0.9408\n",
      "Epoch 4/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3214 - acc: 0.8513 - val_loss: 0.1829 - val_acc: 0.9389\n",
      "Epoch 5/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3226 - acc: 0.8517 - val_loss: 0.2025 - val_acc: 0.9289\n",
      "Epoch 6/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3204 - acc: 0.8509 - val_loss: 0.1855 - val_acc: 0.9375\n",
      "Epoch 7/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3221 - acc: 0.8508 - val_loss: 0.1853 - val_acc: 0.9398\n",
      "Epoch 8/100\n",
      "121146/121146 [==============================] - 0s 3us/step - loss: 0.3211 - acc: 0.8515 - val_loss: 0.2015 - val_acc: 0.9261\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Efetua o treinamento até atingir uma precisão no recall acima de 95%\n",
    "Maxrecall = 0\n",
    "Maxprecision = 0\n",
    "for i in range(5980,8790,10):\n",
    "    batch_size = i\n",
    "    print(i)\n",
    "    metrics = Metrics()\n",
    "    history = model2.fit(X2_train, y2_train, epochs = 100,  batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)\n",
    "    if recall_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxrecall and precision_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxprecision:\n",
    "        print(recall_score(y_test,np.round(model2.predict(X_test)).T[0]), i)\n",
    "        Maxrecall = recall_score(y_test,np.round(model2.predict(X_test)).T[0])\n",
    "        Maxprecision = precision_score(y_test,np.round(model2.predict(X_test)).T[0])\n",
    "        if Maxrecall > 0.95:\n",
    "            break\n",
    "#    metrics.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VeW5/vHvszMCYZJ5EMFqVUAEjGhFERU9OGFrraLSVm3ltNVj1Q5aa2u19RyPVn/aU1vr2FoVVJQee5RatThWlCCIAlpwJMyiEKaEDM/vj3cl2QlJVoAsdgL357r2xV5rvWvtJ5tk3/t912TujoiISFNSmS5ARERaP4WFiIjEUliIiEgshYWIiMRSWIiISCyFhYiIxFJYSOLM7I9m9qtmtv3IzMYlWMt5Zvb3pLafJDP7hZk9GD0fYGYbzSwrru0OvtYCMxu7o+s3sd0XzOzbLb1dSV52pgsQaS4z+yNQ7O7X7Og23P0h4KEWKypD3P0ToKAlttXQ++ruQ1pi27L7UM9Cdhtmpi8/IglRWAhQM/zzIzObb2abzOxeM+tlZjPMbIOZPWdmXdPaT4iGKtZFQwsHpS0bYWZvRus9AuTXe61TzWxetO4/zWxYM+qbDJwH/DgafvlrWt1Xmtl8YJOZZZvZVWb2fvT6C83sK2nbOd/MXkmbdjP7jpktNrPPzewOM7MGXr+vmW0xs73q/ZyfmlmOme1nZi+a2fpo3iON/Bx/M7NL6s17y8zOiJ7fbmZLzazEzOaY2dGNbGdgVHt2ND0oev0NZvYs0L1e+8fMbGVU30tmNqQZ7+u46Hmemd1mZsujx21mlhctG2tmxWb2AzNbbWYrzOyChv8Xt/kZUmZ2jZl9HK37gJl1jpblm9mDZrY2+j2ZbWa9omXnm9kH0c/6oZmd15zXk53k7nroAfARMAvoBfQDVgNvAiOAPOAfwLVR2y8Cm4ATgBzgx8ASIDd6fAxcHi07EygHfhWtOzLa9uFAFvDN6LXz0uoY10iNf6zeTr265wF7A+2ieV8D+hK+DJ0d1donWnY+8Era+g78H9AFGACsAcY38vr/AC5Km74ZuDN6PgX4afSa+cBRjWzjG8CradODgXVpP/8koBthiPgHwEogP1r2C+DB6PnAqPbsaPo14Nbo/2oMsKG6bbT8QqBjtPw2YF4z3tdx0fPro9+NnkAP4J/AL6NlY4GKqE0OcDKwGejayM//AvDttJqWAPsShtSeAP4cLft34K9A++j35FCgE9ABKAEOiNr1AYZk+u9nT3ioZyHp/sfdV7n7MuBl4HV3n+vuZcB0QnBA+AB+yt2fdfdy4NdAO+BI4AjCh8Zt7l7u7tOA2WmvcRHwB3d/3d0r3f1PQFm03o76jbsvdfctAO7+mLsvd/cqd38EWAyMamL9G919nYf9ADOB4Y20exg4ByDqfUyM5kEIxH2Avu5e6u6vNLwJpgPDzWyfaPo84InoPcbdH3T3te5e4e63ED7cD2jqhzezAcBhwM/cvczdXyJ80NZw9/vcfUP0Or8ADqn+Ft8M5wHXu/tqd18DXAd8PW15ebS83N2fBjbG1Zy23Vvd/QN33wj8BJgY9ZbKCaG5X/R7MsfdS6L1qoChZtbO3Ve4+4Jm/hyyExQWkm5V2vMtDUxX71DtS+g9AODuVcBSQo+kL7DM3dOvUPlx2vN9gB9EQwvrzGwdoVfQdyfqXpo+YWbfSBvmWgcMpd6wTD0r055vpvEdx9OAL5lZX8K3dyeEKoTelQFvRMNzFza0AXffADxFCBqif2t2uEfDOYui4aJ1QOeY2iG8d5+7+6a0eTXvuZllmdmN0dBcCaHXQDO2m7799P/Dj6n7/7XW3SvSppt6D+O2m03o3f4ZeAaYGg193WRmOdHPeDbwHWCFmT1lZgc28+eQnaCwkB2xnPChD9R8y94bWAasAPrVG/cfkPZ8KXCDu3dJe7R39ynNeN3GLpFcMz/6xn43cAnQzd27AO8QPsh3iruvA/4OnAWcC0ypDkV3X+nuF7l7X8IQyu/MbL9GNjUFOMfMvkTokc2Maj8auDLafteo9vXNqH0F0NXMOqTNS3/PzwVOB8YRwmdgNL96u3GXnq7z/x1te3nMOs3R0HYrgFVRL+U6dx9M6LGeShjCw92fcfcTCENQ7xL+vyVhCgvZEY8Cp5jZ8WaWQxhbLyOMZb9G+IO/NNrZfAZ1h4DuBr5jZodb0MHMTjGzjs143VWE8e2mdCB8+K0BiHa2Dt2eHy7Gw4QPra9SOwSFmX3NzPpHk59HNVQ2so2nCR+S1wOPRD0zCPsUKqLas83s54Rx+ia5+8dAEXCdmeWa2VHAaWlNOhL+f9YS9gH8Z71NxL2vU4BrzKyHmXUHfg7s8Dkc9bZ7ebRzviCq6xF3rzCzY83sYAvnkZQQhqUqLRx0MSEKxjLCkFdj77O0IIWFbDd3f4+wI/Z/gE8JH0ynuftWd98KnEHYkfw5YcjgibR1iwj7LX4bLV8StW2Oe4HB0fDSXxqpbSFwCyG0VgEHA69u30/YpCeB/Qnfft9Km38Y8LqZbYzafN/dP2ykxjLCezKOtMAhDLvMAP5FGJIppd4QWxPOJRw08BlwLfBA2rIHou0tAxYSdlani3tff0UIo/nA24QDH5p1kmWM+wjDTS8BHxJ+3v+IlvUmDPuVAIuAFwkBlSJ8OVlO+FmPAb7XArVIDKs7tCwiIrIt9SxERCSWwkJERGIpLEREJJbCQkREYu02F17r3r27Dxw4MNNliIi0KXPmzPnU3XvEtdttwmLgwIEUFRVlugwRkTbFzD6Ob6VhKBERaQaFhYiIxFJYiIhIrN1mn4WI7F7Ky8spLi6mtLQ006XsFvLz8+nfvz85OTk7tL7CQkRapeLiYjp27MjAgQOxbW9eKNvB3Vm7di3FxcUMGjRoh7ahYSgRaZVKS0vp1q2bgqIFmBndunXbqV6awkJEWi0FRcvZ2fdSw1BtgLuzoayCVetLWbG+lJLScgAMw6z2Djbhd8Go/p0wwi+I1Swjal+7UoNt6m0Xa+i1aqctrWH9NimDnKwU+TlZ5GWnwiN6np0yfRhEqqqcsooqtpRXhsfWSkobeF4aTW8pr6KsopIOudl0bpdDp3Y5dG6XQ5f24d/O7XJon5ul91dajMIiw9ydzzZtZcX6UlauL2VlSfh3xfpSVpZsCfPWl7Jp6+53f5eUQV52Fnk5KfKjf0OgRMGSk/Y8Oy1w0oMnbb3aQGpo/Szyc2rb52alSKXiP0jdow/xrZV1PsjLKirZsrX2w7203vLS8rQP+/KqRj/8qwOgtLwqtpbtlZNldMrPoXNagFQ/uqQFTM289rk1z/NzUm06aNydyiqnyp0qp/Z5lVPpRP82Mq8qrLOhZB1PTX+MSRdMxgxS0Rek6n/NjBS1X5xS0bxzvno6d9/3AF26dqnbNvrSVd0ufZ3qL22tWaJhYWbjgduBLOAed7+x3vLvABcT7nS1EZjs7gvN7ATgRiAX2Ar8yN3/kWStSaiorGLNxjJWrC+t6RWsLCmtnS7Zwqr1ZWytrPtBkZUyenbMo3fnfA7o3ZExX+xBn8759O7cjt6d8unSPhzNUH0rEsdxD9Me3SGzZlm9eU74QwrrVbfztG1F7eq1cbzm5pvp8+qu57X354yWV1VBeWUVZRXhm3BZRRVl5bXPS8u3nVfTtryKki0VtW3S1i+tqGRnb8WSm5WqGyo5YVS2rLyq5kN9S/mOhXRedop2uVm0ywmP/Jysmumu7XNpl5tFflqb9OXtcrLIr5mfanB5u9wscrJSbNpawfrN5azfUk7JlnLWbQnP6zyi5Ws3buWDNZtC29LyJt+/3KwUner1VBp9tA/hU93Dyc/J2qH3rKyikk1llWwsrWBjWQVbK6oo2VIefeA7lVWkfbiH362qtFBIn1fVzF8OMyPLjFQqhECWGdlZKVIGazZu4OE/3sOkCydHgVNFlUNFZQWpVBZVHn7/67/WLfdOpcSh5LPN2/Xz1wTLNoFS22OvH1jV/+ZmpehWkLddr7e9EguL6HaIdwAnAMXAbDN7MrqTWbWH3f3OqP0E4FZgPNHd19x9uZkNJdxBrF9Ste6I0vJKVpXU9gZqegbrS1lRUsrK9VtYs6GMqnq/s7nZqfDB3ymfQwd0pVfnfPp0ioKgcz59OufTvSCPrGZ8692TuTsV0dBNbeCkhU153eApLa8NoKaCy6HOB3J+9fOcVM10ftryhj7I87Kb12tpCZ3yc+iUn8Pe27leVZWzobRim2BZt2VrzfOS6nmby1lVUsq/Vm1g/ZZyNpRWNLntvOzUNkNindrlkJ2yEAZlFWwqC4GQ/ry8su4fy90T+pBau2mb7afMwgd79AGfShk5qRR5qfAhm5Wybdo0NC8VzWvMlRdfzycffchXTziKnJwcCgoK6NOnD/PmzWPhwoV8+ctfZunSpZSWlvIfl17KRRdNxt3Z7wv78uprr7Nx40ZOP+1UvnTkkcya9Rp9+vbj4UemkdeuXc0Xsqr0f6kNn/R/HWp6SBVVVbXrEn1Bc8jPyWq7YUG47/ISd/8AwMymEm4aXxMW7l6S1r763sm4+9y0+QuAfDPLi25HmbgNpeGPY8X69F5A7fDQqpJSPtu0dZv1OuZl07tzPr075/PFnmm9gc559O7Ujj6dQ6+gtXc32wIzIyfLyMlKUZCn0dTtlUpZGJ5qv/3H3FdWeU2Q1A2a2oCp7s2s27KVZetKWbRiAxVVVRTkZVOQl02HvGwGdGhf87xDXjYd87PpkJtV87x71Vq+0KOArJTxX08vYtHKDbTkX87gvp249rQhjS6/8cYbeeedd5g3bx4vvPACp5xyCu+8807Noaf33Xcfe+21F1u2bOGwww7ja2eeSbdu3QDIy8miPCeLJUsWM3XqFO6/717OOussnpvxVyZNmtSCP0WwK+54muRfWT/q3j+4mHCP4DrM7GLgCsKQ03ENbOerwNyGgsLMJgOTAQYMGLBDRX6+aSs3PL2ozv6CjWXbfnPq1iGX3p3z6dcln5EDutQZFqoOCH1oyZ4gK2V07ZBL1w65ib7OokXr6BD9TaVS1qJBsSNGjRpV5xyF3/zmN0yfPh2ApUuXsnjx4pqwqDZo0CCGDx8OwKGHHspHH32USG274gtokp9uDVW/Tfy5+x3AHWZ2LnAN8M2aDZgNAf4bOLGhF3D3u4C7AAoLC3coWnOzU7yy+FP6dMln/54FHL1/9zpB0KdzPj075ZGXvWPjsCKy85rqAewqHTp0qHn+wgsv8Nxzz/Haa6/Rvn17xo4d2+A5DHl5tUNDWVlZbNmyZZfUmoQkw6IY6gyl9geWN9F+KvD76gkz6w9MB77h7u8nUiHQIS+bWVcfn9TmRaSN6tixIxs2bGhw2fr16+natSvt27fn3XffZdasWbu4ul0vybCYDexvZoOAZcBE4Nz0Bma2v7svjiZPARZH87sATwE/cfdXE6xRRKRB3bp1Y/To0QwdOpR27drRq1evmmXjx4/nzjvvZNiwYRxwwAEcccQRGax017Akd4yY2cnAbYRDZ+9z9xvM7HqgyN2fNLPbgXFAOfA5cIm7LzCza4CfEIVH5ER3X93YaxUWFrpufiSy+1i0aBEHHXRQpsvYrTT0nprZHHcvjFs30T2y7v408HS9eT9Pe/79Rtb7FfCrJGsTEZHm07WhREQklsJCRERiKSxERCSWwkJERGIpLEREJJbCQkSkBRQUFACwfPlyzjzzzAbbjB07lrhD/G+77TY2b669Yu3JJ5/MunXrWq7QHaSwEBFpQX379mXatGk7vH79sHj66afp0qVLS5S2UxQWIiINuPLKK/nd735XM/2LX/yC6667juOPP56RI0dy8MEH87//+7/brPfRRx8xdOhQALZs2cLEiRMZNmwYZ599dp1rQ333u9+lsLCQIUOGcO211wLh4oTLly/n2GOP5dhjjwVg4MCBfPrppwDceuutDB06lKFDh3LbbbfVvN5BBx3ERRddxJAhQzjxxBMTuQaVLpMqIq3fjKtg5dstu83eB8NJNza6eOLEiVx22WV873vfA+DRRx/lb3/7G5dffjmdOnXi008/5YgjjmDChAmNXvX197//Pe3bt2f+/PnMnz+fkSNH1iy74YYb2GuvvaisrOT4449n/vz5XHrppdx6663MnDmT7t2719nWnDlzuP/++3n99ddxdw4//HCOOeYYunbtyuLFi5kyZQp33303Z511Fo8//niLXwpdPQsRkQaMGDGC1atXs3z5ct566y26du1Knz59uPrqqxk2bBjjxo1j2bJlrFq1qtFtvPTSSzUf2sOGDWPYsGE1yx599FFGjhzJiBEjWLBgAQsXLmxsMwC88sorfOUrX6FDhw4UFBRwxhln8PLLLwO75lLo6lmISOvXRA8gSWeeeSbTpk1j5cqVTJw4kYceeog1a9YwZ84ccnJyGDhwYIOXJk/XUK/jww8/5Ne//jWzZ8+ma9eunH/++bHbaeo6frviUujqWYiINGLixIlMnTqVadOmceaZZ7J+/Xp69uxJTk4OM2fO5OOPP25y/TFjxvDQQw8B8M477zB//nwASkpK6NChA507d2bVqlXMmDGjZp3GLo0+ZswY/vKXv7B582Y2bdrE9OnTOfroo1vwp22aehYiIo0YMmQIGzZsoF+/fvTp04fzzjuP0047jcLCQoYPH86BBx7Y5Prf/e53ueCCCxg2bBjDhw9n1KhRABxyyCGMGDGCIUOGsO+++zJ69OiadSZPnsxJJ51Enz59mDlzZs38kSNHcv7559ds49vf/jYjRoxI7O579SV6ifJdSZcoF9m96BLlLW9nLlGuYSgREYmlsBARkVgKCxFptXaXYfLWYGffS4WFiLRK+fn5rF27VoHRAtydtWvXkp+fv8PbSPRoKDMbD9xOuAf3Pe5+Y73l3wEuBiqBjcBkd18YLfsJ8K1o2aXu/kyStYpI69K/f3+Ki4tZs2ZNpkvZLeTn59O/f/8dXj+xsDCzLOAO4ASgGJhtZk9Wh0HkYXe/M2o/AbgVGG9mg4GJwBCgL/CcmX3R3SuTqldEWpecnBwGDRqU6TIkkuQw1Chgibt/4O5bganA6ekN3L0kbbIDUN3fPB2Y6u5l7v4hsCTanoiIZECSw1D9gKVp08XA4fUbmdnFwBVALnBc2rqz6q3br4F1JwOTAQYMGNAiRYuIyLaS7Fk0dBnGbfZUufsd7v4F4Ergmu1c9y53L3T3wh49euxUsSIi0rgkw6IY2Dttuj+wvIn2U4Ev7+C6IiKSoCTDYjawv5kNMrNcwg7rJ9MbmNn+aZOnAIuj508CE80sz8wGAfsDbyRYq4iINCGxfRbuXmFmlwDPEA6dvc/dF5jZ9UCRuz8JXGJm44By4HPgm9G6C8zsUWAhUAFcrCOhREQyRxcSFBHZg+lCgiIi0mIUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhIr0bAws/Fm9p6ZLTGzqxpYfoWZLTSz+Wb2vJntk7bsJjNbYGaLzOw3ZmZJ1ioiIo1LLCzMLAu4AzgJGAycY2aD6zWbCxS6+zBgGnBTtO6RwGhgGDAUOAw4JqlaRUSkaUn2LEYBS9z9A3ffCkwFTk9v4O4z3X1zNDkL6F+9CMgHcoE8IAdYlWCtIiLShCTDoh+wNG26OJrXmG8BMwDc/TVgJrAiejzj7ovqr2Bmk82syMyK1qxZ02KFi4hIXUmGRUP7GLzBhmaTgELg5mh6P+AgQk+jH3CcmY3ZZmPud7l7obsX9ujRo8UKFxGRupIMi2Jg77Tp/sDy+o3MbBzwU2CCu5dFs78CzHL3je6+kdDjOCLBWkVEpAlJhsVsYH8zG2RmucBE4Mn0BmY2AvgDIShWpy36BDjGzLLNLIewc3ubYSgREdk1EgsLd68ALgGeIXzQP+ruC8zsejObEDW7GSgAHjOzeWZWHSbTgPeBt4G3gLfc/a9J1SoiIk0z9wZ3I7Q5hYWFXlRUlOkyRETaFDOb4+6Fce10BreIiMRSWIiISCyFhYiIxFJYiIhILIWFiIjEUliIiEgshYWIiMRSWIiISCyFhYiIxFJYiIhILIWFiIjEUliIiEgshYWIiMRSWIiISCyFhYiIxFJYiIhILIWFiIjEUliIiEisRMPCzMab2XtmtsTMrmpg+RVmttDM5pvZ82a2T9qyAWb2dzNbFLUZmGStIiLSuMTCwsyygDuAk4DBwDlmNrhes7lAobsPA6YBN6UtewC42d0PAkYBq5OqVUREmpZkz2IUsMTdP3D3rcBU4PT0Bu4+0903R5OzgP4AUahku/uzUbuNae1ERGQXSzIs+gFL06aLo3mN+RYwI3r+RWCdmT1hZnPN7Oaop1KHmU02syIzK1qzZk2LFS4iInUlGRbWwDxvsKHZJKAQuDmalQ0cDfwQOAzYFzh/m4253+Xuhe5e2KNHj5aoWUREGpBkWBQDe6dN9weW129kZuOAnwIT3L0sbd250RBWBfAXYGSCtYqISBOSDIvZwP5mNsjMcoGJwJPpDcxsBPAHQlCsrrduVzOr7i4cByxMsFYREWlCYmER9QguAZ4BFgGPuvsCM7vezCZEzW4GCoDHzGyemT0ZrVtJGIJ63szeJgxp3Z1UrSIi0jRzb3A3QptTWFjoRUVFmS5DRKRNMbM57l4Y105ncIuISCyFhYiIxGpWWJjZ982skwX3mtmbZnZi0sWJiEjr0NyexYXuXgKcCPQALgBuTKwqERFpVZobFtUn2J0M3O/ub9HwSXciIrIbam5YzDGzvxPC4hkz6whUJVeWiIi0JtnNbPctYDjwgbtvNrO9CENRIiKyB2huz+JLwHvuvi66jtM1wPrkyhIRkdakuWHxe2CzmR0C/Bj4mHC/CRER2QM0NywqPJzqfTpwu7vfDnRMriwREWlNmrvPYoOZ/QT4OnB0dG+JnOTKEhGR1qS5PYuzgTLC+RYrCTcxurnpVUREZHfRrLCIAuIhoLOZnQqUurv2WYiI7CGae7mPs4A3gK8BZwGvm9mZSRYmIiKtR3P3WfwUOKz6BkXRTYmeA6YlVZiIiLQezd1nkap3J7u127GuiIi0cc3tWfzNzJ4BpkTTZwNPJ1OSiIi0Ns0KC3f/kZl9FRhNuIDgXe4+PdHKRESk1Wj2UJK7P+7uV7j75c0NCjMbb2bvmdkSM7uqgeVXmNlCM5tvZs+b2T71lncys2Vm9tvm1ikiIi2vybAwsw1mVtLAY4OZlcSsmwXcAZwEDAbOMbPB9ZrNBQrdfRhhZ/lN9Zb/Enhxe34gERFpeU2Ghbt3dPdODTw6ununmG2PApa4+wfuvhWYSrhcSPr2Z7r75mhyFtC/epmZHQr0Av6+vT+UiIi0rCSPaOoHLE2bLo7mNeZbwAwAM0sBtwA/auoFzGyymRWZWdGaNWt2slwREWlMkmHR0J30vMGG4bLnhdReQuR7wNPuvrSh9jUbc7/L3QvdvbBHjx47VayIiDSuuYfO7ohiYO+06f7A8vqNzGwc4aS/Y9y9LJr9JcIFC78HFAC5ZrbR3bfZSS4iIslLMixmA/ub2SBgGTARODe9gZmNAP4AjE8/6c/dz0trcz5hJ7iCQkQkQxIbhnL3CuAS4BlgEfCouy8ws+vNbELU7GZCz+ExM5tnZk8mVY+IiOw4C/c0avsKCwu9qKgo02WIiLQpZjbH3Qvj2un6TiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrESDQszG29m75nZEjO7qoHlV5jZQjObb2bPm9k+0fzhZvaamS2Ilp2dZJ0iItK0xMLCzLKAO4CTgMHAOWY2uF6zuUChuw8DpgE3RfM3A99w9yHAeOA2M+uSVK0iItK0JHsWo4Al7v6Bu28FpgKnpzdw95nuvjmanAX0j+b/y90XR8+XA6uBHgnWKiIiTUgyLPoBS9Omi6N5jfkWMKP+TDMbBeQC7zewbLKZFZlZ0Zo1a3ayXBERaUySYWENzPMGG5pNAgqBm+vN7wP8GbjA3au22Zj7Xe5e6O6FPXqo4yEikpTsBLddDOydNt0fWF6/kZmNA34KHOPuZWnzOwFPAde4+6wE6xQRkRhJ9ixmA/ub2SAzywUmAk+mNzCzEcAfgAnuvjptfi4wHXjA3R9LsEYREWmGxMLC3SuAS4BngEXAo+6+wMyuN7MJUbObgQLgMTObZ2bVYXIWMAY4P5o/z8yGJ1WriIg0zdwb3I3Q5hQWFnpRUVGmyxARaVPMbI67F8a10xncIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrEUFiJt1YaVUFWV6SpkD6GwEGlr1hfDI5PglgPgzqPgvRmwm9zxUnZQZUXiL5FoWJjZeDN7z8yWmNlVDSy/wswWmtl8M3vezPZJW/ZNM1scPb6ZZJ0ibUJlObx6O/x2FCx+Dg7/LlRsgSkT4d4T4cOXM12h7Gor34Gp58HjFyb+UomFhZllAXcAJwGDgXPMbHC9ZnOBQncfBkwDborW3Qu4FjgcGAVca2Zdk6pVpNX76FW482h49ucwaAxc/DqcdCNc/AacelvobfzpVPjzV2D53ExXK0lbtQAe+TrcORo+fAl6Dk68d5md4LZHAUvc/QMAM5sKnA4srG7g7jPT2s8CJkXP/w141t0/i9Z9FhgPTEmwXpHWZ+MaePZn8NYU6DwAJk6BA0+uXZ6VA4UXwCETYfY98PKtcNdYOGgCHHcN9DggY6VLAlYvghduhIV/gdyOMObH8KXvQbvkv0snGRb9gKVp08WEnkJjvgXMaGLdfvVXMLPJwGSAAQMG7EytIq1LVSXMuR+evx62boajroAxP4TcDg23z2kHR/4HjPwmvHYHvPZbePewrpvaAAAQjElEQVT/4JBzYeyV0EV/H23a6nfhxf+GBdPD78DRP4QvXQzt99plJSQZFtbAvAb7SWY2CSgEjtmedd39LuAugMLCQu3hk93DsjfhqSvCcNLAo+GUW5rfQ8jvBMf+BEZdFHoZs++Btx+FwgvDB0xBj2Rrl5a15j148SZ45/EQEkddHr4U7MKQqJZkWBQDe6dN9weW129kZuOAnwLHuHtZ2rpj6637QiJVirQWW9bBP34Js++Fgp5wxj1w8JlgDX13itGhO4z/zzBE8eJ/wxt3w5t/hiO+Gz5s2nVp+fql5Xy6OPy/vT0NctrD6O/DkZdCh24ZK8k8oZ0iZpYN/As4HlgGzAbOdfcFaW1GEHZsj3f3xWnz9wLmACOjWW8Ch1bvw2hIYWGhFxUVtfjPIZI4d5j/CPz9Gti8FkZNhmOvhvzOLfcany6BmTfAgicgvwscdRmM+nfIbd9yryE779Ml8NJN8PZjkJ0Ph307BEWH7om9pJnNcffC2HZJhUVUxMnAbUAWcJ+732Bm1wNF7v6kmT0HHAysiFb5xN0nROteCFwdzb/B3e9v6rUUFtImrX4XnvoBfPwK9CuEU2+FPock93or3oLnfwlLnoWC3nDMj2DENyA7N7nXlHhr34eXbg5fGrLyYNS34cjv75Jhw1YRFruSwkLalK2bwlj0a7+F3AI44brwoZ3aRefJfvzPsPP8k9eg60AYe3UY8kpl7ZrXl+CzD+ClX8NbU8ORbdU9iYKeu6wEhYVIa+QO7z4FM66EkmIYMQnGXZfoMEOTtSx+NoTGqrfDsfrH/QwOOGnH9pNI8332Ibz8a5g3JTr8+UIYfRl07LXLS2luWCS5g1tE0n32YQiJxc9AzyFw5r0w4IjM1WMGXzwR9hsHC6fDP26AqedA/8Pg+OjkP2lZn38chpvemgKWFY5aO+py6Ng705XFUliIJK2iDF79TfgmmcqGE2+Aw/89fKNsDVIpGPrVcCLfvIfDUTh/Og32PRaO/xn0OzTTFbZ96z4Jw03zHgJLhZ7EUZdDp76ZrqzZFBYiSXr/H/DUD+Gz92Hwl+Hf/hM6b3N+aeuQlQOHfhOGnR2dDX4L3H0cHHRaGJ7S2eDbb93S8D7OfTD05A69IIREa/0daILCQiQJJSvgmavDoap77QuTnoD9js90Vc2Tkw9HXgIjvwGzfgf//G3YzzJsIoy9CrruE7+NPd36ZSEk3nwgTI/8Bhx9BXTun9m6doJ2cIu0pMoKeOMumPmfULk1XKLjyEvDB3BbtWktvHJrOLHPq8IQypgf7tIjdtqMkuXhzPk3/xQOIBgxCY7+AXTZO37dDNHRUCK72ievh8t0rHoH9jsBTr4p9Cp2F+uXhf0Zcx+E7LzobPBLdTY4hJ7kK/8P5vwRvBKGnxcCtQ1ck0thIbKrbFoLz/08fIh26gcn/TcceOrue/jp2vfD2eDvPB7OMh99GRz+nT3zbPANK0NIFN0PVRUw/NwQEl0HZrqyZlNYiCStqgrm/hmeuxbKNoSrgI75MeQVZLqyXWPF/HAtq8V/h4JeMOZH4aq3e8LZ4BtWwau3QdF94aZUh5wTQmKvQZmubLspLESStGJ+GHIqng37jA5Xhu15UKaryoyPX4vOBv8ndNknXNfq4K/tnmeDb1wd7lY4+96wT+qQiWGfRLcvZLqyHaawEElCaUnYef3GH6DdXvBvN4RDTXfXIafmcoclz8Pz18HK+dDjoHDzpQNP2T3em41r4J+3wxv3QGVZ+D8f86M2HRLVdAa3SEtyD2P0z/wUNq6Cw74VPgx3wR3K2gQz2H8cfOG4cBe3mTfAI+eFiyMe/3PY95j4bbRGm9ZGIXE3VJSGHtOYH0P3/TJd2S6nsBCJ8+nicGXYD1+EPsPhnId1VnNjUikYekY4G/yth8MtQB+YAPuOheGTwv4MS4VLXViq9pFK1Z3epo2FYa0G21jadhpqU+/RYJt6vZ9Na+G1/4HX74LyzeEii2N+DD2+mIl3tVVQWIg0ZuvmcGLVq7eHG9Cccks4A3d3HItvaVnZ4US0g88KO4Ff/jU88e1MV9UEqxseVRXhnJKhZ8AxV+rsdRQW4UiG92aE21Hmd4a8TuHmMPmdWs+1e2TXe+9vMONH4Zo+h5wDJ1yvk9B2RE5+uFvfoefD5x+FD+CaR2UY3kufV1VZr01VzHKPttNYmwaW17TxerWkLU9lhyGnPfWghQYoLLZ8Do9+veFlOe2j8OhcL0w615tXfzr6N7fD7rFzb0+y7hOYcRW89xT0OBDOfwoGHpXpqtq+3PbQa3Cmq5CdoLBo1xW+8yqUrg+PspLoeQmUrqs7vfmzcJnp6nmVW5vetmWFAKkTMJ3rhU79EEoPIPVuElexFVYvhOVzYdmccM9js9CTOOJ7ev9FIgqLrBzoPXTH1i0vrRcw9QOnOnTS5n32Ye10WUn8a+S03zZMCnpDryHRY2hGb+LeplRWwKfvhWCofqx8JxwKCeGLw4GnwLhftOpr+YhkQqJhYWbjgdsJ9+C+x91vrLd8DOEe3cOAie4+LW3ZTcApQAp4Fvi+t7aTQnLyw2NH725VVRnO/G2wV9NICG1eG+6jPO/B2u3UD4/eQ6Hb/nvGmbSNqaoKlwVf9mZaMMwPR7ZACN8+h4T7SvQdAf1GhhPKNGwo0qDEwsLMsoA7gBOAYmC2mT3p7gvTmn0CnA/8sN66RwKjCSEC8ApwDPBCUvVmRCorXIRtRy7EtnE1rFoQPd4Jj9dfrh0aS+WEIzhqQiQKkoJeu98HonvYebp8Lix/E5bPC4+tG8LynPbQe1jYydp3RHjs9YVdd79rkd1Akj2LUcASd/8AwMymAqcDNWHh7h9Fy6rqretAPpALGJADrEqw1ranoGd4fOHY2nmV5bB2SVqALICPXoH5j9S2ad8thEavobUh0uPAtnMJbXcoWVbbW6juOZSuC8uzcqH3wXDI2dB3ZAiG7l8Mh3KKyA5L8i+oH7A0bboYOLw5K7r7a2Y2E1hBCIvfuvui+u3MbDIwGWDAgNZ/KeDEZeWEQ/16HhROIqq2+bN6vZAF4dj3ii1huWVBt/1qw6P3weHfTv0y3wvZuLruUNLyubBpdViWyoaeg2Hw6bU9hp6D9+zhN5GEJBkWDX3KNGufg5ntBxwEVN9W6lkzG+PuL9XZmPtdwF0Qrg21E7Xu3trvBYOODo9qVZVhZ/uqt2uDZFlRuLNbtfzOdXsgvYaGIMrtkEydmz+rN5Q0N/QiIJwo1f0A2P+E2mDoNQRy2iVTi4jUkWRYFAPph5T0B5Y3c92vALPcfSOAmc0AjgBeanItab5UVri+Tff9YMhXaueXrofVi0IPZGXUC5n3MGzdGDWwcEOf6vCoDpIu+2zfPoDS9WFHffpQ0rqPa5d32w/2OTIKhpGht7OnXPpbpBVKMixmA/ub2SBgGTAROLeZ634CXGRm/0XooRxDOGpKkpbfGQYcER7VqqrCB3n9oaxFf6Wms5hbEIaAeg0JR2P1Ghqm8zvB1k3hkt41Q0lvhn0r1boMCIFQeGEIhz6H6O5rIq1MopcoN7OTCR/yWcB97n6DmV0PFLn7k2Z2GDAd6AqUAivdfUh0JNXvgDGET6O/ufsVTb2WLlGeAWUbYc27teFRHSSl62vbFPQO+xg8OoahY9/oUNVoKKnPCJ0nIpJBup+FZEb10Uoro8N51y6Jeg5ROHTsnekKRSSN7mchmWEGnfuHxwHjM12NiLQQnZUkIiKxFBYiIhJLYSEiIrEUFiIiEkthISIisRQWIiISS2EhIiKxFBYiIhJrtzmD28zWAB/HNmxcd+DTFionaW2pVmhb9balWqFt1duWaoW2Ve/O1LqPu/eIa7TbhMXOMrOi5pzy3hq0pVqhbdXblmqFtlVvW6oV2la9u6JWDUOJiEgshYWIiMRSWNS6K9MFbIe2VCu0rXrbUq3QtuptS7VC26o38Vq1z0JERGKpZyEiIrEUFiIiEmuPDwszG29m75nZEjO7KtP1NMXM7jOz1Wb2TqZriWNme5vZTDNbZGYLzOz7ma6pKWaWb2ZvmNlbUb3XZbqmOGaWZWZzzez/Ml1LHDP7yMzeNrN5Ztaqb2lpZl3MbJqZvRv9/n4p0zU1xswOiN7T6keJmV2WyGvtyfssont9/ws4ASgGZgPnuPvCjBbWCDMbA2wEHnD3oZmupylm1gfo4+5vmllHYA7w5Vb83hrQwd03mlkO8ArwfXefleHSGmVmVwCFQCd3PzXT9TTFzD4CCt291Z/kZmZ/Al5293vMLBdo7+7rMl1XnOjzbBlwuLvvzAnKDdrTexajgCXu/oG7bwWmAqdnuKZGuftLwGeZrqM53H2Fu78ZPd8ALAL6ZbaqxnmwMZrMiR6t9puUmfUHTgHuyXQtuxMz6wSMAe4FcPetbSEoIscD7ycRFKCw6AcsTZsuphV/oLVVZjYQGAG8ntlKmhYN68wDVgPPuntrrvc24MdAVaYLaSYH/m5mc8xscqaLacK+wBrg/miI7x4z65DpopppIjAlqY3v6WFhDcxrtd8m2yIzKwAeBy5z95JM19MUd6909+FAf2CUmbXKoT4zOxVY7e5zMl3Ldhjt7iOBk4CLoyHV1igbGAn83t1HAJuAVr0vEyAaLpsAPJbUa+zpYVEM7J023R9YnqFadjvR2P/jwEPu/kSm62muaNjhBWB8hktpzGhgQrQfYCpwnJk9mNmSmubuy6N/VwPTCUPArVExUJzWq5xGCI/W7iTgTXdfldQL7OlhMRvY38wGRck8EXgywzXtFqIdxvcCi9z91kzXE8fMephZl+h5O2Ac8G5mq2qYu//E3fu7+0DC7+w/3H1ShstqlJl1iA5yIBrSORFolUf0uftKYKmZHRDNOh5olQdl1HMOCQ5BQehy7bHcvcLMLgGeAbKA+9x9QYbLapSZTQHGAt3NrBi41t3vzWxVjRoNfB14O9oPAHC1uz+dwZqa0gf4U3RESQp41N1b/SGpbUQvYHr4/kA28LC7/y2zJTXpP4CHoi+QHwAXZLieJplZe8IRnf+e6OvsyYfOiohI8+zpw1AiItIMCgsREYmlsBARkVgKCxERiaWwEBGRWAoLkVbAzMa2havHyp5LYSEiIrEUFiLbwcwmRfe9mGdmf4guPrjRzG4xszfN7Hkz6xG1HW5ms8xsvplNN7Ou0fz9zOy56N4Zb5rZF6LNF6TdR+Gh6Cx4kVZBYSHSTGZ2EHA24aJ4w4FK4DygA+G6PCOBF4Fro1UeAK5092HA22nzHwLucPdDgCOBFdH8EcBlwGDC1U9HJ/5DiTTTHn25D5HtdDxwKDA7+tLfjnA58yrgkajNg8ATZtYZ6OLuL0bz/wQ8Fl0jqZ+7Twdw91KAaHtvuHtxND0PGEi4CZNIxiksRJrPgD+5+0/qzDT7Wb12TV1Dp6mhpbK055Xo71NaEQ1DiTTf88CZZtYTwMz2MrN9CH9HZ0ZtzgVecff1wOdmdnQ0/+vAi9E9PYrN7MvRNvKiC8GJtGr65iLSTO6+0MyuIdzxLQWUAxcTbpAzxMzmAOsJ+zUAvgncGYVB+tVLvw78wcyuj7bxtV34Y4jsEF11VmQnmdlGdy/IdB0iSdIwlIiIxFLPQkREYqlnISIisRQWIiISS2EhIiKxFBYiIhJLYSEiIrH+PxjCDeErqEbjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Avaliando o Modelo\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss'), \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [0.2014742661745214, 0.926107890340965]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", model2.evaluate(X_valid, y_valid, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model2.predict_proba(X_valid)\n",
    "preds = probs[:,0]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_valid, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_predicted = np.round(model2.predict(X_test)).T[0]\n",
    "y2_correct = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(y2_predicted, y2_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inacurácia: 0.07676479234031831\n",
      "Acurácia: 0.9232352076596817\n",
      "Taxa de Falsos Negativos: 0.09409190371991247\n",
      "Taxa de Falsos Positivos: 0.9059080962800875\n",
      "Falsos Negativos/total: 0.007222945449964305\n"
     ]
    }
   ],
   "source": [
    "false_neg = 0\n",
    "false_pos = 0\n",
    "incorrect = 0\n",
    "total = len(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    if y2_predicted[i] != y2_correct[i]:\n",
    "        incorrect += 1\n",
    "        if y2_correct[i] == 1 and y2_predicted[i] == 0:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            false_pos += 1\n",
    "\n",
    "inaccuracy = incorrect / total\n",
    "\n",
    "print('Inacurácia:', inaccuracy)\n",
    "print('Acurácia:', 1 - inaccuracy)\n",
    "if incorrect > 0:\n",
    "    print('Taxa de Falsos Negativos:', false_neg/incorrect)\n",
    "    print('Taxa de Falsos Positivos:', false_pos / incorrect )    \n",
    "print('Falsos Negativos/total:', false_neg/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results\n",
      "0.9336870026525199\n",
      "\n",
      "Test Results\n",
      "0.9232352076596817\n",
      "0.9363669996300407\n"
     ]
    }
   ],
   "source": [
    "print('Validation Results')\n",
    "print(recall_score(y_valid,np.round(model2.predict(X_valid)).T[0]))\n",
    "print('\\nTest Results')\n",
    "print(1 - inaccuracy)\n",
    "print(recall_score(y_test,np.round(model2.predict(X_test)).T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828\n"
     ]
    }
   ],
   "source": [
    "print(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False  True  __all__\n",
      "Actual                         \n",
      "False      19454  1656    21110\n",
      "True         172  2531     2703\n",
      "__all__    19626  4187    23813\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: 23813\n",
      "P: 2703\n",
      "N: 21110\n",
      "PositiveTest: 4187\n",
      "NegativeTest: 19626\n",
      "TP: 2531\n",
      "TN: 19454\n",
      "FP: 1656\n",
      "FN: 172\n",
      "TPR: 0.9363669996300407\n",
      "TNR: 0.9215537659876836\n",
      "PPV: 0.6044900883687605\n",
      "NPV: 0.9912361153571793\n",
      "FPR: 0.07844623401231644\n",
      "FDR: 0.39550991163123955\n",
      "FNR: 0.06363300036995931\n",
      "ACC: 0.9232352076596817\n",
      "F1_score: 0.7346879535558781\n",
      "MCC: 0.7149027072260242\n",
      "informedness: 0.8579207656177243\n",
      "markedness: 0.5957262037259397\n",
      "prevalence: 0.11350942762356696\n",
      "LRP: 11.936417489245263\n",
      "LRN: 0.06904968838335772\n",
      "DOR: 172.86707251994153\n",
      "FOR: 0.008763884642820748\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix2 = ConfusionMatrix(y2_correct, y2_predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix2)\n",
    "confusion_matrix2.plot(normalized=True)\n",
    "plt.show()\n",
    "confusion_matrix2.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "O modelo de Deep Learning conseguiu atingir um excelente índice de acurácia no recall da classe minoritária, demonstrando ser uma ferramenta eficaz na identificação antecipada de pacientes que necessitarão de uma maior atenção da equipe assistencial, por possuir uma alta probabilidade de ocorrência de readmissão.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Referencias\n",
    "\n",
    "Data Science Acabemy: Formação Inteligencia Artificial           \n",
    "https://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\n",
    "\n",
    "Fator de Qualidade: dados de readmissão hospitalar devem ser informados à ANS   \n",
    "http://www.ans.gov.br/aans/noticias-ans/qualidade-da-saude/3167-fator-de-qualidade-dados-de-readmissao-hospitalar-devem-ser-informados-a-ans\n",
    "\n",
    "3 formas únicas de diminuir a readmissão hospitalar       \n",
    "https://saudebusiness.com/noticias/3-formas-diminuir-readmissao-hospitalar/\n",
    "\n",
    "Resampling strategies for imbalanced datasets       \n",
    "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "\n",
    "The Right Way to Oversample in Predictive Modeling       \n",
    "https://beckernick.github.io/oversampling-modeling/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
